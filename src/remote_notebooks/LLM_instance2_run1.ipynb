{"cells":[{"cell_type":"markdown","metadata":{"id":"XxChZLAKliXg"},"source":["Content taken from GitHub repository hosting the code:\n","\n","https://github.com/matthewjohnson42/umass-cs685-finalproject\n","\n","Substantial modifications made to adapt to Colab training environment."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25230,"status":"ok","timestamp":1715719139533,"user":{"displayName":"Matthew Johnson","userId":"09672185346885346735"},"user_tz":240},"id":"U0NG9bOzleFZ","outputId":"17cae7c9-f469-4402-a1ec-54b3ec52a539"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","\n","import torch\n","import numpy as np\n","\n","import random\n","from typing import Optional\n","import time\n","import os\n","import gc\n","\n","\n","drive.mount('/content/drive', force_remount=True)\n","root_filepath = '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/'\n","is_remote_execution = True\n","\n","torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sllajv_0n2Vu","executionInfo":{"status":"ok","timestamp":1715719139533,"user_tz":240,"elapsed":7,"user":{"displayName":"Matthew Johnson","userId":"09672185346885346735"}}},"outputs":[],"source":["SETimesByT5Vaswani2017Kocmi2018_0 = {\n","    'dataset_transformer_name': 'dataset_transformer_setimesbyt5',\n","    'model_name': 'transformer_vaswani2017',\n","    'trainer_name': 'model_trainer_kocmi2018',\n","    'latest_param_filename_tag': '1715672129',\n","    # corresponds to dictionary 'get' calls in the dataset_loader constructor\n","    'dataset_transformer_hyperparameters': {\n","        'sentence_length_min_percentile': 5,\n","        'sentence_length_max_percentile': 95,\n","        'parsed_dataset_filename': 'setimes_parsed-1715586293'\n","    },\n","    # corresponds to dictionary 'get' calls in the model constructor\n","    'model_hyperparameters': {\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 2048,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    },\n","    # corresponds to dictionary 'get' calls in the trainer constructor\n","    'trainer_hyperparameters': {\n","        # optimization and lr schedule following Kocmi 2018 - Trivial TL - Sec 3\n","        'optimizer_name': 'Adam',\n","        'lr_scheduler_name': 'ExponentialLR',\n","        'initial_lr': 0.001,\n","        'exp_decay': 0.5,\n","        'epochs': 10,\n","        'epoch_starting_index': 0,\n","        'batch_size_limit': 175,\n","        'element_difference_limit': 19,\n","        'batch_starting_index': 765\n","    }\n","}\n","\n","SETimesByT5Vaswani2017Kocmi2018_1 = {\n","    'dataset_transformer_name': 'dataset_transformer_setimesbyt5',\n","    'model_name': 'transformer_vaswani2017',\n","    'trainer_name': 'model_trainer_kocmi2018',\n","    'latest_param_filename_tag': '1715672186',\n","    # corresponds to dictionary 'get' calls in the dataset_loader constructor\n","    'dataset_transformer_hyperparameters': {\n","        'sentence_length_min_percentile': 5,\n","        'sentence_length_max_percentile': 95,\n","        'parsed_dataset_filename': 'setimes_parsed-1715586974'\n","    },\n","    # corresponds to dictionary 'get' calls in the model constructor\n","    'model_hyperparameters': {\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1796,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    },\n","    # corresponds to dictionary 'get' calls in the trainer constructor\n","    'trainer_hyperparameters': {\n","        # optimization and lr schedule following Kocmi 2018 - Trivial TL - Sec 3\n","        'optimizer_name': 'Adam',\n","        'lr_scheduler_name': 'ExponentialLR',\n","        'initial_lr': 0.001,\n","        'exp_decay': 0.5,\n","        'epochs': 10,\n","        'epoch_starting_index': 1,\n","        'batch_size_limit': 250,\n","        'element_difference_limit': 19,\n","        'batch_starting_index': 0\n","    }\n","}\n","\n","SETimesByT5Vaswani2017Kocmi2018_2 = {\n","    'dataset_transformer_name': 'dataset_transformer_setimesbyt5',\n","    'model_name': 'transformer_vaswani2017',\n","    'trainer_name': 'model_trainer_kocmi2018',\n","    'latest_param_filename_tag': '1715672061',\n","    # corresponds to dictionary 'get' calls in the dataset_loader constructor\n","    'dataset_transformer_hyperparameters': {\n","        'sentence_length_min_percentile': 5,\n","        'sentence_length_max_percentile': 95,\n","        'parsed_dataset_filename': 'setimes_parsed-1715586293'\n","    },\n","    # corresponds to dictionary 'get' calls in the model constructor\n","    'model_hyperparameters': {\n","        'd_model': 128,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1024,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    },\n","    # corresponds to dictionary 'get' calls in the trainer constructor\n","    'trainer_hyperparameters': {\n","        # optimization and lr schedule following Kocmi 2018 - Trivial TL - Sec 3\n","        'optimizer_name': 'Adam',\n","        'lr_scheduler_name': 'ExponentialLR',\n","        'initial_lr': 0.001,\n","        'exp_decay': 0.5,\n","        'epochs': 10,\n","        'epoch_starting_index': 1,\n","        'batch_size_limit': 400,\n","        'element_difference_limit': 19,\n","        'batch_starting_index': 0\n","    }\n","}\n","\n","loss_weights_0 = torch.tensor([0, 0, 0.244293498, 0.354252208, 0.119055746, 0.129490827, 0.234447, 0.331966255, 0.062493498, 0.200436672, 0.128134531, 0.115730135, 0.114789455, 0.111928721, 0.120232896, 0.172708905, 0.091552235, 0.18382728, 0.167772254, 0.221570123, 0.26933557, 0.207267199, 0.161156936, 0.163208063, 0.206129376, 0.1923278, 0.246448966, 0.309805116, 0.312319847, 0.341492611, 0.287077958, 0.317765776, 0.282694925, 0.328107375, 0.305618951, 0.342956521, 0.33013427, 0.36810305, 0.29995848, 0.39787945, 0.36195021, 0.231575726, 0.238716465, 0.228047462, 0.589442629, 0.428540415, 0.349514641, 0.428583838, 0.372321337, 0.35910363, 0.314836573, 0.359830778, 0.329628544, 0.358553271, 0.38731315, 0.301171514, 0.414866271, 0.454463773, 0.410552656, 0.418645272, 0.489374491, 0.60674051, 0.415410876, 0.387694004, 0.469862412, 0.387466491, 0.404244134, 0.410692024, 0.3650963, 0.328332631, 0.362047807, 0.390990331, 0.407611852, 0.353090879, 0.519082435, 0.465516001, 0.382984849, 0.382904558, 0.417591599, 0.47406963, 0.513595183, 0.561900207, 0.600687916, 0.552947486, 0.672534847, 0.775971182, 0.728910416, 0.783069437, 0.726598021, 0.870987145, 0.761128877, 0.716818395, 0.708372474, 0.70507743, 0.731312344, 0.718124592, 0.931839821, 0.675783865, 0.863679642, 0.79786095, 0.870987145, 0.888835536, 0.845831251, 0.845831251, 0.870987145, 0.831986963, 0.817320914, 0.888835536, 0.82798286, 0.931839821, 0.900147142, 0.91399143, 0.91399143, 0.956995715, 0.931839821, 1, 0.900147142, 0.870987145, 0.870987145, 1, 0.900147142, 0.888835536, 0.956995715, 0.824221583, 1, 1, 0.956995715, 1, 0.931839821, 0.931839821, 0.956995715, 0.956995715, 0.956995715, 0.888835536, 0.931839821, 1, 0.900147142, 0.91399143, 0.956995715, 0.956995715, 0.956995715, 1, 1, 1, 0.900147142, 0.931839821, 0.900147142, 0.931839821, 0.956995715, 0.91399143, 1, 0.956995715, 1, 1, 1, 1, 1, 0.956995715, 1, 1, 1, 1, 1, 1, 1, 0.931839821, 1, 0.956995715, 1, 1, 0.956995715])\n","loss_weights_1 = torch.tensor([0, 0, 0.283786926, 0.387999164, 0.165094134, 0.174983875, 0.274455009, 0.366877881, 0.111487844, 0.242222071, 0.173698459, 0.16194232, 0.1610508, 0.158339569, 0.166209765, 0.215943364, 0.139027964, 0.226480689, 0.211264704, 0.262251081, 0.307520293, 0.248695632, 0.204995104, 0.206939038, 0.247617272, 0.23453697, 0.285829749, 0.345874889, 0.3482582, 0.375906387, 0.324335459, 0.353419523, 0.320181484, 0.363220667, 0.341907494, 0.377293793, 0.365141636, 0.401126157, 0.336542841, 0.429346435, 0.395294867, 0.271733788, 0.27850135, 0.268389913, 0.610898469, 0.458405049, 0.383509184, 0.458446203, 0.405123996, 0.39259705, 0.350643401, 0.393286196, 0.36466234, 0.392075452, 0.419332333, 0.337692481, 0.44544552, 0.482973645, 0.441357336, 0.449027029, 0.51605992, 0.627292358, 0.445961663, 0.419693282, 0.497567547, 0.419477659, 0.435378499, 0.44148942, 0.398276541, 0.363434152, 0.395387363, 0.422817343, 0.438570219, 0.386898527, 0.544215318, 0.493448281, 0.415230229, 0.415154135, 0.448028421, 0.501554895, 0.539014832, 0.584795422, 0.621556075, 0.576310573, 0.689648265, 0.78767899, 0.743077631, 0.794406288, 0.740886083, 0.877729392, 0.773612348, 0.731617543, 0.723613008, 0.720490164, 0.745354034, 0.732855478, 0.93540189, 0.692727489, 0.870803781, 0.808424792, 0.877729392, 0.894645021, 0.853888152, 0.853888152, 0.877729392, 0.84076737, 0.826867773, 0.894645021, 0.836972522, 0.93540189, 0.90536548, 0.918486261, 0.918486261, 0.959243131, 0.93540189, 1, 0.90536548, 0.877729392, 0.877729392, 1, 0.90536548, 0.894645021, 0.959243131, 0.833407811, 1, 1, 0.959243131, 1, 0.93540189, 0.93540189, 0.959243131, 0.959243131, 0.959243131, 0.894645021, 0.93540189, 1, 0.90536548, 0.918486261, 0.959243131, 0.959243131, 0.959243131, 1, 1, 1, 0.90536548, 0.93540189, 0.90536548, 0.93540189, 0.959243131, 0.918486261, 1, 0.959243131, 1, 1, 1, 1, 1, 0.959243131, 1, 1, 1, 1, 1, 1, 1, 0.93540189, 1, 0.959243131, 1, 1, 0.959243131])\n","\n","english_char_mappings = {\n","    ord(\" \"): ord(\" \"),\n","    ord(\"e\"): ord(\"e\"),\n","    ord(\"t\"): ord(\"t\"),\n","    ord(\"a\"): ord(\"a\"),\n","    ord(\"i\"): ord(\"i\"),\n","    ord(\"o\"): ord(\"o\"),\n","    ord(\"n\"): ord(\"n\"),\n","    ord(\"r\"): ord(\"r\"),\n","    ord(\"s\"): ord(\"s\"),\n","    ord(\"h\"): ord(\"h\"),\n","    ord(\"l\"): ord(\"l\"),\n","    ord(\"d\"): ord(\"d\"),\n","    ord(\"c\"): ord(\"c\"),\n","    ord(\"u\"): ord(\"u\"),\n","    ord(\"m\"): ord(\"m\"),\n","    ord(\"p\"): ord(\"p\"),\n","    ord(\"f\"): ord(\"f\"),\n","    ord(\"g\"): ord(\"g\"),\n","    ord(\"y\"): ord(\"y\"),\n","    ord(\"b\"): ord(\"b\"),\n","    ord(\"w\"): ord(\"w\"),\n","    ord(\"v\"): ord(\"v\"),\n","    ord(\",\"): ord(\",\"),\n","    ord(\".\"): ord(\".\"),\n","    ord(\"k\"): ord(\"k\"),\n","    ord(\"T\"): ord(\"T\"),\n","    ord(\"S\"): ord(\"S\"),\n","    ord(\"0\"): ord(\"0\"),\n","    ord(\"A\"): ord(\"A\"),\n","    ord(\"-\"): ord(\"-\"),\n","    ord(\"B\"): ord(\"B\"),\n","    ord(\"M\"): ord(\"M\"),\n","    ord(\"\\\"\"): ord(\"\\\"\"),\n","    ord(\"E\"): ord(\"E\"),\n","    ord(\"P\"): ord(\"P\"),\n","    ord(\"C\"): ord(\"C\"),\n","    ord(\"I\"): ord(\"I\"),\n","    ord(\"1\"): ord(\"1\"),\n","    ord(\"'\"): ord(\"'\"),\n","    ord(\"j\"): ord(\"j\"),\n","    ord(\"2\"): ord(\"2\"),\n","    ord(\"R\"): ord(\"R\"),\n","    ord(\"U\"): ord(\"U\"),\n","    ord(\"K\"): ord(\"K\"),\n","    ord(\"D\"): ord(\"D\"),\n","    ord(\"x\"): ord(\"x\"),\n","    ord(\"H\"): ord(\"H\"),\n","    ord(\"F\"): ord(\"F\"),\n","    ord(\"N\"): ord(\"N\"),\n","    ord(\"G\"): ord(\"G\"),\n","    ord(\"/\"): ord(\"/\"),\n","    ord(\"O\"): ord(\"O\"),\n","    ord(\")\"): ord(\")\"),\n","    ord(\"(\"): ord(\"(\"),\n","    ord(\"9\"): ord(\"9\"),\n","    ord(\"W\"): ord(\"W\"),\n","    ord(\"z\"): ord(\"z\"),\n","    ord(\"5\"): ord(\"5\"),\n","    ord(\"3\"): ord(\"3\"),\n","    ord(\"J\"): ord(\"J\"),\n","    ord(\"4\"): ord(\"4\"),\n","    ord(\"q\"): ord(\"q\"),\n","    ord(\"L\"): ord(\"L\"),\n","    ord(\"6\"): ord(\"6\"),\n","    ord(\"7\"): ord(\"7\"),\n","    ord(\"8\"): ord(\"8\"),\n","    ord(\"V\"): ord(\"V\"),\n","    ord(\"[\"): ord(\"[\"),\n","    ord(\"]\"): ord(\"]\"),\n","    ord(\":\"): ord(\":\"),\n","    ord(\"Z\"): ord(\"Z\"),\n","    ord(\"%\"): ord(\"%\"),\n","    ord(\"Y\"): ord(\"Y\"),\n","    ord(\";\"): ord(\";\"),\n","    ord(\"–\"): ord(\"-\"),\n","    ord(\"?\"): ord(\"?\"),\n","    ord(\"X\"): ord(\"X\"),\n","    ord(\"$\"): ord(\"$\"),\n","    ord(\"’\"): ord(\"'\"),\n","    ord(\"Q\"): ord(\"Q\"),\n","    ord(\"…\"): ord(\"\\u0120\"),\n","    ord(\"!\"): ord(\"\\u0120\"),\n","    ord(\"&\"): ord(\"\\u0120\"),\n","    ord(\"ć\"): ord(\"\\u0120\"),\n","    ord(\"”\"): ord(\"\\\"\"),\n","    ord(\"é\"): ord(\"\\u0120\"),\n","    ord(\"“\"): ord(\"\\\"\"),\n","    ord(\"ı\"): ord(\"\\u0120\"),\n","    ord(\"ü\"): ord(\"\\u0120\"),\n","    ord(\"+\"): ord(\"\\u0120\"),\n","    ord(\"ç\"): ord(\"\\u0120\"),\n","    ord(\"ˈ\"): ord(\"'\"),\n","    ord(\"ö\"): ord(\"\\u0120\"),\n","    ord(\"ë\"): ord(\"\\u0120\"),\n","    ord(\"Ç\"): ord(\"\\u0120\"),\n","    ord(\"ğ\"): ord(\"\\u0120\"),\n","    ord(\"č\"): ord(\"\\u0120\"),\n","    ord(\"š\"): ord(\"\\u0120\"),\n","    ord(\"‘\"): ord(\"'\"),\n","    ord(\"ž\"): ord(\"\\u0120\"),\n","    ord(\"*\"): ord(\"\\u0120\"),\n","    ord(\"ş\"): ord(\"\\u0120\"),\n","    ord(\"б\"): ord(\"\\u0120\"),\n","    ord(\"Š\"): ord(\"\\u0120\"),\n","    ord(\"İ\"): ord(\"\\u0120\"),\n","    ord(\"à\"): ord(\"\\u0120\"),\n","    ord(\"г\"): ord(\"\\u0120\"),\n","    ord(\"ï\"): ord(\"\\u0120\"),\n","    ord(\"Ü\"): ord(\"\\u0120\"),\n","    ord(\"—\"): ord(\"-\"),\n","    ord(\"€\"): ord(\"\\u0120\"),\n","    ord(\"Ö\"): ord(\"\\u0120\"),\n","    ord(\"đ\"): ord(\"\\u0120\"),\n","    ord(\"\\\\\"): ord(\"\\u0120\"),\n","    ord(\"Ι\"): ord(\"\\u0120\"),\n","    ord(\"Α\"): ord(\"A\"),\n","    ord(\"`\"): ord(\"'\"),\n","    ord(\"_\"): ord(\"\\u0120\"),\n","    ord(\"=\"): ord(\"\\u0120\"),\n","    ord(\"@\"): ord(\"\\u0120\"),\n","    ord(\"А\"): ord(\"A\"),\n","    ord(\"ó\"): ord(\"\\u0120\"),\n","    ord(\"í\"): ord(\"\\u0120\"),\n","    ord(\"â\"): ord(\"\\u0120\"),\n","    ord(\"\\x80\"): ord(\"\\u0120\"),\n","    ord(\"Ž\"): ord(\"\\u0120\"),\n","    ord(\"Κ\"): ord(\"K\"),\n","    ord(\"Ο\"): ord(\"O\"),\n","    ord(\"р\"): ord(\"p\"),\n","    ord(\"Ş\"): ord(\"\\u0120\"),\n","    ord(\"á\"): ord(\"\\u0120\"),\n","    ord(\"у\"): ord(\"y\"),\n","    ord(\"\\x93\"): ord(\"\\u0120\"),\n","    ord(\"°\"): ord(\"\\u0120\"),\n","    ord(\"<\"): ord(\"\\u0120\"),\n","    ord(\"½\"): ord(\"\\u0120\"),\n","    ord(\"Т\"): ord(\"T\"),\n","    ord(\"ø\"): ord(\"\\u0120\"),\n","    ord(\"Ε\"): ord(\"E\"),\n","    ord(\"#\"): ord(\"\\u0120\"),\n","    ord(\"ý\"): ord(\"\\u0120\"),\n","    ord(\"л\"): ord(\"\\u0120\"),\n","    ord(\"я\"): ord(\"\\u0120\"),\n","    ord(\"�\"): ord(\"\\u0120\"),\n","    ord(\"º\"): ord(\"\\u0120\"),\n","    ord(\"ä\"): ord(\"\\u0120\"),\n","    ord(\"е\"): ord(\"e\"),\n","    ord(\"о\"): ord(\"o\"),\n","    ord(\"Đ\"): ord(\"\\u0120\"),\n","    ord(\"è\"): ord(\"\\u0120\"),\n","    ord(\"¾\"): ord(\"\\u0120\"),\n","    ord(\"\\x96\"): ord(\"\\u0120\"),\n","    ord(\"æ\"): ord(\"\\u0120\"),\n","    ord(\">\"): ord(\"\\u0120\"),\n","    ord(\"¦\"): ord(\"\\u0120\"),\n","    ord(\"μ\"): ord(\"\\u0120\"),\n","    ord(\"Č\"): ord(\"\\u0120\"),\n","    ord(\"ñ\"): ord(\"\\u0120\"),\n","    ord(\"х\"): ord(\"x\"),\n","    ord(\"М\"): ord(\"M\"),\n","    ord(\"É\"): ord(\"\\u0120\"),\n","    ord(\"±\"): ord(\"\\u0120\"),\n","    ord(\"£\"): ord(\"\\u0120\"),\n","    ord(\"{\"): ord(\"\\u0120\"),\n","    ord(\"}\"): ord(\"\\u0120\"),\n","    ord(\"ъ\"): ord(\"\\u0120\"),\n","    ord(\"а\"): ord(\"\\u0120\"),\n","    ord(\"и\"): ord(\"\\u0120\")\n","}\n","\n","turkish_char_mappings = {\n","    ord(\" \"): ord(\" \"),\n","    ord(\"a\"): ord(\"a\"),\n","    ord(\"e\"): ord(\"e\"),\n","    ord(\"i\"): ord(\"i\"),\n","    ord(\"n\"): ord(\"n\"),\n","    ord(\"r\"): ord(\"r\"),\n","    ord(\"l\"): ord(\"l\"),\n","    ord(\"ı\"): ord(\"ı\"),\n","    ord(\"k\"): ord(\"k\"),\n","    ord(\"d\"): ord(\"d\"),\n","    ord(\"t\"): ord(\"t\"),\n","    ord(\"s\"): ord(\"s\"),\n","    ord(\"u\"): ord(\"u\"),\n","    ord(\"m\"): ord(\"m\"),\n","    ord(\"y\"): ord(\"y\"),\n","    ord(\"o\"): ord(\"o\"),\n","    ord(\"ü\"): ord(\"ü\"),\n","    ord(\"b\"): ord(\"b\"),\n","    ord(\"ş\"): ord(\"ş\"),\n","    ord(\"v\"): ord(\"v\"),\n","    ord(\"g\"): ord(\"g\"),\n","    ord(\"z\"): ord(\"z\"),\n","    ord(\"ç\"): ord(\"ç\"),\n","    ord(\"ğ\"): ord(\"ğ\"),\n","    ord(\".\"): ord(\".\"),\n","    ord(\",\"): ord(\",\"),\n","    ord(\"c\"): ord(\"c\"),\n","    ord(\"h\"): ord(\"h\"),\n","    ord(\"p\"): ord(\"p\"),\n","    ord(\"ö\"): ord(\"ö\"),\n","    ord(\"'\"): ord(\"'\"),\n","    ord(\"B\"): ord(\"B\"),\n","    ord(\"A\"): ord(\"A\"),\n","    ord(\"f\"): ord(\"f\"),\n","    ord(\"S\"): ord(\"S\"),\n","    ord(\"K\"): ord(\"K\"),\n","    ord(\"0\"): ord(\"0\"),\n","    ord(\"\\\"\"): ord(\"\\\"\"),\n","    ord(\"T\"): ord(\"T\"),\n","    ord(\"M\"): ord(\"M\"),\n","    ord(\"1\"): ord(\"1\"),\n","    ord(\"P\"): ord(\"P\"),\n","    ord(\"D\"): ord(\"D\"),\n","    ord(\"H\"): ord(\"H\"),\n","    ord(\"2\"): ord(\"2\"),\n","    ord(\"E\"): ord(\"E\"),\n","    ord(\"R\"): ord(\"R\"),\n","    ord(\"-\"): ord(\"-\"),\n","    ord(\"G\"): ord(\"G\"),\n","    ord(\"j\"): ord(\"j\"),\n","    ord(\"Y\"): ord(\"Y\"),\n","    ord(\"C\"): ord(\"C\"),\n","    ord(\"İ\"): ord(\"İ\"),\n","    ord(\"O\"): ord(\"O\"),\n","    ord(\"/\"): ord(\"/\"),\n","    ord(\"F\"): ord(\"F\"),\n","    ord(\"N\"): ord(\"N\"),\n","    ord(\"9\"): ord(\"9\"),\n","    ord(\"5\"): ord(\"5\"),\n","    ord(\"3\"): ord(\"3\"),\n","    ord(\"I\"): ord(\"I\"),\n","    ord(\"4\"): ord(\"4\"),\n","    ord(\"L\"): ord(\"L\"),\n","    ord(\"6\"): ord(\"6\"),\n","    ord(\")\"): ord(\")\"),\n","    ord(\"(\"): ord(\"(\"),\n","    ord(\"7\"): ord(\"7\"),\n","    ord(\"8\"): ord(\"8\"),\n","    ord(\"U\"): ord(\"U\"),\n","    ord(\"V\"): ord(\"V\"),\n","    ord(\":\"): ord(\":\"),\n","    ord(\"[\"): ord(\"[\"),\n","    ord(\"]\"): ord(\"]\"),\n","    ord(\"’\"): ord(\"'\"),\n","    ord(\"Z\"): ord(\"Z\"),\n","    ord(\"Ç\"): ord(\"Ç\"),\n","    ord(\"J\"): ord(\"J\"),\n","    ord(\"Ü\"): ord(\"Ü\"),\n","    ord(\"Ş\"): ord(\"Ş\"),\n","    ord(\"Ö\"): ord(\"Ö\"),\n","    ord(\"%\"): ord(\"%\"),\n","    ord(\";\"): ord(\";\"),\n","    ord(\"w\"): ord(\"w\"),\n","    ord(\"W\"): ord(\"W\"),\n","    ord(\"â\"): ord(\"â\"),\n","    ord(\"–\"): ord(\"-\"),\n","    ord(\"?\"): ord(\"?\"),\n","    ord(\"x\"): ord(\"x\"),\n","    ord(\"“\"): ord(\"\\\"\"),\n","    ord(\"”\"): ord(\"\\\"\"),\n","    ord(\"X\"): ord(\"X\"),\n","    ord(\"q\"): ord(\"q\"),\n","    ord(\"î\"): ord(\"\\u0120\"),\n","    ord(\"&\"): ord(\"\\u0120\"),\n","    ord(\"!\"): ord(\"\\u0120\"),\n","    ord(\"ð\"): ord(\"\\u0120\"),\n","    ord(\"Q\"): ord(\"\\u0120\"),\n","    ord(\"+\"): ord(\"\\u0120\"),\n","    ord(\"þ\"): ord(\"\\u0120\"),\n","    ord(\"‘\"): ord(\"'\"),\n","    ord(\"é\"): ord(\"\\u0120\"),\n","    ord(\"º\"): ord(\"\\u0120\"),\n","    ord(\"ë\"): ord(\"\\u0120\"),\n","    ord(\"…\"): ord(\"\\u0120\"),\n","    ord(\"б\"): ord(\"\\u0120\"),\n","    ord(\"*\"): ord(\"\\u0120\"),\n","    ord(\"=\"): ord(\"\\u0120\"),\n","    ord(\"`\"): ord(\"'\"),\n","    ord(\"ć\"): ord(\"\\u0120\"),\n","    ord(\"û\"): ord(\"\\u0120\"),\n","    ord(\"Ý\"): ord(\"\\u0120\"),\n","    ord(\"г\"): ord(\"\\u0120\"),\n","    ord(\"—\"): ord(\"\\u0120\"),\n","    ord(\"\\\\\"): ord(\"\\u0120\"),\n","    ord(\"°\"): ord(\"\\u0120\"),\n","    ord(\"@\"): ord(\"\\u0120\"),\n","    ord(\"č\"): ord(\"\\u0120\"),\n","    ord(\"ó\"): ord(\"\\u0120\"),\n","    ord(\"š\"): ord(\"\\u0120\"),\n","    ord(\"р\"): ord(\"\\u0120\"),\n","    ord(\"_\"): ord(\"\\u0120\"),\n","    ord(\"{\"): ord(\"\\u0120\"),\n","    ord(\"#\"): ord(\"\\u0120\"),\n","    ord(\"^\"): ord(\"\\u0120\"),\n","    ord(\">\"): ord(\"\\u0120\"),\n","    ord(\"$\"): ord(\"\\u0120\"),\n","    ord(\"đ\"): ord(\"\\u0120\"),\n","    ord(\"í\"): ord(\"\\u0120\"),\n","    ord(\"ø\"): ord(\"\\u0120\"),\n","    ord(\"Š\"): ord(\"\\u0120\"),\n","    ord(\"ž\"): ord(\"\\u0120\"),\n","    ord(\"Þ\"): ord(\"\\u0120\"),\n","    ord(\"л\"): ord(\"\\u0120\"),\n","    ord(\"я\"): ord(\"\\u0120\"),\n","    ord(\"}\"): ord(\"\\u0120\"),\n","    ord(\"±\"): ord(\"\\u0120\"),\n","    ord(\"½\"): ord(\"\\u0120\"),\n","    ord(\"¾\"): ord(\"\\u0120\"),\n","    ord(\"ª\"): ord(\"\\u0120\"),\n","    ord(\"á\"): ord(\"\\u0120\"),\n","    ord(\"É\"): ord(\"\\u0120\"),\n","    ord(\"è\"): ord(\"\\u0120\"),\n","    ord(\"Ğ\"): ord(\"\\u0120\"),\n","    ord(\"ï\"): ord(\"\\u0120\"),\n","    ord(\"ñ\"): ord(\"\\u0120\"),\n","    ord(\"ý\"): ord(\"\\u0120\"),\n","    ord(\"Ž\"): ord(\"\\u0120\"),\n","    ord(\"μ\"): ord(\"\\u0120\"),\n","    ord(\"а\"): ord(\"\\u0120\"),\n","    ord(\"и\"): ord(\"\\u0120\"),\n","    ord(\"у\"): ord(\"\\u0120\"),\n","    ord(\"х\"): ord(\"\\u0120\"),\n","    ord(\"ъ\"): ord(\"\\u0120\"),\n","}\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RJAevV6XnuW1","executionInfo":{"status":"ok","timestamp":1715719139533,"user_tz":240,"elapsed":6,"user":{"displayName":"Matthew Johnson","userId":"09672185346885346735"}}},"outputs":[],"source":["class DatasetHolder:\n","\n","    def __init__(self):\n","        self.unknown_vocabulary_type = None\n","        self.padding_vocabulary_type = None\n","        self.end_of_sequence_type = None\n","        self.target_vocab = None\n","        self.target_vocab_array = None\n","        self.target_vocab_counts = None\n","        self.source_vocab = None\n","        self.source_vocab_array = None\n","        self.source_vocab_counts = None\n","        self.target_encodings = None\n","        self.target_encodings_train = None\n","        self.target_encodings_test = None\n","        self.source_encodings = None\n","        self.source_encodings_train = None\n","        self.source_encodings_test = None\n","        self.max_src_seq_obs = 0\n","        self.max_tgt_seq_obs = 0\n","\n","    def get_unknown_vocabulary_type(self):\n","        return self.unknown_vocabulary_type\n","\n","    def set_unknown_vocabulary_type(self, unknown_vocabulary_type):\n","        self.unknown_vocabulary_type = unknown_vocabulary_type\n","\n","    def get_padding_vocabulary_type(self):\n","        return self.padding_vocabulary_type\n","\n","    def set_padding_vocabulary_type(self, padding_vocabulary_type):\n","        self.padding_vocabulary_type = padding_vocabulary_type\n","\n","    def get_end_of_sequence_vocabulary_type(self):\n","        return self.end_of_sequence_type\n","\n","    def set_end_of_sequence_vocabulary_type(self, end_of_sequence_type):\n","        self.end_of_sequence_type = end_of_sequence_type\n","\n","    def get_target_vocab(self):\n","        return self.target_vocab\n","\n","    def set_target_vocab(self, target_vocab):\n","        self.target_vocab = target_vocab\n","\n","    def get_target_vocab_numpy(self):\n","        if self.target_vocab_array is None:\n","            self.target_vocab_array = np.array(self.target_vocab)\n","        return self.target_vocab_array\n","\n","    def get_target_vocab_counts(self):\n","        return self.target_vocab_counts\n","\n","    def set_target_vocab_counts(self, target_vocab_counts):\n","        self.target_vocab_counts = target_vocab_counts\n","\n","    def get_source_vocab(self):\n","        return self.source_vocab\n","\n","    def set_source_vocab(self, source_vocab):\n","        self.source_vocab = source_vocab\n","\n","    def get_source_vocab_numpy(self):\n","        if self.source_vocab_array is None:\n","            self.source_vocab_array = np.array(self.source_vocab)\n","        return self.source_vocab_array\n","\n","    def get_source_vocab_counts(self):\n","        return self.source_vocab_counts\n","\n","    def set_source_vocab_counts(self, source_vocab_counts):\n","        self.source_vocab_counts = source_vocab_counts\n","\n","    def get_target_encodings(self):\n","        return self.target_encodings\n","\n","    def set_target_encodings(self, target_encodings):\n","        del self.target_encodings\n","        self.target_encodings = target_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings(self):\n","        return self.source_encodings\n","\n","    def set_source_encodings(self, source_encodings):\n","        del self.source_encodings\n","        self.source_encodings = source_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_train(self):\n","        return self.target_encodings_train\n","\n","    def set_target_encodings_train(self, target_encodings_train):\n","        del self.target_encodings_train\n","        self.target_encodings_train = target_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_train(self):\n","        return self.source_encodings_train\n","\n","    def set_source_encodings_train(self, source_encodings_train):\n","        del self.source_encodings_train\n","        self.source_encodings_train = source_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_test(self):\n","        return self.target_encodings_test\n","\n","    def set_target_encodings_test(self, target_encodings_test):\n","        del self.target_encodings_test\n","        self.target_encodings_test = target_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_test(self):\n","        return self.source_encodings_test\n","\n","    def set_source_encodings_test(self, source_encodings_test):\n","        del self.source_encodings_test\n","        self.source_encodings_test = source_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_max_src_seq_obs(self):\n","        return self.max_src_seq_obs\n","\n","    def set_max_src_seq_obs(self, max_src_seq_obs):\n","        self.max_src_seq_obs = max_src_seq_obs\n","\n","    def get_max_tgt_seq_obs(self):\n","        return self.max_tgt_seq_obs\n","\n","    def set_max_tgt_seq_obs(self, max_tgt_seq_obs):\n","        self.max_tgt_seq_obs = max_tgt_seq_obs\n","\n","\n","# class name matches file name\n","class dataset_transformer_setimesbyt5():\n","\n","    def __init__(self,\n","                 datasets_directory=root_filepath+\"resources\",\n","                 raw_dataset_directory=\"raw_datasets/setimes\",\n","                 parsed_dataset_directory=\"parsed_datasets/setimes\",\n","                 ids_filename='SETIMES.en-tr.ids',\n","                 en_filename='SETIMES.en-tr.en',\n","                 tr_filename='SETIMES.en-tr.tr',\n","                 dataset_hyperparameters=None):\n","        self.datasets_directory = datasets_directory\n","        self.raw_dataset_directory = raw_dataset_directory\n","        self.parsed_dataset_directory = parsed_dataset_directory\n","        self.ids_filename = ids_filename\n","        self.en_filename = en_filename\n","        self.tr_filename = tr_filename\n","        self.parsed_dataset_filename = None\n","        if 'parsed_dataset_filename' in dataset_hyperparameters:\n","            self.parsed_dataset_filename = dataset_hyperparameters['parsed_dataset_filename']\n","        self.sentence_length_min_percentile = None\n","        if 'sentence_length_min_percentile' in dataset_hyperparameters:\n","            self.sentence_length_min_percentile = dataset_hyperparameters['sentence_length_min_percentile']\n","        self.sentence_length_max_percentile = None\n","        if 'sentence_length_max_percentile' in dataset_hyperparameters:\n","            self.sentence_length_max_percentile = dataset_hyperparameters['sentence_length_max_percentile']\n","\n","\n","    def read_dataset(self):\n","        dataset_holder = None\n","        if self.parsed_dataset_filename is not None:\n","            dataset_holder = torch.load(self.datasets_directory + \"/\"\n","                                        + self.parsed_dataset_directory + \"/\"\n","                                        + self.parsed_dataset_filename)\n","        else:\n","            target_sentences = list()\n","            source_sentences = list()\n","            index_file = open(self.datasets_directory + \"/\" + self.raw_dataset_directory + \"/\" + self.ids_filename)\n","            en_file = open(self.datasets_directory + \"/\" + self.raw_dataset_directory + \"/\" + self.en_filename)\n","            tr_file = open(self.datasets_directory + \"/\" + self.raw_dataset_directory + \"/\" + self.tr_filename)\n","            indices = list()\n","            en_sentences = list()\n","            tr_sentences = list()\n","            line_number = 1\n","            for line in index_file:\n","                line_segments = line.strip().split()\n","                if len(line_segments) != 4:\n","                    print(\"Line segmentation error on line \" + str(line_number))\n","                    print(\"Content: \" + line)\n","                    continue\n","                if line_segments[0].startswith(\"en\") and line_segments[1].startswith(\"tr\"):\n","                    indices.append((int(line_segments[2]), int(line_segments[3])))\n","                elif line_segments[0].startswith(\"tr\") and line_segments[1].startswith(\"en\"):\n","                    indices.append((int(line_segments[3]), int(line_segments[2])))\n","                else:\n","                    print(\"Index parsing error on line \" + str(line_number))\n","                    print(\"Content: \" + line)\n","                    continue\n","                line_number = line_number + 1\n","            for line in en_file:\n","                en_sentences.append(line.strip())\n","            for line in tr_file:\n","                tr_sentences.append(line.strip())\n","            for index in indices:\n","                target_sentences.append(en_sentences[index[0] - 1])\n","                source_sentences.append(tr_sentences[index[1] - 1])\n","            target_sentence_lengths = list()\n","            for sentence in target_sentences:\n","                target_sentence_lengths.append(len(sentence))\n","            source_sentence_lengths = list()\n","            for sentence in source_sentences:\n","                source_sentence_lengths.append(len(sentence))\n","            target_sentences_length_limited = list()\n","            source_sentences_length_limited = list()\n","            target_min_len = int(np.percentile(sorted(target_sentence_lengths), self.sentence_length_min_percentile))\n","            target_max_len = int(np.percentile(sorted(target_sentence_lengths), self.sentence_length_max_percentile))\n","            source_min_len = int(np.percentile(sorted(source_sentence_lengths), self.sentence_length_min_percentile))\n","            source_max_len = int(np.percentile(sorted(source_sentence_lengths), self.sentence_length_max_percentile))\n","            max_src_seq_obs = 0\n","            max_tgt_seq_obs = 0\n","            for i in range(0, len(target_sentences)):\n","                if (len(target_sentences[i]) > target_min_len and len(target_sentences[i]) <= target_max_len\n","                        and len(source_sentences[i]) > source_min_len and len(source_sentences[i]) <= source_max_len):\n","                    if len(source_sentences[i]) > max_src_seq_obs:\n","                        max_src_seq_obs = len(source_sentences[i])\n","                    if len(target_sentences[i]) > max_tgt_seq_obs:\n","                        max_tgt_seq_obs = len(target_sentences[i])\n","                    target_sentences_length_limited.append(target_sentences[i].translate(english_char_mappings))\n","                    source_sentences_length_limited.append(source_sentences[i].translate(turkish_char_mappings))\n","            dataset_holder = DatasetHolder()\n","            dataset_holder.set_max_src_seq_obs(max_src_seq_obs)\n","            dataset_holder.set_max_tgt_seq_obs(max_tgt_seq_obs)\n","            # encode to Pytorch tensors as raw UTF-8 character vocabulary\n","            # method replicated from Xue 2021 - ByT5 - Introduction, sec 3.1\n","            unknown_vocabulary_type = \"\\u0120\".encode('utf-8').decode('utf-8')\n","            padding_vocabulary_type = \"\\u0121\".encode('utf-8').decode('utf-8')\n","            end_of_sequence_vocabulary_type = \"\\u0122\".encode('utf-8').decode('utf-8')\n","            dataset_holder.set_unknown_vocabulary_type(unknown_vocabulary_type)\n","            dataset_holder.set_padding_vocabulary_type(padding_vocabulary_type)\n","            dataset_holder.set_end_of_sequence_vocabulary_type(end_of_sequence_vocabulary_type)\n","            target_vocab = list([unknown_vocabulary_type, padding_vocabulary_type, end_of_sequence_vocabulary_type])\n","            source_vocab = list([unknown_vocabulary_type, padding_vocabulary_type, end_of_sequence_vocabulary_type])\n","            target_encodings = list()\n","            source_encodings = list()\n","            for entry in target_sentences_length_limited:\n","                encoding = list()\n","                for character in entry:\n","                    if character not in target_vocab:\n","                        target_vocab.append(character)\n","                    encoding.append(target_vocab.index(character))\n","                encoding.append(target_vocab.index(end_of_sequence_vocabulary_type))\n","                target_encodings.append(torch.tensor(encoding))\n","            for entry in source_sentences_length_limited:\n","                encoding = list()\n","                for character in entry:\n","                    if character not in source_vocab:\n","                        source_vocab.append(character)\n","                    encoding.append(source_vocab.index(character))\n","                encoding.append(target_vocab.index(end_of_sequence_vocabulary_type))\n","                source_encodings.append(torch.tensor(encoding))\n","            # fix vocabulary indices using tuple type\n","            dataset_holder.set_target_vocab(tuple(target_vocab))\n","            dataset_holder.set_target_encodings(target_encodings)\n","            dataset_holder.set_source_vocab(tuple(source_vocab))\n","            dataset_holder.set_source_encodings(source_encodings)\n","            dataset_holder = DatasetUtils.create_dataset_segments(dataset_holder)\n","            torch.save(dataset_holder,\n","                       self.datasets_directory + \"/\" +\n","                       self.parsed_dataset_directory + \"/\" +\n","                       \"setimes_parsed-\" + str(int(time.time())))\n","        return dataset_holder\n","\n","\n","class Utils:\n","\n","    def __init__(self):\n","        pass\n","\n","    @staticmethod\n","    def load_python_object(object_path: str, object_attribute: str):\n","        path_segments = object_path.split('.')\n","        module = __import__(object_path)\n","        for segment in path_segments[1:]:\n","            module = getattr(module, segment)\n","        return getattr(module, object_attribute)\n","\n","\n","class DatasetUtils:\n","\n","    @staticmethod\n","    def shuffle_dataset(dataset_holder: DatasetHolder):\n","        new_source_encodings, new_target_encodings = (\n","            DatasetUtils.shuffle_lists(\n","                dataset_holder.get_source_encodings(),\n","                dataset_holder.get_target_encodings()\n","            )\n","        )\n","        dataset_holder.set_source_encodings(new_source_encodings)\n","        dataset_holder.set_target_encodings(new_target_encodings)\n","        return dataset_holder\n","\n","    @staticmethod\n","    def shuffle_training_dataset(dataset_holder: DatasetHolder):\n","        new_source_encodings, new_target_encodings = (\n","            DatasetUtils.shuffle_lists(\n","                dataset_holder.get_source_encodings_train(),\n","                dataset_holder.get_target_encodings_train()\n","            )\n","        )\n","        dataset_holder.set_source_encodings(new_source_encodings)\n","        dataset_holder.set_target_encodings(new_target_encodings)\n","        return dataset_holder\n","\n","    @staticmethod\n","    def shuffle_test_dataset(dataset_holder: DatasetHolder):\n","        new_source_encodings, new_target_encodings = (\n","            DatasetUtils.shuffle_lists(\n","                dataset_holder.get_source_encodings_test(),\n","                dataset_holder.get_target_encodings_test()\n","            )\n","        )\n","        dataset_holder.set_source_encodings(new_source_encodings)\n","        dataset_holder.set_target_encodings(new_target_encodings)\n","        return dataset_holder\n","\n","    @staticmethod\n","    def shuffle_lists(source_list, target_list):\n","        assert len(source_list) == len(target_list)\n","        list_element_shuffle_indices = list(range(0, len(source_list)))\n","        random.shuffle(list_element_shuffle_indices)\n","        new_source_list = list()\n","        new_target_list = list()\n","        for i in list_element_shuffle_indices:\n","            new_source_list.append(source_list[i])\n","            new_target_list.append(target_list[i])\n","        assert (len(new_source_list) == len(new_target_list)\n","                == len(source_list) == len(target_list))\n","        return new_source_list, new_target_list\n","\n","    @staticmethod\n","    def create_dataset_segments(dataset_holder: DatasetHolder):\n","        split_with_even_target_distribution = False\n","        iteration = 1\n","        best_split_target_encodings = None\n","        best_split_source_encodings = None\n","        best_split_deviation_from_desired = 1\n","        segments = 20\n","        split_size = len(dataset_holder.get_target_encodings()) // segments\n","        train_size = split_size * (segments - 1)\n","        while not split_with_even_target_distribution and iteration <= 100:\n","            segment_attempt_start = time.time()\n","            dataset_holder = DatasetUtils.shuffle_dataset(dataset_holder)\n","            target_encodings = dataset_holder.get_target_encodings()\n","            source_encodings = dataset_holder.get_source_encodings()\n","            train_set_target_enc = target_encodings[0:train_size]\n","            test_set_target_enc = target_encodings[train_size:]\n","            numpy_encodings = list()\n","            for encoding in train_set_target_enc:\n","                numpy_encodings.append(encoding.flatten().numpy())\n","            train_set_target_enc_cts = np.bincount(\n","                np.concatenate([\n","                    np.concatenate(numpy_encodings),\n","                    np.arange(0, 170)\n","                ])\n","            )\n","            numpy_encodings = list()\n","            for encoding in test_set_target_enc:\n","                numpy_encodings.append(encoding.flatten().numpy())\n","            test_set_target_enc_cts = np.bincount(\n","                np.concatenate([\n","                    np.concatenate(numpy_encodings),\n","                    np.arange(0, 170)\n","                ])\n","            )\n","            # terms with probability ~ 1%\n","            total_5 = train_set_target_enc_cts[5] + test_set_target_enc_cts[5]\n","            total_40 = train_set_target_enc_cts[40] + test_set_target_enc_cts[40]\n","            total_42 = train_set_target_enc_cts[42] + test_set_target_enc_cts[42]\n","            # top 3 terms\n","            total_7 = train_set_target_enc_cts[7] + test_set_target_enc_cts[7]\n","            total_15 = train_set_target_enc_cts[15] + test_set_target_enc_cts[15]\n","            total_12 = train_set_target_enc_cts[12] + test_set_target_enc_cts[12]\n","            train_dist_goal = (segments - 1)/segments\n","            test_dist_goal = (1 / segments)\n","            deviation_from_desired = (\n","                    np.abs(((segments - 2)/segments) - (train_set_target_enc_cts[5] / total_5)) +\n","                    np.abs(train_dist_goal - (train_set_target_enc_cts[40] / total_40)) +\n","                    np.abs(train_dist_goal - (train_set_target_enc_cts[42] / total_42)) +\n","                    np.abs(train_dist_goal - (train_set_target_enc_cts[7] / total_7)) +\n","                    np.abs(train_dist_goal - (train_set_target_enc_cts[15] / total_15)) +\n","                    np.abs(train_dist_goal - (train_set_target_enc_cts[12] / total_12)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[5] / total_5)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[40] / total_40)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[42] / total_42)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[7] / total_7)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[15] / total_15)) +\n","                    np.abs(test_dist_goal - (test_set_target_enc_cts[12] / total_12))\n","            )\n","            if deviation_from_desired <= 12 * 0.0001:\n","                split_with_even_target_distribution = True\n","                best_split_target_encodings = target_encodings\n","                best_split_source_encodings = source_encodings\n","                print(f\"Found dataset split within tolerance for deviation from uniform distribution over characters\")\n","            if deviation_from_desired < best_split_deviation_from_desired:\n","                best_split_target_encodings = target_encodings\n","                best_split_source_encodings = source_encodings\n","                best_split_deviation_from_desired = deviation_from_desired\n","            segment_attempt_end = time.time()\n","            print(f\"Completed data split attempt. \"\n","                  f\"iteration:{iteration} \"\n","                  f\"best_split_deviation_from_desired:{best_split_deviation_from_desired} \"\n","                  f\"time_to_complete_attempt:{segment_attempt_end-segment_attempt_start}\")\n","            iteration = iteration + 1\n","        best_split_source_encodings_train = best_split_source_encodings[0:train_size]\n","        best_split_target_encodings_train = best_split_target_encodings[0:train_size]\n","        assert len(best_split_source_encodings_train) == len(best_split_target_encodings_train)\n","        train_split_encoding_length_sum_and_encoding_index_pairs = list()\n","        for i in range(0, len(best_split_source_encodings_train)):\n","            train_split_encoding_length_sum_and_encoding_index_pairs.append(\n","                list([\n","                    best_split_source_encodings_train[i].shape[0] + best_split_target_encodings_train[i].shape[0],\n","                    np.abs(best_split_source_encodings_train[i].shape[0] - best_split_target_encodings_train[i].shape[0]),\n","                    i\n","                ])\n","            )\n","        train_split_encoding_length_sum_and_encoding_index_pairs = sorted(\n","            train_split_encoding_length_sum_and_encoding_index_pairs,\n","            key=lambda pair: (pair[0], pair[1])\n","        )\n","        train_source_encs_length_sorted = list()\n","        train_target_encs_length_sorted = list()\n","        for encoding_length_sum_and_encoding_index_pair in train_split_encoding_length_sum_and_encoding_index_pairs:\n","            train_source_encs_length_sorted.append(best_split_source_encodings_train[encoding_length_sum_and_encoding_index_pair[2]])\n","            train_target_encs_length_sorted.append(best_split_target_encodings_train[encoding_length_sum_and_encoding_index_pair[2]])\n","        best_split_source_encodings_test = best_split_source_encodings[train_size:]\n","        best_split_target_encodings_test = best_split_target_encodings[train_size:]\n","        assert len(best_split_source_encodings_test) == len(best_split_target_encodings_test)\n","        test_split_encoding_length_sum_and_encoding_index_pairs = list()\n","        for i in range(0, len(best_split_source_encodings_test)):\n","            test_split_encoding_length_sum_and_encoding_index_pairs.append(\n","                list([\n","                    best_split_source_encodings_test[i].shape[0] + best_split_target_encodings_test[i].shape[0],\n","                    np.abs(best_split_source_encodings_test[i].shape[0] - best_split_target_encodings_test[i].shape[0]),\n","                    i\n","                ])\n","            )\n","        test_split_encoding_length_sum_and_encoding_index_pairs = sorted(\n","            test_split_encoding_length_sum_and_encoding_index_pairs,\n","            key=lambda pair: (pair[0], pair[1])\n","        )\n","        test_source_encs_length_sorted = list()\n","        test_target_encs_length_sorted = list()\n","        for encoding_length_sum_and_encoding_index_pair in test_split_encoding_length_sum_and_encoding_index_pairs:\n","            test_source_encs_length_sorted.append(best_split_source_encodings_test[encoding_length_sum_and_encoding_index_pair[2]])\n","            test_target_encs_length_sorted.append(best_split_target_encodings_test[encoding_length_sum_and_encoding_index_pair[2]])\n","        dataset_holder.set_source_encodings(best_split_source_encodings)\n","        dataset_holder.set_target_encodings(best_split_target_encodings)\n","        dataset_holder.set_source_encodings_train(train_source_encs_length_sorted)\n","        dataset_holder.set_target_encodings_train(train_target_encs_length_sorted)\n","        dataset_holder.set_source_encodings_test(test_source_encs_length_sorted)\n","        dataset_holder.set_target_encodings_test(test_target_encs_length_sorted)\n","        return dataset_holder\n","\n","    # use a dedicated padding token to pad batches as in Xue 2021 - ByT5 - Sec 3.1\n","    @staticmethod\n","    def prepare_batches(\n","            source_encodings,\n","            target_encodings,\n","            source_vocab,\n","            target_vocab,\n","            batch_size_limit: int,\n","            element_difference_limit: int,\n","            padding_value):\n","        assert len(source_encodings) == len(target_encodings)\n","        total_elements = len(source_encodings)\n","        source_encodings_batches = list()\n","        target_encodings_batches = list()\n","        source_encodings_tensors = list()\n","        target_encodings_tensors = list()\n","        encodings_index = 0\n","        while encodings_index < total_elements - 1:\n","            batch_size = 0\n","            batch_end_reached = False\n","            min_source_enc_len = source_encodings[encodings_index+batch_size].shape[0]\n","            max_source_enc_len = source_encodings[encodings_index+batch_size].shape[0]\n","            min_target_enc_len = target_encodings[encodings_index+batch_size].shape[0]\n","            max_target_enc_len = target_encodings[encodings_index+batch_size].shape[0]\n","            while not batch_end_reached:\n","                if (max(abs(source_encodings[encodings_index+batch_size].shape[0] - min_source_enc_len),\n","                        abs(source_encodings[encodings_index+batch_size].shape[0] - max_source_enc_len)) > element_difference_limit\n","                        or max(abs(target_encodings[encodings_index+batch_size].shape[0] - min_target_enc_len),\n","                               abs(target_encodings[encodings_index+batch_size].shape[0] - max_target_enc_len)) > element_difference_limit):\n","                    batch_end_reached = True\n","                if batch_size == batch_size_limit - 1:\n","                    batch_end_reached = True\n","                if encodings_index + batch_size + 1 < total_elements:\n","                    batch_size = batch_size + 1\n","                else:\n","                    batch_end_reached = True\n","            max_src_len_for_batch = 0\n","            max_tgt_len_for_batch = 0\n","            for batch_index in range(0, batch_size):\n","                if len(source_encodings[encodings_index+batch_index]) > max_src_len_for_batch:\n","                    max_src_len_for_batch = len(source_encodings[encodings_index+batch_index])\n","                if len(target_encodings[encodings_index+batch_index]) > max_tgt_len_for_batch:\n","                    max_tgt_len_for_batch = len(target_encodings[encodings_index+batch_index])\n","            for batch_index in range(0, batch_size):\n","                source_encoding = source_encodings[encodings_index]\n","                target_encoding = target_encodings[encodings_index]\n","                source_encodings_tensors.append(\n","                    torch.nn.functional.pad(\n","                        source_encoding,\n","                        (0, max_src_len_for_batch - len(source_encoding)),\n","                        value=source_vocab.index(padding_value)\n","                    )\n","                )\n","                target_encodings_tensors.append(\n","                    torch.nn.functional.pad(\n","                        target_encoding,\n","                        (0, max_tgt_len_for_batch - len(target_encoding)),\n","                        value=target_vocab.index(padding_value)\n","                    )\n","                )\n","                encodings_index = encodings_index + 1\n","            if batch_size > 0:\n","                source_batch = torch.stack(source_encodings_tensors)\n","                target_batch = torch.stack(target_encodings_tensors)\n","                source_encodings_batches.append(source_batch)\n","                target_encodings_batches.append(target_batch)\n","            source_encodings_tensors = list()\n","            target_encodings_tensors = list()\n","        target_encodings_batches_with_index = list()\n","        for i in range(0, len(target_encodings_batches)):\n","            target_encodings_batches_with_index.append(list([target_encodings_batches[i], i]))\n","        assert len(target_encodings_batches_with_index) == len(target_encodings_batches)\n","        target_encodings_batches_with_index_sorted = sorted(\n","            target_encodings_batches_with_index,\n","            key=lambda batch_pair: (-batch_pair[0].shape[0], -batch_pair[0].shape[1])\n","        )\n","        source_encodings_batches_sorted = list()\n","        target_encodings_batches_sorted = list()\n","        for i in range(0, len(target_encodings_batches_with_index_sorted)):\n","            source_encodings_batches_sorted.append(\n","                source_encodings_batches[target_encodings_batches_with_index_sorted[i][1]]\n","            )\n","            target_encodings_batches_sorted.append(\n","                target_encodings_batches[target_encodings_batches_with_index_sorted[i][1]]\n","            )\n","        del source_encodings_tensors\n","        del target_encodings_tensors\n","        del source_encodings_batches\n","        del target_encodings_batches\n","        del target_encodings_batches_with_index_sorted\n","        return source_encodings_batches_sorted, target_encodings_batches_sorted\n","\n","    @staticmethod\n","    def prepare_training_batches(\n","            dataset_holder: DatasetHolder,\n","            batch_size_limit: int,\n","            element_difference_limit: int):\n","        source_encodings_batches, target_encodings_batches = DatasetUtils.prepare_batches(\n","            dataset_holder.get_source_encodings_train(),\n","            dataset_holder.get_target_encodings_train(),\n","            dataset_holder.get_source_vocab(),\n","            dataset_holder.get_target_vocab(),\n","            batch_size_limit,\n","            element_difference_limit,\n","            dataset_holder.get_padding_vocabulary_type()\n","        )\n","\n","        source_vocab_counts = {}\n","        for source_encoding_batch in source_encodings_batches:\n","            for source_encoding in source_encoding_batch:\n","                for character in source_encoding:\n","                    if character.item() not in source_vocab_counts:\n","                        source_vocab_counts[character.item()] = 0\n","                    source_vocab_counts[character.item()] = source_vocab_counts[character.item()] + 1\n","        target_vocab_counts = {}\n","        for target_vocab_batch in target_encodings_batches:\n","            for target_encoding in target_vocab_batch:\n","                for character in target_encoding:\n","                    if character.item() not in target_vocab_counts:\n","                        target_vocab_counts[character.item()] = 0\n","                    target_vocab_counts[character.item()] = target_vocab_counts[character.item()] + 1\n","        dataset_holder.set_source_vocab_counts(source_vocab_counts)\n","        dataset_holder.set_target_vocab_counts(target_vocab_counts)\n","        return source_encodings_batches, target_encodings_batches\n","\n","    @staticmethod\n","    def decode_target_tensor(dataset_holder: DatasetHolder, tensor_to_decode):\n","        vocab = dataset_holder.get_target_vocab_numpy()\n","        decoded_tensor = np.take(vocab, tensor_to_decode.detach().to(device=\"cpu\").flatten().numpy())\n","        return \"\".join(decoded_tensor.tolist())\n","\n","    @staticmethod\n","    def decode_source_tensor(dataset_holder: DatasetHolder, tensor_to_decode):\n","        vocab = dataset_holder.get_source_vocab_numpy()\n","        decoded_tensor = np.take(vocab, tensor_to_decode.detach().to(device=\"cpu\").flatten().numpy())\n","        return \"\".join(decoded_tensor.tolist())\n","\n","\n","# class name matches file name\n","class transformer_vaswani2017(torch.nn.Transformer):\n","\n","    def __init__(self,\n","                 model_hyperparameters):\n","        super().__init__(\n","            d_model=model_hyperparameters['d_model'],\n","            nhead=model_hyperparameters['nhead'],\n","            num_encoder_layers=model_hyperparameters['num_encoder_layers'],\n","            num_decoder_layers=model_hyperparameters['num_decoder_layers'],\n","            dim_feedforward=model_hyperparameters['dim_feedforward'],\n","            dropout=model_hyperparameters['dropout'],\n","            activation=model_hyperparameters['activation'],\n","            custom_encoder=model_hyperparameters['custom_encoder'],\n","            custom_decoder=model_hyperparameters['custom_decoder'],\n","            layer_norm_eps=model_hyperparameters['layer_norm_eps'],\n","            batch_first=model_hyperparameters['batch_first'],\n","            norm_first=model_hyperparameters['norm_first'],\n","            bias=model_hyperparameters['bias'],\n","            device=model_hyperparameters['device'],\n","            dtype=model_hyperparameters['dtype']\n","        )\n","        # add one for the end of sequence token\n","        self.max_src_seq_len = model_hyperparameters['max_src_seq_len'] + 1\n","        self.max_tgt_seq_len = model_hyperparameters['max_tgt_seq_len'] + 1\n","        self.src_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['src_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.tgt_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['tgt_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.src_pos_enc = torch.nn.Embedding(\n","            self.max_src_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.tgt_pos_enc = torch.nn.Embedding(\n","            self.max_tgt_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.linear_output_projection_1 = torch.nn.Linear(\n","            self.max_tgt_seq_len,\n","            1,\n","            bias=False\n","        )\n","        self.linear_output_projection_2 = torch.nn.Linear(\n","            model_hyperparameters['d_model'],\n","            model_hyperparameters['tgt_vocab_size'],\n","            bias=False\n","        )\n","        self.logsoftmax_output = torch.nn.LogSoftmax(dim=1)\n","        self.model_hyperparameters = model_hyperparameters\n","        self.tgt_mask_cache = {}\n","        for i in range(1, self.max_tgt_seq_len + 1):\n","            tgt_mask = self.generate_square_subsequent_mask(i)\n","            if is_remote_execution:\n","                self.tgt_mask_cache[i] = tgt_mask.to(device=\"cuda\")\n","            self.tgt_mask_cache[i] = tgt_mask\n","        self.indices_cache = {}\n","        for i in range(1, max([self.max_src_seq_len, self.max_tgt_seq_len]) + 1):\n","            indices = torch.tensor(np.arange(0, i), dtype=torch.long)\n","            if is_remote_execution:\n","                indices = indices.to(device=\"cuda\")\n","            self.indices_cache[i] = indices\n","\n","    def forward(self,\n","                src: torch.Tensor,\n","                tgt: torch.Tensor,\n","                src_mask: Optional[torch.Tensor] = None,\n","                tgt_mask: Optional[torch.Tensor] = None,\n","                memory_mask: Optional[torch.Tensor] = None,\n","                src_key_padding_mask: Optional[torch.Tensor] = None,\n","                tgt_key_padding_mask: Optional[torch.Tensor] = None,\n","                memory_key_padding_mask: Optional[torch.Tensor] = None,\n","                src_is_causal: Optional[bool] = None,\n","                tgt_is_causal: Optional[bool] = True,\n","                memory_is_causal: bool = False) -> torch.Tensor:\n","        src_embedding_pos_enc = self.src_embeddings(src) + self.src_pos_enc(self.indices_cache[src.shape[1]])\n","        tgt_embedding_pos_enc = self.tgt_embeddings(tgt) + self.tgt_pos_enc(self.indices_cache[tgt.shape[1]])\n","        tgt_mask = self.tgt_mask_cache[tgt.shape[1]]\n","        transformer_output = super().forward(src_embedding_pos_enc, tgt_embedding_pos_enc, src_mask,\n","                                             tgt_mask, memory_mask, src_key_padding_mask,\n","                                             tgt_key_padding_mask, memory_key_padding_mask,\n","                                             src_is_causal, tgt_is_causal, memory_is_causal)\n","        transformer_output = torch.swapaxes(transformer_output, -1, -2)\n","        transformer_output = torch.nn.functional.pad(\n","            transformer_output,\n","            (0, self.max_tgt_seq_len - transformer_output.shape[2]),\n","            value=0\n","        )\n","        output = self.logsoftmax_output(\n","            self.linear_output_projection_2(\n","                torch.squeeze(\n","                    self.linear_output_projection_1(\n","                        transformer_output\n","                    ),\n","                    dim=2\n","                )\n","            )\n","        )\n","        del src_embedding_pos_enc\n","        del tgt_embedding_pos_enc\n","        del transformer_output\n","        return output\n","\n","\n","class model_trainer_kocmi2018():\n","\n","    def __init__(self,\n","                 trainer_hyperparameters=None,\n","                 model_parameter_directory=None,\n","                 trainer_parameter_directory=None,\n","                 runner_hyperparameters_name=None,\n","                 latest_param_filename_tag=None):\n","        self.trainer_hyperparameters = trainer_hyperparameters\n","        self.optimizer_name = self.trainer_hyperparameters['optimizer_name']\n","        self.initial_lr = self.trainer_hyperparameters['initial_lr']\n","        self.exp_decay = self.trainer_hyperparameters['exp_decay']\n","        self.lr_scheduler_name = self.trainer_hyperparameters['lr_scheduler_name']\n","        self.epochs = self.trainer_hyperparameters['epochs']\n","        self.epoch_starting_index = self.trainer_hyperparameters['epoch_starting_index']\n","        self.batch_size_limit = self.trainer_hyperparameters['batch_size_limit']\n","        self.element_difference_limit = self.trainer_hyperparameters['element_difference_limit']\n","        self.batch_starting_index = self.trainer_hyperparameters['batch_starting_index']\n","        self.model_parameter_directory = model_parameter_directory\n","        self.trainer_parameter_directory = trainer_parameter_directory\n","        self.runner_hyperparameters_name = runner_hyperparameters_name\n","        self.latest_param_filename_tag = latest_param_filename_tag\n","        self.dataset_holder = None\n","        self.model = None\n","        self.source_encoding_batches = None\n","        self.target_encoding_batches = None\n","        self.optimizer = None\n","        self.lr_scheduler = None\n","        self.loss_fcn = None\n","\n","    def init_trainer(self):\n","        self.source_encoding_batches, self.target_encoding_batches = (\n","            DatasetUtils.prepare_training_batches(\n","                self.dataset_holder,\n","                self.batch_size_limit,\n","                self.element_difference_limit\n","            )\n","        )\n","        # get_target_vocab_counts requires that training batches have been prepared\n","        # this ensures that vocab counts include padding and eos tokens\n","        loss_weights = list()\n","        for vocab_term in self.dataset_holder.get_target_vocab():\n","            loss_weights.append(\n","                1 / self.dataset_holder.get_target_vocab_counts()[\n","                    self.dataset_holder.get_target_vocab().index(vocab_term)\n","                ]\n","            )\n","        # set padding to have 0 weight\n","        loss_weights[\n","            self.dataset_holder.get_target_vocab().index(self.dataset_holder.get_padding_vocabulary_type())] = 0\n","        loss_weights = torch.tensor(loss_weights, dtype=torch.float)\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","            self.model.cuda()\n","            loss_weights = loss_weights.to(device=\"cuda\")\n","        self.loss_fcn = torch.nn.NLLLoss(weight=loss_weights)\n","        _optimizer_class_ = Utils.load_python_object('torch.optim', self.optimizer_name)\n","        self.optimizer = _optimizer_class_(self.model.parameters(), lr=self.initial_lr)\n","        _lr_scheduler_class_ = Utils.load_python_object('torch.optim.lr_scheduler', self.lr_scheduler_name)\n","        # constructor call assumes that the scheduler is the ExponentialLR scheduler\n","        self.lr_scheduler = _lr_scheduler_class_(self.optimizer, self.exp_decay)\n","        scheduler_parameter_filepath = self.trainer_parameter_directory + \"/\" + self.runner_hyperparameters_name + \"-\" + self.latest_param_filename_tag + \"-scheduler.params\"\n","        if os.path.exists(scheduler_parameter_filepath):\n","            scheduler_parameters = torch.load(scheduler_parameter_filepath)\n","            self.lr_scheduler.load_state_dict(scheduler_parameters)\n","        parameter_count = 0\n","        bytes_consumed = 0\n","        for parameter in self.model.parameters():\n","            if parameter.requires_grad:\n","                parameter_count = parameter_count + np.prod(parameter.data.shape)\n","                bytes_consumed = bytes_consumed + parameter.data.nbytes\n","        gb_consumed = bytes_consumed / 1024 / 1024 / 1024\n","        print(f\"Model trainer initialization complete.\"\n","              f\"Trainer will run on model with parameter count {parameter_count} \"\n","              f\"and parameter memory use {gb_consumed} GB\")\n","\n","    # pretraining is not used for monolingual english as described in Xue 2021 - ByT5 - Sec 3.1\n","    def run_trainer(self):\n","        assert self.epoch_starting_index < self.epochs\n","        for i in range(self.epoch_starting_index, self.epochs):\n","            while self.lr_scheduler.get_last_lr() > (self.initial_lr) * (self.exp_decay) ** i:\n","                self.lr_scheduler.step()\n","            epoch_start = time.time()\n","            print(f\"Beginning epoch {i+1} of {self.epochs} with scheduler {self.lr_scheduler.state_dict()}\")\n","            # if i > 0:\n","            #     self.source_encoding_batches, self.target_encoding_batches = DatasetUtils.shuffle_lists(\n","            #         self.source_encoding_batches, self.target_encoding_batches\n","            #     )\n","            source_batches = None\n","            target_batches = None\n","            if is_remote_execution:\n","                source_batches = list()\n","                target_batches = list()\n","                for batch in self.source_encoding_batches:\n","                    source_batches.append(batch.to(device=\"cuda\"))\n","                    del batch\n","                for batch in self.target_encoding_batches:\n","                    target_batches.append(batch.to(device=\"cuda\"))\n","                    del batch\n","                torch.cuda.empty_cache()\n","            else:\n","                source_batches = self.source_encoding_batches\n","                target_batches = self.target_encoding_batches\n","            assert len(source_batches) == len(target_batches)\n","            batch_ct = len(source_batches)\n","            batch_size = source_batches[0].shape[0]\n","            samples_passed = 0\n","            last_log = 0\n","            last_loss = 0\n","            note_step_prediction = False\n","            step_prediction_at_percentage_of_sample = 0\n","            total_batch_time = 0\n","            assert self.batch_starting_index < batch_ct\n","            for j in range(self.batch_starting_index, batch_ct):\n","                batch_start = time.time()\n","                batch_sequence_length = target_batches[j].shape[1]\n","                step_prediction_step_number = int(batch_sequence_length * step_prediction_at_percentage_of_sample)\n","                print(f\"Starting batch.\")\n","                print(f\"epoch:{i+1}/{self.epochs} batch:{j+1}/{batch_ct} batch_size:{target_batches[j].shape[0]}\")\n","                for k in range(1, batch_sequence_length-1):\n","                    target_batch_slices = torch.tensor_split(target_batches[j], [k], dim=1)\n","                    self.model.zero_grad()\n","                    output_logits = self.model.forward(\n","                        source_batches[j],\n","                        target_batch_slices[0]\n","                    )\n","                    next_word_indices = target_batch_slices[1][:, 0]\n","                    last_loss = self.loss_fcn(output_logits, next_word_indices)\n","                    last_loss.backward()\n","                    self.optimizer.step()\n","                    if note_step_prediction and k == step_prediction_step_number:\n","                        note_step_prediction = False\n","                        full_sequence = DatasetUtils.decode_target_tensor(self.dataset_holder, target_batches[j][0])\n","                        prefix_sequence = DatasetUtils.decode_target_tensor(\n","                            self.dataset_holder,\n","                            target_batch_slices[0][0]\n","                        )\n","                        next_token = DatasetUtils.decode_target_tensor(\n","                            self.dataset_holder,\n","                            next_word_indices[0]\n","                        )\n","                        predicted_token = DatasetUtils.decode_target_tensor(\n","                            self.dataset_holder,\n","                            torch.argmax(output_logits[0])\n","                        )\n","                        print(f\"Next token prediction. step:{k}/{batch_sequence_length} \"\n","                              f\"batch:{j+1}/{batch_ct} epoch:{i+1}/{self.epochs}\")\n","                        print(f\"full seq: {full_sequence}\")\n","                        print(f\"pref seq: {prefix_sequence}\")\n","                        print(f\"next tok: {next_token.rjust(k, ' ')}\")\n","                        print(f\"pred tok: {predicted_token.rjust(k, ' ')}\")\n","                        del full_sequence\n","                        del prefix_sequence\n","                        del next_token\n","                        del predicted_token\n","                    del target_batch_slices\n","                    del output_logits\n","                    del next_word_indices\n","                    last_loss = last_loss.detach()\n","                    gc.collect()\n","                    if is_remote_execution:\n","                        torch.cuda.empty_cache()\n","                batch_end = time.time()\n","                batch_time = batch_end - batch_start\n","                total_batch_time = total_batch_time + batch_time\n","                print(f\"Completed batch.\")\n","                print(f\"epoch:{i+1}/{self.epochs} batch:{j+1}/{batch_ct} batch_size:{target_batches[j].shape[0]} loss:{last_loss} \"\n","                      f\"time_for_batch_instance:{batch_time} total_batch_time:{total_batch_time} running_batch_average:{total_batch_time/(j+1)}\")\n","                samples_passed = samples_passed + batch_size\n","                if samples_passed - last_log > 100:\n","                    last_log = samples_passed\n","                    note_step_prediction = True\n","                    step_prediction_at_percentage_of_sample = random.random()\n","                    if is_remote_execution:\n","                        print(f\"Memory usage summary:\")\n","                        print(f\"{torch.cuda.memory_summary()}\")\n","                        torch.cuda.reset_max_memory_allocated()\n","                        torch.cuda.reset_max_memory_cached()\n","                        torch.cuda.reset_peak_memory_stats()\n","                    param_filename_tag = str(int(time.time()))\n","                    torch.save(\n","                        self.model.state_dict(),\n","                        self.model_parameter_directory + \"/\" + self.runner_hyperparameters_name + \"-\" + param_filename_tag + \"-model.params\"\n","                    )\n","                    torch.save(\n","                        self.lr_scheduler.state_dict(),\n","                        self.trainer_parameter_directory + \"/\" + self.runner_hyperparameters_name + \"-\" + param_filename_tag + \"-scheduler.params\"\n","                    )\n","                    torch.save(\n","                        f\"epoch:{i+1}/{self.epochs} batch:{j+1}/{batch_ct}\",\n","                        self.trainer_parameter_directory + \"/\" + self.runner_hyperparameters_name + \"-\" + param_filename_tag + \"-trainer.params\"\n","                    )\n","            del source_batches\n","            del target_batches\n","            gc.collect()\n","            if is_remote_execution:\n","                torch.cuda.empty_cache()\n","            self.lr_scheduler.step()\n","            epoch_end = time.time()\n","            print(f\"Completed epoch {i+1}/{self.epochs} in {(epoch_end - epoch_start) / 60 }m\")\n","            print(f\"epoch:{i+1}, batch:{j+1}/{batch_ct}, loss:{last_loss}\")\n","\n","    def get_dataset_holder(self):\n","        return self.dataset_holder\n","\n","    def set_dataset_holder(self, dataset_holder):\n","        self.dataset_holder = dataset_holder\n","\n","    def get_model(self):\n","        return self.model\n","\n","    def set_model(self, model):\n","        self.model = model\n","\n","\n","class Runner:\n","\n","    def __init__(self,\n","                 model_parameter_directory=root_filepath+\"resources/model_parameters\",\n","                 trainer_parameter_directory=root_filepath+\"resources/trainer_parameters\",\n","                 runner_hyperparameters_name=\"SETimesByT5Vaswani2017Kocmi2018_0\"):\n","        self.model_parameter_directory = model_parameter_directory\n","        self.trainer_parameter_directory = trainer_parameter_directory\n","        self.runner_hyperparameters_name = runner_hyperparameters_name\n","        self.runner_hyperparameters = SETimesByT5Vaswani2017Kocmi2018_0\n","        self.dataset_holder: DatasetHolder = None\n","        self.model = None\n","        self.trainer = None\n","        self.latest_param_filename_tag = None\n","        if 'latest_param_filename_tag' in self.runner_hyperparameters:\n","            self.latest_param_filename_tag = self.runner_hyperparameters['latest_param_filename_tag']\n","        print(f\"Initialized runner {runner_hyperparameters_name} with parameters {self.runner_hyperparameters}\")\n","\n","    def load_dataset(self):\n","        dataset_transformer_name = self.runner_hyperparameters.get('dataset_transformer_name')\n","        dataset_hyperparameters = self.runner_hyperparameters.get('dataset_transformer_hyperparameters')\n","        dataset_transformer = dataset_transformer_setimesbyt5(dataset_hyperparameters=dataset_hyperparameters)\n","        self.dataset_holder = dataset_transformer.read_dataset()\n","\n","    def load_model(self):\n","        model_hyperparameters = self.runner_hyperparameters.get('model_hyperparameters')\n","        model_hyperparameters['src_vocab_size'] = len(self.dataset_holder.get_source_vocab())\n","        model_hyperparameters['tgt_vocab_size'] = len(self.dataset_holder.get_target_vocab())\n","        model_hyperparameters['max_src_seq_len'] = self.dataset_holder.get_max_src_seq_obs()\n","        model_hyperparameters['max_tgt_seq_len'] = self.dataset_holder.get_max_tgt_seq_obs()\n","        model_parameter_filepath = self.model_parameter_directory + \"/\" + self.runner_hyperparameters_name + \"-\" + self.latest_param_filename_tag + \"-model.params\"\n","        self.model = transformer_vaswani2017(model_hyperparameters=model_hyperparameters)\n","        if os.path.exists(model_parameter_filepath):\n","            model_parameters = torch.load(model_parameter_filepath)\n","            self.model.load_state_dict(model_parameters)\n","\n","    def load_trainer(self):\n","        trainer_hyperparameters = self.runner_hyperparameters.get('trainer_hyperparameters')\n","        self.trainer = model_trainer_kocmi2018(\n","            trainer_hyperparameters=trainer_hyperparameters,\n","            model_parameter_directory=self.model_parameter_directory,\n","            trainer_parameter_directory=self.trainer_parameter_directory,\n","            runner_hyperparameters_name=self.runner_hyperparameters_name,\n","            latest_param_filename_tag=self.latest_param_filename_tag\n","        )\n","\n","    def run_trainer(self):\n","        self.trainer.set_dataset_holder(self.dataset_holder)\n","        self.trainer.set_model(self.model)\n","        self.trainer.init_trainer()\n","        self.trainer.run_trainer()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qcv-CZAnpo1","outputId":"76a99f65-0bc6-4fef-d4b4-be0c15fca619"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized runner SETimesByT5Vaswani2017Kocmi2018_0 with parameters {'dataset_transformer_name': 'dataset_transformer_setimesbyt5', 'model_name': 'transformer_vaswani2017', 'trainer_name': 'model_trainer_kocmi2018', 'latest_param_filename_tag': '1715672129', 'dataset_transformer_hyperparameters': {'sentence_length_min_percentile': 5, 'sentence_length_max_percentile': 95, 'parsed_dataset_filename': 'setimes_parsed-1715586293'}, 'model_hyperparameters': {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 9, 'num_decoder_layers': 3, 'dim_feedforward': 2048, 'dropout': 0.1, 'activation': <function relu at 0x798968bb29e0>, 'custom_encoder': None, 'custom_decoder': None, 'layer_norm_eps': 1e-05, 'batch_first': True, 'norm_first': False, 'bias': True, 'device': None, 'dtype': None}, 'trainer_hyperparameters': {'optimizer_name': 'Adam', 'lr_scheduler_name': 'ExponentialLR', 'initial_lr': 0.001, 'exp_decay': 0.5, 'epochs': 10, 'epoch_starting_index': 0, 'batch_size_limit': 175, 'element_difference_limit': 19, 'batch_starting_index': 765}}\n","Model trainer initialization complete.Trainer will run on model with parameter count 16637953 and parameter memory use 0.0619812048971653 GB\n","Beginning epoch 1 of 10\n","Starting batch.\n","epoch:1/10 batch:766/8961 batch_size:131\n","Completed batch.\n","epoch:1/10 batch:766/8961 batch_size:131 loss:3.603943347930908 time_for_batch_instance:168.52587223052979 total_batch_time:168.52587223052979 running_batch_average:0.22000766609729738\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 673724 KiB |   7538 MiB |   3668 GiB |   3668 GiB |\n","|       from large pool | 219118 KiB |   7095 MiB |   3653 GiB |   3653 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     15 GiB |     15 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 673724 KiB |   7538 MiB |   3668 GiB |   3668 GiB |\n","|       from large pool | 219118 KiB |   7095 MiB |   3653 GiB |   3653 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     15 GiB |     15 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7511 MiB |   3658 GiB |   3658 GiB |\n","|       from large pool | 213248 KiB |   7073 MiB |   3643 GiB |   3643 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |     15 GiB |     15 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    970 MiB |   7914 MiB |    824 GiB |    823 GiB |\n","|       from large pool |    522 MiB |   7450 MiB |    821 GiB |    821 GiB |\n","|       from small pool |    448 MiB |    478 MiB |      2 GiB |      2 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 319556 KiB |    982 MiB |   1219 GiB |   1219 GiB |\n","|       from large pool | 315410 KiB |    976 MiB |   1202 GiB |   1202 GiB |\n","|       from small pool |   4146 KiB |     26 MiB |     16 GiB |     16 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |  210645    |  191770    |\n","|       from large pool |      98    |     248    |  111238    |  111140    |\n","|       from small pool |   18777    |   18924    |   99407    |   80630    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |  210645    |  191770    |\n","|       from large pool |      98    |     248    |  111238    |  111140    |\n","|       from small pool |   18777    |   18924    |   99407    |   80630    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     239    |     323    |   10686    |   10447    |\n","|       from large pool |      15    |      90    |    9371    |    9356    |\n","|       from small pool |     224    |     239    |    1315    |    1091    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      82    |     161    |   96116    |   96034    |\n","|       from large pool |      13    |      58    |   66113    |   66100    |\n","|       from small pool |      69    |     120    |   30003    |   29934    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Starting batch.\n","epoch:1/10 batch:767/8961 batch_size:131\n","Next token prediction. step:27/147 batch:767/8961 epoch:1/10\n","full seq: UN Chief Prosecutor Carla del Ponte speaks during the inauguration of Bosnia and Herzegovina's first war crimes court earlier this year. [AFP]Ģġġġġ\n","pref seq: UN Chief Prosecutor Carla d\n","next tok:                           e\n","pred tok:                           .\n","Completed batch.\n","epoch:1/10 batch:767/8961 batch_size:131 loss:2.1073875427246094 time_for_batch_instance:166.28142189979553 total_batch_time:334.8072941303253 running_batch_average:0.4365153769626145\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 673792 KiB |   7520 MiB |   7308 GiB |   7307 GiB |\n","|       from large pool | 219186 KiB |   7078 MiB |   7278 GiB |   7277 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     30 GiB |     29 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 673792 KiB |   7520 MiB |   7308 GiB |   7307 GiB |\n","|       from large pool | 219186 KiB |   7078 MiB |   7278 GiB |   7277 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     30 GiB |     29 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7497 MiB |   7288 GiB |   7287 GiB |\n","|       from large pool | 213248 KiB |   7059 MiB |   7257 GiB |   7257 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |     30 GiB |     29 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1122 MiB |   7906 MiB |   1650 GiB |   1649 GiB |\n","|       from large pool |    674 MiB |   7444 MiB |   1645 GiB |   1645 GiB |\n","|       from small pool |    448 MiB |    478 MiB |      4 GiB |      4 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 475136 KiB |    780 MiB |   2409 GiB |   2409 GiB |\n","|       from large pool | 470990 KiB |    773 MiB |   2377 GiB |   2376 GiB |\n","|       from small pool |   4146 KiB |     26 MiB |     32 GiB |     32 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |  400842    |  381967    |\n","|       from large pool |      98    |     248    |  221606    |  221508    |\n","|       from small pool |   18777    |   18924    |  179236    |  160459    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |  400842    |  381967    |\n","|       from large pool |      98    |     248    |  221606    |  221508    |\n","|       from small pool |   18777    |   18924    |  179236    |  160459    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     240    |     323    |   21050    |   20810    |\n","|       from large pool |      16    |      90    |   18659    |   18643    |\n","|       from small pool |     224    |     239    |    2391    |    2167    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      83    |     161    |  191382    |  191299    |\n","|       from large pool |      13    |      58    |  132046    |  132033    |\n","|       from small pool |      70    |     120    |   59336    |   59266    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:768/8961 batch_size:131\n","Next token prediction. step:117/146 batch:768/8961 epoch:1/10\n","full seq: The Croatian central bank's foreign reserves increased to $8.9 billion in the beginning of December from $8.1 billion at the end of October.Ģġġġġġ\n","pref seq: The Croatian central bank's foreign reserves increased to $8.9 billion in the beginning of December from $8.1 billion\n","next tok:                                                                                                                      \n","pred tok:                                                                                                                     %\n","Completed batch.\n","epoch:1/10 batch:768/8961 batch_size:131 loss:1.6792634725570679 time_for_batch_instance:164.08739519119263 total_batch_time:498.89468932151794 running_batch_average:0.6496024600540599\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 674189 KiB |   7447 MiB |  10886 GiB |  10885 GiB |\n","|       from large pool | 219583 KiB |   7004 MiB |  10841 GiB |  10841 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     45 GiB |     44 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 674189 KiB |   7447 MiB |  10886 GiB |  10885 GiB |\n","|       from large pool | 219583 KiB |   7004 MiB |  10841 GiB |  10841 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     45 GiB |     44 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7417 MiB |  10852 GiB |  10852 GiB |\n","|       from large pool | 213248 KiB |   6979 MiB |  10807 GiB |  10807 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |     45 GiB |     44 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1048 MiB |   7832 MiB |   2441 GiB |   2440 GiB |\n","|       from large pool |    596 MiB |   7366 MiB |   2434 GiB |   2433 GiB |\n","|       from small pool |    452 MiB |    478 MiB |      6 GiB |      6 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 398962 KiB |    994 MiB |   3660 GiB |   3660 GiB |\n","|       from large pool | 390720 KiB |    986 MiB |   3612 GiB |   3612 GiB |\n","|       from small pool |   8242 KiB |     27 MiB |     48 GiB |     48 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |  589723    |  570848    |\n","|       from large pool |      98    |     248    |  331202    |  331104    |\n","|       from small pool |   18777    |   18924    |  258521    |  239744    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |  589723    |  570848    |\n","|       from large pool |      98    |     248    |  331202    |  331104    |\n","|       from small pool |   18777    |   18924    |  258521    |  239744    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     241    |     322    |   31170    |   30929    |\n","|       from large pool |      15    |      90    |   27700    |   27685    |\n","|       from small pool |     226    |     239    |    3470    |    3244    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      92    |     160    |  286645    |  286553    |\n","|       from large pool |      11    |      56    |  198366    |  198355    |\n","|       from small pool |      81    |     119    |   88279    |   88198    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:769/8961 batch_size:131\n","Next token prediction. step:37/138 batch:769/8961 epoch:1/10\n","full seq: Monday's decision came in response to a request by the United States for military assistance in bringing peace and stability to Iraq.Ģġġġġ\n","pref seq: Monday's decision came in response to\n","next tok:                                      \n","pred tok:                                     %\n","Completed batch.\n","epoch:1/10 batch:769/8961 batch_size:131 loss:3.054927349090576 time_for_batch_instance:151.42525339126587 total_batch_time:650.3199427127838 running_batch_average:0.8456696264145434\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 671949 KiB |   7103 MiB |  14118 GiB |  14117 GiB |\n","|       from large pool | 217343 KiB |   6661 MiB |  14059 GiB |  14059 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     59 GiB |     58 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 671949 KiB |   7103 MiB |  14118 GiB |  14117 GiB |\n","|       from large pool | 217343 KiB |   6661 MiB |  14059 GiB |  14059 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |     59 GiB |     58 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7075 MiB |  14071 GiB |  14070 GiB |\n","|       from large pool | 213248 KiB |   6637 MiB |  14012 GiB |  14012 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |     58 GiB |     58 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1140 MiB |   7490 MiB |   3169 GiB |   3168 GiB |\n","|       from large pool |    690 MiB |   7024 MiB |   3160 GiB |   3159 GiB |\n","|       from small pool |    450 MiB |    478 MiB |      8 GiB |      8 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 495411 KiB |    837 MiB |   4682 GiB |   4682 GiB |\n","|       from large pool | 489217 KiB |    828 MiB |   4619 GiB |   4619 GiB |\n","|       from small pool |   6194 KiB |     25 MiB |     62 GiB |     62 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |     768 K  |  749201    |\n","|       from large pool |      98    |     248    |     434 K  |  434524    |\n","|       from small pool |   18777    |   18924    |     333 K  |  314677    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |     768 K  |  749201    |\n","|       from large pool |      98    |     248    |     434 K  |  434524    |\n","|       from small pool |   18777    |   18924    |     333 K  |  314677    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     242    |     326    |   40839    |   40597    |\n","|       from large pool |      17    |      94    |   36363    |   36346    |\n","|       from small pool |     225    |     239    |    4476    |    4251    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      92    |     172    |  361580    |  361488    |\n","|       from large pool |      15    |      69    |  245692    |  245677    |\n","|       from small pool |      77    |     119    |  115888    |  115811    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:770/8961 batch_size:130\n","Next token prediction. step:104/257 batch:770/8961 epoch:1/10\n","full seq: According to data provided by the Turkish Central Bank, revenue obtained from the tourism sector increased to nearly 7 billion euros during a ten-month period in 2003, an increase of 12 per cent in comparison with the same period in 2002.Ģġġġġġġġġġġġġġġġġġġ\n","pref seq: According to data provided by the Turkish Central Bank, revenue obtained from the tourism sector increas\n","next tok:                                                                                                        e\n","pred tok:                                                                                                        C\n","Completed batch.\n","epoch:1/10 batch:770/8961 batch_size:130 loss:2.088479995727539 time_for_batch_instance:284.2094187736511 total_batch_time:934.5293614864349 running_batch_average:1.2136744954369285\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 670885 KiB |  12640 MiB |  24739 GiB |  24738 GiB |\n","|       from large pool | 216279 KiB |  12205 MiB |  24656 GiB |  24656 GiB |\n","|       from small pool | 454606 KiB |    469 MiB |     82 GiB |     81 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 670885 KiB |  12640 MiB |  24739 GiB |  24738 GiB |\n","|       from large pool | 216279 KiB |  12205 MiB |  24656 GiB |  24656 GiB |\n","|       from small pool | 454606 KiB |    469 MiB |     82 GiB |     81 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |  12605 MiB |  24665 GiB |  24664 GiB |\n","|       from large pool | 213248 KiB |  12174 MiB |  24583 GiB |  24582 GiB |\n","|       from small pool | 450193 KiB |    464 MiB |     82 GiB |     81 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1276 MiB |  13260 MiB |   5731 GiB |   5730 GiB |\n","|       from large pool |    828 MiB |  12798 MiB |   5719 GiB |   5718 GiB |\n","|       from small pool |    448 MiB |    474 MiB |     12 GiB |     11 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 635738 KiB |   1149 MiB |   8258 GiB |   8258 GiB |\n","|       from large pool | 631592 KiB |   1143 MiB |   8171 GiB |   8171 GiB |\n","|       from small pool |   4146 KiB |     23 MiB |     87 GiB |     87 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1103 K  |    1084 K  |\n","|       from large pool |      98    |     263    |     637 K  |     636 K  |\n","|       from small pool |   18777    |   18924    |     466 K  |     447 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1103 K  |    1084 K  |\n","|       from large pool |      98    |     263    |     637 K  |     636 K  |\n","|       from small pool |   18777    |   18924    |     466 K  |     447 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     242    |     326    |   59508    |   59266    |\n","|       from large pool |      18    |      94    |   53161    |   53143    |\n","|       from small pool |     224    |     237    |    6347    |    6123    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      80    |     151    |  537536    |  537456    |\n","|       from large pool |      18    |      49    |  368990    |  368972    |\n","|       from small pool |      62    |     118    |  168546    |  168484    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:771/8961 batch_size:129\n","Next token prediction. step:3/191 batch:771/8961 epoch:1/10\n","full seq: ESI failed to offer any supportive evidence for its implicit thesis that civil society and responsible politicians will blossom after OHR gives up its powers, said Perry.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: ESI\n","next tok:    \n","pred tok:   ġ\n","Completed batch.\n","epoch:1/10 batch:771/8961 batch_size:129 loss:3.8973500728607178 time_for_batch_instance:210.47935032844543 total_batch_time:1145.0087118148804 running_batch_average:1.485095605466771\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 668885 KiB |   9389 MiB |  30601 GiB |  30600 GiB |\n","|       from large pool | 214279 KiB |   8944 MiB |  30498 GiB |  30498 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    102 GiB |    102 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 668885 KiB |   9389 MiB |  30601 GiB |  30600 GiB |\n","|       from large pool | 214279 KiB |   8944 MiB |  30498 GiB |  30498 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    102 GiB |    102 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   9361 MiB |  30512 GiB |  30511 GiB |\n","|       from large pool | 213248 KiB |   8920 MiB |  30409 GiB |  30409 GiB |\n","|       from small pool | 450193 KiB |    470 MiB |    102 GiB |    102 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    884 MiB |   9802 MiB |   7115 GiB |   7115 GiB |\n","|       from large pool |    436 MiB |   9340 MiB |   7100 GiB |   7100 GiB |\n","|       from small pool |    448 MiB |    478 MiB |     15 GiB |     14 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 236330 KiB |   1075 MiB |  10190 GiB |  10190 GiB |\n","|       from large pool | 232184 KiB |   1071 MiB |  10081 GiB |  10081 GiB |\n","|       from small pool |   4146 KiB |     25 MiB |    109 GiB |    109 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1351 K  |    1332 K  |\n","|       from large pool |      98    |     248    |     781 K  |     781 K  |\n","|       from small pool |   18777    |   18924    |     569 K  |     551 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1351 K  |    1332 K  |\n","|       from large pool |      98    |     248    |     781 K  |     781 K  |\n","|       from small pool |   18777    |   18924    |     569 K  |     551 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     239    |     325    |   73203    |   72964    |\n","|       from large pool |      15    |      94    |   65506    |   65491    |\n","|       from small pool |     224    |     239    |    7697    |    7473    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      80    |     147    |  636920    |  636840    |\n","|       from large pool |      11    |      45    |  427769    |  427758    |\n","|       from small pool |      69    |     118    |  209151    |  209082    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:772/8961 batch_size:129\n","Next token prediction. step:131/144 batch:772/8961 epoch:1/10\n","full seq: Underlining the broad-based support, parliament passed a resolution supporting the decision by a vote of 89-8 on July 28th.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: Underlining the broad-based support, parliament passed a resolution supporting the decision by a vote of 89-8 on July 28th.Ģġġġġġġġ\n","next tok:                                                                                                                                   ġ\n","pred tok:                                                                                                                                   1\n","Completed batch.\n","epoch:1/10 batch:772/8961 batch_size:129 loss:4.436600685119629 time_for_batch_instance:161.60031270980835 total_batch_time:1306.6090245246887 running_batch_average:1.6924987364309438\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 673940 KiB |   7204 MiB |  34012 GiB |  34011 GiB |\n","|       from large pool | 219334 KiB |   6762 MiB |  33895 GiB |  33895 GiB |\n","|       from small pool | 454606 KiB |    472 MiB |    117 GiB |    116 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 673940 KiB |   7204 MiB |  34012 GiB |  34011 GiB |\n","|       from large pool | 219334 KiB |   6762 MiB |  33895 GiB |  33895 GiB |\n","|       from small pool | 454606 KiB |    472 MiB |    117 GiB |    116 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7188 MiB |  33919 GiB |  33919 GiB |\n","|       from large pool | 213248 KiB |   6750 MiB |  33802 GiB |  33802 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |    117 GiB |    116 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1248 MiB |   7614 MiB |   7892 GiB |   7891 GiB |\n","|       from large pool |    798 MiB |   7150 MiB |   7875 GiB |   7874 GiB |\n","|       from small pool |    450 MiB |    476 MiB |     17 GiB |     16 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 604011 KiB |    807 MiB |  11415 GiB |  11415 GiB |\n","|       from large pool | 597817 KiB |    799 MiB |  11290 GiB |  11290 GiB |\n","|       from small pool |   6194 KiB |     24 MiB |    124 GiB |    124 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1537 K  |    1518 K  |\n","|       from large pool |      98    |     248    |     889 K  |     889 K  |\n","|       from small pool |   18777    |   18924    |     647 K  |     629 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1537 K  |    1518 K  |\n","|       from large pool |      98    |     248    |     889 K  |     889 K  |\n","|       from small pool |   18777    |   18924    |     647 K  |     629 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     244    |     320    |   83234    |   82990    |\n","|       from large pool |      19    |      88    |   74463    |   74444    |\n","|       from small pool |     225    |     238    |    8771    |    8546    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      90    |     184    |  738933    |  738843    |\n","|       from large pool |      15    |      82    |  500970    |  500955    |\n","|       from small pool |      75    |     118    |  237963    |  237888    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:773/8961 batch_size:129\n","Next token prediction. step:72/143 batch:773/8961 epoch:1/10\n","full seq: NATO and EU officials have taken note of government initiatives in this area, but warn concrete results are still missing.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: NATO and EU officials have taken note of government initiatives in this \n","next tok:                                                                        a\n","pred tok:                                                                        1\n","Completed batch.\n","epoch:1/10 batch:773/8961 batch_size:129 loss:4.39432954788208 time_for_batch_instance:160.37243342399597 total_batch_time:1466.9814579486847 running_batch_average:1.8977767890668624\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 676431 KiB |   7185 MiB |  37396 GiB |  37395 GiB |\n","|       from large pool | 221825 KiB |   6743 MiB |  37264 GiB |  37264 GiB |\n","|       from small pool | 454606 KiB |    472 MiB |    131 GiB |    131 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 676431 KiB |   7185 MiB |  37396 GiB |  37395 GiB |\n","|       from large pool | 221825 KiB |   6743 MiB |  37264 GiB |  37264 GiB |\n","|       from small pool | 454606 KiB |    472 MiB |    131 GiB |    131 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   7174 MiB |  37299 GiB |  37298 GiB |\n","|       from large pool | 213248 KiB |   6737 MiB |  37167 GiB |  37167 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |    131 GiB |    131 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1350 MiB |   7628 MiB |   8655 GiB |   8654 GiB |\n","|       from large pool |    902 MiB |   7164 MiB |   8636 GiB |   8635 GiB |\n","|       from small pool |    448 MiB |    476 MiB |     19 GiB |     18 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 705969 KiB |    798 MiB |  12617 GiB |  12616 GiB |\n","|       from large pool | 701823 KiB |    788 MiB |  12477 GiB |  12476 GiB |\n","|       from small pool |   4146 KiB |     23 MiB |    139 GiB |    139 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1722 K  |    1703 K  |\n","|       from large pool |      98    |     248    |     996 K  |     996 K  |\n","|       from small pool |   18777    |   18924    |     725 K  |     706 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1722 K  |    1703 K  |\n","|       from large pool |      98    |     248    |     996 K  |     996 K  |\n","|       from small pool |   18777    |   18924    |     725 K  |     706 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     242    |     321    |   93074    |   92832    |\n","|       from large pool |      18    |      89    |   83236    |   83218    |\n","|       from small pool |     224    |     238    |    9838    |    9614    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      87    |     184    |     839 K  |     839 K  |\n","|       from large pool |      14    |      81    |     573 K  |     573 K  |\n","|       from small pool |      73    |     118    |     266 K  |     266 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:774/8961 batch_size:129\n","Next token prediction. step:52/112 batch:774/8961 epoch:1/10\n","full seq: Elements of his work \"Four Seasons\" can be seen at the Croatian National Theatre in Zagreb.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: Elements of his work \"Four Seasons\" can be seen at t\n","next tok:                                                    h\n","pred tok:                                                    2\n","Completed batch.\n","epoch:1/10 batch:774/8961 batch_size:129 loss:3.6593129634857178 time_for_batch_instance:125.19662094116211 total_batch_time:1592.1780788898468 running_batch_average:2.0570776213047117\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 673170 KiB |   5723 MiB |  39479 GiB |  39478 GiB |\n","|       from large pool | 218564 KiB |   5284 MiB |  39337 GiB |  39336 GiB |\n","|       from small pool | 454606 KiB |    470 MiB |    142 GiB |    141 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 673170 KiB |   5723 MiB |  39479 GiB |  39478 GiB |\n","|       from large pool | 218564 KiB |   5284 MiB |  39337 GiB |  39336 GiB |\n","|       from small pool | 454606 KiB |    470 MiB |    142 GiB |    141 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   5708 MiB |  39377 GiB |  39377 GiB |\n","|       from large pool | 213248 KiB |   5273 MiB |  39235 GiB |  39235 GiB |\n","|       from small pool | 450193 KiB |    466 MiB |    142 GiB |    141 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1056 MiB |   6038 MiB |   9109 GiB |   9108 GiB |\n","|       from large pool |    604 MiB |   5572 MiB |   9088 GiB |   9088 GiB |\n","|       from small pool |    452 MiB |    474 MiB |     20 GiB |     20 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 408174 KiB |   1019 MiB |  13305 GiB |  13305 GiB |\n","|       from large pool | 399932 KiB |   1014 MiB |  13154 GiB |  13153 GiB |\n","|       from small pool |   8242 KiB |     25 MiB |    151 GiB |    151 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1866 K  |    1847 K  |\n","|       from large pool |      98    |     248    |    1080 K  |    1079 K  |\n","|       from small pool |   18777    |   18924    |     786 K  |     767 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1866 K  |    1847 K  |\n","|       from large pool |      98    |     248    |    1080 K  |    1079 K  |\n","|       from small pool |   18777    |   18924    |     786 K  |     767 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     245    |     330    |  100685    |  100440    |\n","|       from large pool |      19    |      96    |   90030    |   90011    |\n","|       from small pool |     226    |     237    |   10655    |   10429    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      97    |     148    |     899 K  |     899 K  |\n","|       from large pool |      22    |      44    |     610 K  |     610 K  |\n","|       from small pool |      75    |     117    |     289 K  |     289 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:775/8961 batch_size:129\n","Next token prediction. step:86/97 batch:775/8961 epoch:1/10\n","full seq: Several people said that they had received death threats from people close to the Rashkovs.Ģġġġġġ\n","pref seq: Several people said that they had received death threats from people close to the Rash\n","next tok:                                                                                      k\n","pred tok:                                                                                      Ģ\n","Completed batch.\n","epoch:1/10 batch:775/8961 batch_size:129 loss:3.056260824203491 time_for_batch_instance:107.30904388427734 total_batch_time:1699.4871227741241 running_batch_average:2.192886610031128\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 672711 KiB |   5096 MiB |  41084 GiB |  41083 GiB |\n","|       from large pool | 218105 KiB |   4658 MiB |  40932 GiB |  40932 GiB |\n","|       from small pool | 454606 KiB |    470 MiB |    151 GiB |    151 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 672711 KiB |   5096 MiB |  41084 GiB |  41083 GiB |\n","|       from large pool | 218105 KiB |   4658 MiB |  40932 GiB |  40932 GiB |\n","|       from small pool | 454606 KiB |    470 MiB |    151 GiB |    151 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   5079 MiB |  40976 GiB |  40976 GiB |\n","|       from large pool | 213248 KiB |   4645 MiB |  40825 GiB |  40825 GiB |\n","|       from small pool | 450193 KiB |    466 MiB |    151 GiB |    151 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1338 MiB |   5528 MiB |   9457 GiB |   9456 GiB |\n","|       from large pool |    888 MiB |   5062 MiB |   9435 GiB |   9434 GiB |\n","|       from small pool |    450 MiB |    474 MiB |     22 GiB |     21 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 697400 KiB |    769 MiB |  13902 GiB |  13902 GiB |\n","|       from large pool | 691206 KiB |    763 MiB |  13741 GiB |  13741 GiB |\n","|       from small pool |   6194 KiB |     23 MiB |    161 GiB |    161 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    1990 K  |    1971 K  |\n","|       from large pool |      98    |     248    |    1151 K  |    1151 K  |\n","|       from small pool |   18777    |   18924    |     839 K  |     820 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    1990 K  |    1971 K  |\n","|       from large pool |      98    |     248    |    1151 K  |    1151 K  |\n","|       from small pool |   18777    |   18924    |     839 K  |     820 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     243    |     324    |  107077    |  106834    |\n","|       from large pool |      18    |      91    |   95704    |   95686    |\n","|       from small pool |     225    |     237    |   11373    |   11148    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      95    |     173    |     965 K  |     965 K  |\n","|       from large pool |      18    |      69    |     656 K  |     656 K  |\n","|       from small pool |      77    |     118    |     308 K  |     308 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:776/8961 batch_size:129\n","Next token prediction. step:54/95 batch:776/8961 epoch:1/10\n","full seq: But I am confident that BiH is not and will not become a base for any kind of terrorism.Ģġġġġġġ\n","pref seq: But I am confident that BiH is not and will not become\n","next tok:                                                       \n","pred tok:                                                      Ģ\n","Completed batch.\n","epoch:1/10 batch:776/8961 batch_size:129 loss:2.848928928375244 time_for_batch_instance:103.1362657546997 total_batch_time:1802.6233885288239 running_batch_average:2.322968284186629\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 674151 KiB |   5001 MiB |  42623 GiB |  42622 GiB |\n","|       from large pool | 219545 KiB |   4565 MiB |  42462 GiB |  42462 GiB |\n","|       from small pool | 454606 KiB |    469 MiB |    160 GiB |    160 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 674151 KiB |   5001 MiB |  42623 GiB |  42622 GiB |\n","|       from large pool | 219545 KiB |   4565 MiB |  42462 GiB |  42462 GiB |\n","|       from small pool | 454606 KiB |    469 MiB |    160 GiB |    160 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   4985 MiB |  42512 GiB |  42512 GiB |\n","|       from large pool | 213248 KiB |   4553 MiB |  42352 GiB |  42352 GiB |\n","|       from small pool | 450193 KiB |    464 MiB |    160 GiB |    159 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1230 MiB |   5454 MiB |   9792 GiB |   9791 GiB |\n","|       from large pool |    782 MiB |   4990 MiB |   9769 GiB |   9768 GiB |\n","|       from small pool |    448 MiB |    474 MiB |     23 GiB |     23 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 585368 KiB | 744670 KiB |  14511 GiB |  14510 GiB |\n","|       from large pool | 581222 KiB | 739623 KiB |  14340 GiB |  14340 GiB |\n","|       from small pool |   4146 KiB |  25579 KiB |    170 GiB |    170 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    2112 K  |    2093 K  |\n","|       from large pool |      98    |     248    |    1222 K  |    1221 K  |\n","|       from small pool |   18777    |   18924    |     890 K  |     871 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    2112 K  |    2093 K  |\n","|       from large pool |      98    |     248    |    1222 K  |    1221 K  |\n","|       from small pool |   18777    |   18924    |     890 K  |     871 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     241    |     323    |  113242    |  113001    |\n","|       from large pool |      17    |      91    |  101180    |  101163    |\n","|       from small pool |     224    |     237    |   12062    |   11838    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      91    |     180    |    1032 K  |    1032 K  |\n","|       from large pool |      15    |      77    |     704 K  |     704 K  |\n","|       from small pool |      76    |     115    |     328 K  |     328 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:777/8961 batch_size:127\n","Next token prediction. step:249/257 batch:777/8961 epoch:1/10\n","full seq: \"A person suffering from PTSD can commit suicide if they are unhappy or sick, but if they are under the constant care and take medication regularly, this should not happen,\" Dzubur Kulenovic of the Tuzla PTSD association Stecak told SETimes.Ģġġġġġġġġġġġġġġġ\n","pref seq: \"A person suffering from PTSD can commit suicide if they are unhappy or sick, but if they are under the constant care and take medication regularly, this should not happen,\" Dzubur Kulenovic of the Tuzla PTSD association Stecak told SETimes.Ģġġġġġġġ\n","next tok:                                                                                                                                                                                                                                                         ġ\n","pred tok:                                                                                                                                                                                                                                                         Ģ\n","Completed batch.\n","epoch:1/10 batch:777/8961 batch_size:127 loss:0.8125143051147461 time_for_batch_instance:286.0697708129883 total_batch_time:2088.693159341812 running_batch_average:2.6881507842236965\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 670560 KiB |  12482 MiB |  53128 GiB |  53127 GiB |\n","|       from large pool | 215954 KiB |  12041 MiB |  52944 GiB |  52943 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |    184 GiB |    183 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 670560 KiB |  12482 MiB |  53128 GiB |  53127 GiB |\n","|       from large pool | 215954 KiB |  12041 MiB |  52944 GiB |  52943 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |    184 GiB |    183 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |  12455 MiB |  52999 GiB |  52999 GiB |\n","|       from large pool | 213248 KiB |  12018 MiB |  52815 GiB |  52815 GiB |\n","|       from small pool | 450193 KiB |    468 MiB |    184 GiB |    183 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1006 MiB |  13094 MiB |  12343 GiB |  12342 GiB |\n","|       from large pool |    558 MiB |  12630 MiB |  12316 GiB |  12316 GiB |\n","|       from small pool |    448 MiB |    476 MiB |     27 GiB |     26 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 359584 KiB |   1244 MiB |  17951 GiB |  17951 GiB |\n","|       from large pool | 355438 KiB |   1241 MiB |  17756 GiB |  17756 GiB |\n","|       from small pool |   4146 KiB |     27 MiB |    195 GiB |    195 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    2447 K  |    2428 K  |\n","|       from large pool |      98    |     257    |    1423 K  |    1423 K  |\n","|       from small pool |   18777    |   18924    |    1023 K  |    1004 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    2447 K  |    2428 K  |\n","|       from large pool |      98    |     257    |    1423 K  |    1423 K  |\n","|       from small pool |   18777    |   18924    |    1023 K  |    1004 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     243    |     331    |  132018    |  131775    |\n","|       from large pool |      19    |      99    |  118044    |  118025    |\n","|       from small pool |     224    |     238    |   13974    |   13750    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      89    |     177    |    1210 K  |    1210 K  |\n","|       from large pool |      20    |      74    |     827 K  |     827 K  |\n","|       from small pool |      69    |     117    |     382 K  |     382 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:778/8961 batch_size:127\n","Next token prediction. step:190/256 batch:778/8961 epoch:1/10\n","full seq: Archaeological sites in Greece, including the Acropolis in Athens and the White Tower in Thessaloniki, remained closed on Monday (December 15th) due to a 24-hour strike called by the trade union representing culture ministry employees.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: Archaeological sites in Greece, including the Acropolis in Athens and the White Tower in Thessaloniki, remained closed on Monday (December 15th) due to a 24-hour strike called by the trade u\n","next tok:                                                                                                                                                                                              n\n","pred tok:                                                                                                                                                                                              )\n","Completed batch.\n","epoch:1/10 batch:778/8961 batch_size:127 loss:1.047560691833496 time_for_batch_instance:286.10595178604126 total_batch_time:2374.7991111278534 running_batch_average:3.0524410168738476\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 669117 KiB |  12242 MiB |  63348 GiB |  63347 GiB |\n","|       from large pool | 214511 KiB |  11792 MiB |  63134 GiB |  63134 GiB |\n","|       from small pool | 454606 KiB |    482 MiB |    214 GiB |    213 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 669117 KiB |  12242 MiB |  63348 GiB |  63347 GiB |\n","|       from large pool | 214511 KiB |  11792 MiB |  63134 GiB |  63134 GiB |\n","|       from small pool | 454606 KiB |    482 MiB |    214 GiB |    213 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |  12217 MiB |  63191 GiB |  63191 GiB |\n","|       from large pool | 213248 KiB |  11771 MiB |  62977 GiB |  62977 GiB |\n","|       from small pool | 450193 KiB |    477 MiB |    214 GiB |    213 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    850 MiB |  12788 MiB |  14830 GiB |  14829 GiB |\n","|       from large pool |    400 MiB |  12324 MiB |  14800 GiB |  14799 GiB |\n","|       from small pool |    450 MiB |    484 MiB |     30 GiB |     29 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 201283 KiB |   1010 MiB |  21090 GiB |  21090 GiB |\n","|       from large pool | 195089 KiB |   1001 MiB |  20863 GiB |  20863 GiB |\n","|       from small pool |   6194 KiB |     24 MiB |    227 GiB |    227 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    2781 K  |    2762 K  |\n","|       from large pool |      98    |     248    |    1618 K  |    1618 K  |\n","|       from small pool |   18777    |   18924    |    1162 K  |    1144 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    2781 K  |    2762 K  |\n","|       from large pool |      98    |     248    |    1618 K  |    1618 K  |\n","|       from small pool |   18777    |   18924    |    1162 K  |    1144 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     240    |     329    |  150892    |  150652    |\n","|       from large pool |      15    |      97    |  135349    |  135334    |\n","|       from small pool |     225    |     242    |   15543    |   15318    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      84    |     152    |    1335 K  |    1334 K  |\n","|       from large pool |      11    |      46    |     898 K  |     898 K  |\n","|       from small pool |      73    |     122    |     436 K  |     436 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:779/8961 batch_size:127\n","Next token prediction. step:81/109 batch:779/8961 epoch:1/10\n","full seq: We deal with complicated procedures and many other difficulties that are hard to remove.Ģġġġġġġġġġġġġġġġġġġġġ\n","pref seq: We deal with complicated procedures and many other difficulties that are hard to \n","next tok:                                                                                 r\n","pred tok:                                                                                 Ģ\n","Completed batch.\n","epoch:1/10 batch:779/8961 batch_size:127 loss:3.255051612854004 time_for_batch_instance:119.16220164299011 total_batch_time:2493.9613127708435 running_batch_average:3.201490773775152\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 668663 KiB |   5530 MiB |  65307 GiB |  65306 GiB |\n","|       from large pool | 214057 KiB |   5091 MiB |  65082 GiB |  65082 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    224 GiB |    224 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 668663 KiB |   5530 MiB |  65307 GiB |  65306 GiB |\n","|       from large pool | 214057 KiB |   5091 MiB |  65082 GiB |  65082 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    224 GiB |    224 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   5492 MiB |  65135 GiB |  65134 GiB |\n","|       from large pool | 213248 KiB |   5057 MiB |  64910 GiB |  64910 GiB |\n","|       from small pool | 450193 KiB |    470 MiB |    224 GiB |    224 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    964 MiB |   5814 MiB |  15257 GiB |  15256 GiB |\n","|       from large pool |    512 MiB |   5348 MiB |  15225 GiB |  15224 GiB |\n","|       from small pool |    452 MiB |    476 MiB |     31 GiB |     31 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 318473 KiB |    813 MiB |  21759 GiB |  21759 GiB |\n","|       from large pool | 310231 KiB |    808 MiB |  21521 GiB |  21521 GiB |\n","|       from small pool |   8242 KiB |     27 MiB |    238 GiB |    238 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    2921 K  |    2902 K  |\n","|       from large pool |      98    |     248    |    1699 K  |    1699 K  |\n","|       from small pool |   18777    |   18924    |    1222 K  |    1203 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    2921 K  |    2902 K  |\n","|       from large pool |      98    |     248    |    1699 K  |    1699 K  |\n","|       from small pool |   18777    |   18924    |    1222 K  |    1203 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     245    |     326    |  158241    |  157996    |\n","|       from large pool |      19    |      93    |  141901    |  141882    |\n","|       from small pool |     226    |     238    |   16340    |   16114    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     105    |     152    |    1400 K  |    1400 K  |\n","|       from large pool |      30    |      51    |     940 K  |     940 K  |\n","|       from small pool |      75    |     115    |     459 K  |     459 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:780/8961 batch_size:127\n","Next token prediction. step:84/97 batch:780/8961 epoch:1/10\n","full seq: New destinations have opened up for Albanians as a result of the EC's decision. [Reuters]Ģġġġġġġġ\n","pref seq: New destinations have opened up for Albanians as a result of the EC's decision. [Reu\n","next tok:                                                                                    t\n","pred tok:                                                                                    Ģ\n","Completed batch.\n","epoch:1/10 batch:780/8961 batch_size:127 loss:2.1803648471832275 time_for_batch_instance:105.85534811019897 total_batch_time:2599.8166608810425 running_batch_average:3.333098283180824\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 674626 KiB |   4990 MiB |  66873 GiB |  66873 GiB |\n","|       from large pool | 220020 KiB |   4553 MiB |  66639 GiB |  66639 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    234 GiB |    233 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 674626 KiB |   4990 MiB |  66873 GiB |  66873 GiB |\n","|       from large pool | 220020 KiB |   4553 MiB |  66639 GiB |  66639 GiB |\n","|       from small pool | 454606 KiB |    474 MiB |    234 GiB |    233 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   4977 MiB |  66699 GiB |  66698 GiB |\n","|       from large pool | 213248 KiB |   4544 MiB |  66465 GiB |  66465 GiB |\n","|       from small pool | 450193 KiB |    470 MiB |    234 GiB |    233 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1148 MiB |   5466 MiB |  15598 GiB |  15597 GiB |\n","|       from large pool |    698 MiB |   5000 MiB |  15565 GiB |  15564 GiB |\n","|       from small pool |    450 MiB |    476 MiB |     33 GiB |     32 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 500926 KiB |    805 MiB |  22375 GiB |  22375 GiB |\n","|       from large pool | 494732 KiB |    798 MiB |  22127 GiB |  22127 GiB |\n","|       from small pool |   6194 KiB |     24 MiB |    248 GiB |    248 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    3045 K  |    3026 K  |\n","|       from large pool |      98    |     248    |    1770 K  |    1770 K  |\n","|       from small pool |   18777    |   18924    |    1274 K  |    1256 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    3045 K  |    3026 K  |\n","|       from large pool |      98    |     248    |    1770 K  |    1770 K  |\n","|       from small pool |   18777    |   18924    |    1274 K  |    1256 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     242    |     323    |  164576    |  164334    |\n","|       from large pool |      17    |      90    |  147510    |  147493    |\n","|       from small pool |     225    |     238    |   17066    |   16841    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      93    |     182    |    1467 K  |    1467 K  |\n","|       from large pool |      15    |      79    |     988 K  |     988 K  |\n","|       from small pool |      78    |     118    |     479 K  |     479 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:781/8961 batch_size:127\n","Next token prediction. step:66/90 batch:781/8961 epoch:1/10\n","full seq: The Thessaloniki summit also marked the end of Greece's chairmanship of the SEECP.Ģġġġġġġġ\n","pref seq: The Thessaloniki summit also marked the end of Greece's chairmansh\n","next tok:                                                                  i\n","pred tok:                                                                  Ģ\n","Completed batch.\n","epoch:1/10 batch:781/8961 batch_size:127 loss:1.525238275527954 time_for_batch_instance:98.24752902984619 total_batch_time:2698.0641899108887 running_batch_average:3.4546276439319956\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 670787 KiB |   4693 MiB |  68241 GiB |  68240 GiB |\n","|       from large pool | 216181 KiB |   4257 MiB |  67998 GiB |  67998 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |    242 GiB |    242 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 670787 KiB |   4693 MiB |  68241 GiB |  68240 GiB |\n","|       from large pool | 216181 KiB |   4257 MiB |  67998 GiB |  67998 GiB |\n","|       from small pool | 454606 KiB |    473 MiB |    242 GiB |    242 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 663441 KiB |   4658 MiB |  68053 GiB |  68052 GiB |\n","|       from large pool | 213248 KiB |   4226 MiB |  67810 GiB |  67810 GiB |\n","|       from small pool | 450193 KiB |    469 MiB |    242 GiB |    242 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1268 MiB |   5096 MiB |  15892 GiB |  15890 GiB |\n","|       from large pool |    818 MiB |   4632 MiB |  15857 GiB |  15856 GiB |\n","|       from small pool |    450 MiB |    476 MiB |     34 GiB |     34 GiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 627645 KiB |    875 MiB |  22885 GiB |  22885 GiB |\n","|       from large pool | 621451 KiB |    859 MiB |  22628 GiB |  22628 GiB |\n","|       from small pool |   6194 KiB |     27 MiB |    257 GiB |    257 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |   18875    |   19046    |    3160 K  |    3141 K  |\n","|       from large pool |      98    |     248    |    1836 K  |    1836 K  |\n","|       from small pool |   18777    |   18924    |    1323 K  |    1305 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |   18875    |   19046    |    3160 K  |    3141 K  |\n","|       from large pool |      98    |     248    |    1836 K  |    1836 K  |\n","|       from small pool |   18777    |   18924    |    1323 K  |    1305 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     243    |     321    |  170366    |  170123    |\n","|       from large pool |      18    |      89    |  152623    |  152605    |\n","|       from small pool |     225    |     238    |   17743    |   17518    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      97    |     147    |    1517 K  |    1517 K  |\n","|       from large pool |      21    |      44    |    1019 K  |    1019 K  |\n","|       from small pool |      76    |     117    |     498 K  |     498 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Starting batch.\n","epoch:1/10 batch:782/8961 batch_size:126\n","Next token prediction. step:55/81 batch:782/8961 epoch:1/10\n","full seq: Olympics Conclude; Athens Congratulated for \"Splendid\" GamesĢġġġġġġġġġġġġġġġġġġġġ\n","pref seq: Olympics Conclude; Athens Congratulated for \"Splendid\" \n","next tok:                                                       G\n","pred tok:                                                       Ģ\n"]}],"source":["runner = Runner(runner_hyperparameters_name=\"SETimesByT5Vaswani2017Kocmi2018_0\")\n","\n","runner.load_dataset()\n","runner.load_model()\n","runner.load_trainer()\n","runner.run_trainer()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPpdkgsrksstkO+IdVjaVT1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}