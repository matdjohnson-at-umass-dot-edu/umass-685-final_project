{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNR+QjbvwNvKvZ+D2HT1NTk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XLxa0tEyK7r","outputId":"abe5fff3-4a5f-4c13-8b41-663ee68aabb3","executionInfo":{"status":"ok","timestamp":1715991191915,"user_tz":240,"elapsed":1193992,"user":{"displayName":"Matthew Johnson","userId":"09672185346885346735"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Initialized runner NewsCommentaryByT5Vaswani2017Kocmi2018_1 with parameters {'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_1', 'model_parameter_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_1-1715966141-model.params', 'datasets_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808', 'output_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_1.json', 'model_hyperparameters': {'src_vocab_size': 150, 'tgt_vocab_size': 81, 'max_src_seq_len': 266, 'max_tgt_seq_len': 256, 'd_model': 256, 'nhead': 8, 'num_encoder_layers': 9, 'num_decoder_layers': 3, 'dim_feedforward': 1796, 'dropout': 0.1, 'activation': <function relu at 0x7c350db3bbe0>, 'custom_encoder': None, 'custom_decoder': None, 'layer_norm_eps': 1e-05, 'batch_first': True, 'norm_first': False, 'bias': True, 'device': None, 'dtype': None}}\n","Completed translation 0 of 427 at 1715990045.000797\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  69637 KiB | 122286 KiB |  18916 MiB |  18848 MiB |\n","|       from large pool |  52872 KiB |  89104 KiB |     95 MiB |     43 MiB |\n","|       from small pool |  16765 KiB |  33182 KiB |  18821 MiB |  18805 MiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  69637 KiB | 122286 KiB |  18916 MiB |  18848 MiB |\n","|       from large pool |  52872 KiB |  89104 KiB |     95 MiB |     43 MiB |\n","|       from small pool |  16765 KiB |  33182 KiB |  18821 MiB |  18805 MiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  68114 KiB | 119309 KiB |  18907 MiB |  18840 MiB |\n","|       from large pool |  51424 KiB |  86208 KiB |     92 MiB |     42 MiB |\n","|       from small pool |  16690 KiB |  33101 KiB |  18815 MiB |  18798 MiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  86016 KiB | 137216 KiB | 137216 KiB |  51200 KiB |\n","|       from large pool |  61440 KiB | 102400 KiB | 102400 KiB |  40960 KiB |\n","|       from small pool |  24576 KiB |  34816 KiB |  34816 KiB |  10240 KiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  16379 KiB |  38198 KiB |  20161 MiB |  20145 MiB |\n","|       from large pool |   8568 KiB |  31256 KiB |    129 MiB |    121 MiB |\n","|       from small pool |   7811 KiB |   9857 KiB |  20031 MiB |  20023 MiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     620    |   76295    |   75855    |\n","|       from large pool |      25    |      48    |      49    |      24    |\n","|       from small pool |     415    |     595    |   76246    |   75831    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     620    |   76295    |   75855    |\n","|       from large pool |      25    |      48    |      49    |      24    |\n","|       from small pool |     415    |     595    |   76246    |   75831    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      22    |      22    |       7    |\n","|       from large pool |       3    |       5    |       5    |       2    |\n","|       from small pool |      12    |      17    |      17    |       5    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       9    |      17    |   30268    |   30259    |\n","|       from large pool |       2    |       2    |       8    |       6    |\n","|       from small pool |       7    |      15    |   30260    |   30253    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation0: {'source': 'Қазір жұрт технологияны да солай игеріп жатыр.Ģ', 'target': 'People engage with technology in similar ways.Ģ', 'translation': 'sruuhruuhnruunhunnhnnnrnnuhurhhhuuhruurhunhhrhruhuhnuruhnnnrurunurnnnhhhrhrunuurrnnhhuhhuuuurhnrnnhhrnunhnuhrhhnnnrunnurnuuurhuunnuhnrrurhhrrhnrhhnnrunrnuuhunnruuhuhnrrnhuhhunrurruunhnnnhrruunuuhurhnrrhhnhuhnrhurhunuurnrhrrhhrrrhnuuhnruhrhhhhnhnnuurnhruurh'}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Completed translation 1 of 427 at 1715990048.5403686\n","Completed translation 2 of 427 at 1715990051.2432992\n","Completed translation 3 of 427 at 1715990053.9420288\n","Completed translation 4 of 427 at 1715990056.6343422\n","Completed translation 5 of 427 at 1715990059.3552709\n","Completed translation 6 of 427 at 1715990062.036751\n","Completed translation 7 of 427 at 1715990064.7065768\n","Completed translation 8 of 427 at 1715990067.3826494\n","Completed translation 9 of 427 at 1715990070.0822623\n","Completed translation 10 of 427 at 1715990072.8249843\n","Completed translation 11 of 427 at 1715990075.5032778\n","Completed translation 12 of 427 at 1715990078.176761\n","Completed translation 13 of 427 at 1715990080.8601973\n","Completed translation 14 of 427 at 1715990083.569607\n","Completed translation 15 of 427 at 1715990086.2678142\n","Completed translation 16 of 427 at 1715990088.960275\n","Completed translation 17 of 427 at 1715990091.714463\n","Completed translation 18 of 427 at 1715990094.3828115\n","Completed translation 19 of 427 at 1715990097.0979567\n","Completed translation 20 of 427 at 1715990099.7721236\n","Completed translation 21 of 427 at 1715990102.4237943\n","Completed translation 22 of 427 at 1715990105.108375\n","Completed translation 23 of 427 at 1715990107.817806\n","Completed translation 24 of 427 at 1715990110.4916613\n","Completed translation 25 of 427 at 1715990113.1590886\n","Completed translation 26 of 427 at 1715990115.846746\n","Completed translation 27 of 427 at 1715990118.5294008\n","Completed translation 28 of 427 at 1715990121.2553177\n","Completed translation 29 of 427 at 1715990123.9520943\n","Completed translation 30 of 427 at 1715990126.6201224\n","Completed translation 31 of 427 at 1715990129.2777095\n","Completed translation 32 of 427 at 1715990131.9949648\n","Completed translation 33 of 427 at 1715990134.6986165\n","Completed translation 34 of 427 at 1715990137.3725355\n","Completed translation 35 of 427 at 1715990140.0554388\n","Completed translation 36 of 427 at 1715990142.7172728\n","Completed translation 37 of 427 at 1715990145.4344816\n","Completed translation 38 of 427 at 1715990148.0891337\n","Completed translation 39 of 427 at 1715990150.7720559\n","Completed translation 40 of 427 at 1715990153.4383755\n","Completed translation 41 of 427 at 1715990156.158601\n","Completed translation 42 of 427 at 1715990158.8510027\n","Completed translation 43 of 427 at 1715990161.516937\n","Completed translation 44 of 427 at 1715990164.2288222\n","Completed translation 45 of 427 at 1715990166.8757105\n","Completed translation 46 of 427 at 1715990169.5978005\n","Completed translation 47 of 427 at 1715990172.2974808\n","Completed translation 48 of 427 at 1715990174.966937\n","Completed translation 49 of 427 at 1715990177.6146688\n","Completed translation 50 of 427 at 1715990180.3159528\n","Completed translation 51 of 427 at 1715990183.0376658\n","Completed translation 52 of 427 at 1715990185.684933\n","Completed translation 53 of 427 at 1715990188.3722074\n","Completed translation 54 of 427 at 1715990191.0293615\n","Completed translation 55 of 427 at 1715990193.70318\n","Completed translation 56 of 427 at 1715990196.369199\n","Completed translation 57 of 427 at 1715990199.0413997\n","Completed translation 58 of 427 at 1715990201.728623\n","Completed translation 59 of 427 at 1715990204.3875453\n","Completed translation 60 of 427 at 1715990207.111289\n","Completed translation 61 of 427 at 1715990209.79367\n","Completed translation 62 of 427 at 1715990212.4701297\n","Completed translation 63 of 427 at 1715990215.136751\n","Completed translation 64 of 427 at 1715990217.8908322\n","Completed translation 65 of 427 at 1715990220.573803\n","Completed translation 66 of 427 at 1715990223.2310698\n","Completed translation 67 of 427 at 1715990225.9070966\n","Completed translation 68 of 427 at 1715990228.6120589\n","Completed translation 69 of 427 at 1715990231.3066258\n","Completed translation 70 of 427 at 1715990233.9843178\n","Completed translation 71 of 427 at 1715990236.6639836\n","Completed translation 72 of 427 at 1715990239.3123295\n","Completed translation 73 of 427 at 1715990242.0261927\n","Completed translation 74 of 427 at 1715990244.7326581\n","Completed translation 75 of 427 at 1715990247.4477704\n","Completed translation 76 of 427 at 1715990250.1172738\n","Completed translation 77 of 427 at 1715990252.810206\n","Completed translation 78 of 427 at 1715990255.554154\n","Completed translation 79 of 427 at 1715990258.2324462\n","Completed translation 80 of 427 at 1715990260.9141133\n","Completed translation 81 of 427 at 1715990263.5890512\n","Completed translation 82 of 427 at 1715990266.3142858\n","Completed translation 83 of 427 at 1715990269.0065887\n","Completed translation 84 of 427 at 1715990271.6755986\n","Completed translation 85 of 427 at 1715990274.3461955\n","Completed translation 86 of 427 at 1715990277.022088\n","Completed translation 87 of 427 at 1715990279.7336907\n","Completed translation 88 of 427 at 1715990282.4156961\n","Completed translation 89 of 427 at 1715990285.0954597\n","Completed translation 90 of 427 at 1715990287.7484975\n","Completed translation 91 of 427 at 1715990290.4647036\n","Completed translation 92 of 427 at 1715990293.177415\n","Completed translation 93 of 427 at 1715990295.828807\n","Completed translation 94 of 427 at 1715990298.5054379\n","Completed translation 95 of 427 at 1715990301.2213857\n","Completed translation 96 of 427 at 1715990303.9344885\n","Completed translation 97 of 427 at 1715990306.6061466\n","Completed translation 98 of 427 at 1715990309.287214\n","Completed translation 99 of 427 at 1715990311.9547923\n","Completed translation 100 of 427 at 1715990314.643958\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  69637 KiB |  89540 KiB |   2054 GiB |   2054 GiB |\n","|       from large pool |  52872 KiB |  52872 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16765 KiB |  36668 KiB |   2054 GiB |   2054 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  69637 KiB |  89540 KiB |   2054 GiB |   2054 GiB |\n","|       from large pool |  52872 KiB |  52872 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16765 KiB |  36668 KiB |   2054 GiB |   2054 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  68114 KiB |  88005 KiB |   2053 GiB |   2053 GiB |\n","|       from large pool |  51424 KiB |  51424 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16690 KiB |  36581 KiB |   2053 GiB |   2053 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  86016 KiB | 100352 KiB |   1090 MiB |   1006 MiB |\n","|       from large pool |  61440 KiB |  61440 KiB |    100 MiB |     40 MiB |\n","|       from small pool |  24576 KiB |  38912 KiB |    990 MiB |    966 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  16379 KiB |  24320 KiB |   2172 GiB |   2172 GiB |\n","|       from large pool |   8568 KiB |   8568 KiB |      0 GiB |      0 GiB |\n","|       from small pool |   7811 KiB |  15752 KiB |   2172 GiB |   2172 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     623    |    7618 K  |    7618 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     598    |    7618 K  |    7618 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     623    |    7618 K  |    7618 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     598    |    7618 K  |    7618 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      22    |     500    |     485    |\n","|       from large pool |       3    |       3    |       5    |       2    |\n","|       from small pool |      12    |      19    |     495    |     483    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       9    |      26    |    3131 K  |    3131 K  |\n","|       from large pool |       2    |       2    |       0 K  |       0 K  |\n","|       from small pool |       7    |      24    |    3131 K  |    3131 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation100: {'source': 'Компания 20 жылдық қызмет мерзімі ішінде Айлзға қатысты осындай сценарийді ұстанды.Ģ', 'target': 'The company followed essentially the same script with regard to Ailes during his 20-year tenure.Ģ', 'translation': 'onhrrhhunhnrnruhnrunhhunrnnhruuunrrhuhrnuurhhrnuurnrhrnrrrhruhruuhururnnnnuurnuuhruhnrnQ))??)?))?)?]Q)QQ]?]?Q])Q?Q?QQ)?])]]QQ?]Q)Q]???QQ)))QQ)Q??])]]]]?]))?]??]Q??)Q))]]?Q?Q]])]?]?QQ?)]?)??QQ?Q]]]?Q])?])Q]])?])]??)?))]])Q)?Q])]]]]?Q)]?)])?)?)QQQ]]?)Q)?)QQ?'}\n","Completed translation 101 of 427 at 1715990317.3137772\n","Completed translation 102 of 427 at 1715990320.0024517\n","Completed translation 103 of 427 at 1715990322.6663268\n","Completed translation 104 of 427 at 1715990325.351072\n","Completed translation 105 of 427 at 1715990328.1148145\n","Completed translation 106 of 427 at 1715990330.812528\n","Completed translation 107 of 427 at 1715990333.4532738\n","Completed translation 108 of 427 at 1715990336.1430674\n","Completed translation 109 of 427 at 1715990338.8488429\n","Completed translation 110 of 427 at 1715990341.528544\n","Completed translation 111 of 427 at 1715990344.2022655\n","Completed translation 112 of 427 at 1715990346.8810139\n","Completed translation 113 of 427 at 1715990349.5699286\n","Completed translation 114 of 427 at 1715990352.3106244\n","Completed translation 115 of 427 at 1715990354.9920435\n","Completed translation 116 of 427 at 1715990357.6840174\n","Completed translation 117 of 427 at 1715990360.3355134\n","Completed translation 118 of 427 at 1715990363.035196\n","Completed translation 119 of 427 at 1715990365.7518342\n","Completed translation 120 of 427 at 1715990368.415433\n","Completed translation 121 of 427 at 1715990371.0814235\n","Completed translation 122 of 427 at 1715990373.7486317\n","Completed translation 123 of 427 at 1715990376.4483507\n","Completed translation 124 of 427 at 1715990379.1050436\n","Completed translation 125 of 427 at 1715990381.773889\n","Completed translation 126 of 427 at 1715990384.456736\n","Completed translation 127 of 427 at 1715990387.132032\n","Completed translation 128 of 427 at 1715990389.8258803\n","Completed translation 129 of 427 at 1715990392.489183\n","Completed translation 130 of 427 at 1715990395.1861253\n","Completed translation 131 of 427 at 1715990397.84895\n","Completed translation 132 of 427 at 1715990400.6002936\n","Completed translation 133 of 427 at 1715990403.2821646\n","Completed translation 134 of 427 at 1715990405.9590435\n","Completed translation 135 of 427 at 1715990408.6502483\n","Completed translation 136 of 427 at 1715990411.330249\n","Completed translation 137 of 427 at 1715990414.0031137\n","Completed translation 138 of 427 at 1715990416.6637933\n","Completed translation 139 of 427 at 1715990419.3325953\n","Completed translation 140 of 427 at 1715990422.0225406\n","Completed translation 141 of 427 at 1715990424.728185\n","Completed translation 142 of 427 at 1715990427.3831446\n","Completed translation 143 of 427 at 1715990430.0461292\n","Completed translation 144 of 427 at 1715990432.7091427\n","Completed translation 145 of 427 at 1715990435.3700647\n","Completed translation 146 of 427 at 1715990438.0801926\n","Completed translation 147 of 427 at 1715990440.770749\n","Completed translation 148 of 427 at 1715990443.4503775\n","Completed translation 149 of 427 at 1715990446.1194284\n","Completed translation 150 of 427 at 1715990448.8121288\n","Completed translation 151 of 427 at 1715990451.4890037\n","Completed translation 152 of 427 at 1715990454.167445\n","Completed translation 153 of 427 at 1715990456.832955\n","Completed translation 154 of 427 at 1715990459.5057375\n","Completed translation 155 of 427 at 1715990462.2165968\n","Completed translation 156 of 427 at 1715990464.8964415\n","Completed translation 157 of 427 at 1715990467.5992243\n","Completed translation 158 of 427 at 1715990470.2751014\n","Completed translation 159 of 427 at 1715990473.0017536\n","Completed translation 160 of 427 at 1715990475.7212644\n","Completed translation 161 of 427 at 1715990478.4114873\n","Completed translation 162 of 427 at 1715990481.072261\n","Completed translation 163 of 427 at 1715990483.7595866\n","Completed translation 164 of 427 at 1715990486.4864843\n","Completed translation 165 of 427 at 1715990489.149639\n","Completed translation 166 of 427 at 1715990491.8408833\n","Completed translation 167 of 427 at 1715990494.515393\n","Completed translation 168 of 427 at 1715990497.265068\n","Completed translation 169 of 427 at 1715990499.9640925\n","Completed translation 170 of 427 at 1715990502.6274493\n","Completed translation 171 of 427 at 1715990505.3088036\n","Completed translation 172 of 427 at 1715990508.0143332\n","Completed translation 173 of 427 at 1715990510.7375922\n","Completed translation 174 of 427 at 1715990513.41234\n","Completed translation 175 of 427 at 1715990516.093557\n","Completed translation 176 of 427 at 1715990518.765953\n","Completed translation 177 of 427 at 1715990521.528263\n","Completed translation 178 of 427 at 1715990524.2644572\n","Completed translation 179 of 427 at 1715990526.9552925\n","Completed translation 180 of 427 at 1715990529.6551476\n","Completed translation 181 of 427 at 1715990532.3922112\n","Completed translation 182 of 427 at 1715990535.1460748\n","Completed translation 183 of 427 at 1715990537.8262112\n","Completed translation 184 of 427 at 1715990540.520895\n","Completed translation 185 of 427 at 1715990543.2062237\n","Completed translation 186 of 427 at 1715990545.9884992\n","Completed translation 187 of 427 at 1715990548.713779\n","Completed translation 188 of 427 at 1715990551.4280624\n","Completed translation 189 of 427 at 1715990554.11404\n","Completed translation 190 of 427 at 1715990556.8057551\n","Completed translation 191 of 427 at 1715990559.5640483\n","Completed translation 192 of 427 at 1715990562.2351482\n","Completed translation 193 of 427 at 1715990564.9092572\n","Completed translation 194 of 427 at 1715990567.590553\n","Completed translation 195 of 427 at 1715990570.3084006\n","Completed translation 196 of 427 at 1715990573.022661\n","Completed translation 197 of 427 at 1715990575.6744943\n","Completed translation 198 of 427 at 1715990578.3691654\n","Completed translation 199 of 427 at 1715990581.046494\n","Completed translation 200 of 427 at 1715990583.7393343\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  69637 KiB |  96439 KiB |   4374 GiB |   4374 GiB |\n","|       from large pool |  52872 KiB |  52872 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16765 KiB |  43567 KiB |   4374 GiB |   4374 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  69637 KiB |  96439 KiB |   4374 GiB |   4374 GiB |\n","|       from large pool |  52872 KiB |  52872 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16765 KiB |  43567 KiB |   4374 GiB |   4374 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  68114 KiB |  94892 KiB |   4373 GiB |   4373 GiB |\n","|       from large pool |  51424 KiB |  51424 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16690 KiB |  43468 KiB |   4373 GiB |   4373 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  86016 KiB | 108544 KiB |   2802 MiB |   2718 MiB |\n","|       from large pool |  61440 KiB |  61440 KiB |    100 MiB |     40 MiB |\n","|       from small pool |  24576 KiB |  47104 KiB |   2702 MiB |   2678 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  16379 KiB |  28360 KiB |   4644 GiB |   4644 GiB |\n","|       from large pool |   8568 KiB |   8568 KiB |      0 GiB |      0 GiB |\n","|       from small pool |   7811 KiB |  19792 KiB |   4644 GiB |   4644 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     623    |   15161 K  |   15161 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     598    |   15161 K  |   15161 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     623    |   15161 K  |   15161 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     598    |   15161 K  |   15161 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      26    |    1356    |    1341    |\n","|       from large pool |       3    |       3    |       5    |       2    |\n","|       from small pool |      12    |      23    |    1351    |    1339    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       9    |      27    |    6254 K  |    6254 K  |\n","|       from large pool |       2    |       2    |       0 K  |       0 K  |\n","|       from small pool |       7    |      25    |    6254 K  |    6254 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation200: {'source': '1981 жылы билікке модернист Махатхир келген соң Ислам – PAS-тың UMNO-ға қарсы ең тиімді идеологиялық қаруына айналды.Ģ', 'target': \"When the modernist Mahathir came to power in 1981, Islamism became the PAS's most effective ideological weapon against the UMNO.Ģ\", 'translation': 'ornnrhuhnuuuhnhhnrhuurrnrhnhuuhrnrhuunnnhnurhhununuuhhhnununurhhnnnurhhnnurruhhnhrhnhhn)Q]]QQ])?])]])]?QQQ]?)?]?)?]Q?]?))))Q?)]QQ?Q]Q])]Q))Q?)??)QQ?Q?)?]])Q???]Q?Q]]?)?QQ)?)Q]]]Q]]?])???)?))Q]])Q?]?Q??]))QQ])??]?)Q]))))])]Q)]))))??)Q]))Q))]]?)Q)))Q?Q)Q)Q]Q'}\n","Completed translation 201 of 427 at 1715990586.4476044\n","Completed translation 202 of 427 at 1715990589.1525235\n","Completed translation 203 of 427 at 1715990591.8432958\n","Completed translation 204 of 427 at 1715990594.5430832\n","Completed translation 205 of 427 at 1715990597.2361598\n","Completed translation 206 of 427 at 1715990599.885241\n","Completed translation 207 of 427 at 1715990602.571896\n","Completed translation 208 of 427 at 1715990605.2588098\n","Completed translation 209 of 427 at 1715990607.9695368\n","Completed translation 210 of 427 at 1715990610.6548944\n","Completed translation 211 of 427 at 1715990613.371326\n","Completed translation 212 of 427 at 1715990616.0485294\n","Completed translation 213 of 427 at 1715990618.757047\n","Completed translation 214 of 427 at 1715990621.4748783\n","Completed translation 215 of 427 at 1715990624.171766\n","Completed translation 216 of 427 at 1715990626.8616445\n","Completed translation 217 of 427 at 1715990629.5619328\n","Completed translation 218 of 427 at 1715990632.2852032\n","Completed translation 219 of 427 at 1715990634.9922168\n","Completed translation 220 of 427 at 1715990637.6918743\n","Completed translation 221 of 427 at 1715990640.395471\n","Completed translation 222 of 427 at 1715990643.1220617\n","Completed translation 223 of 427 at 1715990645.8013933\n","Completed translation 224 of 427 at 1715990648.5024073\n","Completed translation 225 of 427 at 1715990651.2213433\n","Completed translation 226 of 427 at 1715990653.9210343\n","Completed translation 227 of 427 at 1715990656.647723\n","Completed translation 228 of 427 at 1715990659.3522074\n","Completed translation 229 of 427 at 1715990662.063699\n","Completed translation 230 of 427 at 1715990664.771774\n","Completed translation 231 of 427 at 1715990667.4819276\n","Completed translation 232 of 427 at 1715990670.2198172\n","Completed translation 233 of 427 at 1715990672.9241047\n","Completed translation 234 of 427 at 1715990675.5939443\n","Completed translation 235 of 427 at 1715990678.3309193\n","Completed translation 236 of 427 at 1715990681.0768874\n","Completed translation 237 of 427 at 1715990683.766313\n","Completed translation 238 of 427 at 1715990686.4664712\n","Completed translation 239 of 427 at 1715990689.1489143\n","Completed translation 240 of 427 at 1715990691.8606257\n","Completed translation 241 of 427 at 1715990694.5687842\n","Completed translation 242 of 427 at 1715990697.28939\n","Completed translation 243 of 427 at 1715990699.9999096\n","Completed translation 244 of 427 at 1715990702.6898463\n","Completed translation 245 of 427 at 1715990705.4126556\n","Completed translation 246 of 427 at 1715990708.1332493\n","Completed translation 247 of 427 at 1715990710.834568\n","Completed translation 248 of 427 at 1715990713.5260901\n","Completed translation 249 of 427 at 1715990716.2322297\n","Completed translation 250 of 427 at 1715990718.9261367\n","Completed translation 251 of 427 at 1715990721.6213703\n","Completed translation 252 of 427 at 1715990724.3226972\n","Completed translation 253 of 427 at 1715990727.00759\n","Completed translation 254 of 427 at 1715990729.6965816\n","Completed translation 255 of 427 at 1715990732.3972735\n","Completed translation 256 of 427 at 1715990735.0974932\n","Completed translation 257 of 427 at 1715990737.7878482\n","Completed translation 258 of 427 at 1715990740.500295\n","Completed translation 259 of 427 at 1715990743.1833441\n","Completed translation 260 of 427 at 1715990745.9004629\n","Completed translation 261 of 427 at 1715990748.5642009\n","Completed translation 262 of 427 at 1715990751.2775455\n","Completed translation 263 of 427 at 1715990753.9965138\n","Completed translation 264 of 427 at 1715990756.6882086\n","Completed translation 265 of 427 at 1715990759.3741107\n","Completed translation 266 of 427 at 1715990762.0800922\n","Completed translation 267 of 427 at 1715990764.7463856\n","Completed translation 268 of 427 at 1715990767.4555137\n","Completed translation 269 of 427 at 1715990770.1737397\n","Completed translation 270 of 427 at 1715990772.8687186\n","Completed translation 271 of 427 at 1715990775.5676935\n","Completed translation 272 of 427 at 1715990778.2526898\n","Completed translation 273 of 427 at 1715990780.9997857\n","Completed translation 274 of 427 at 1715990783.7058907\n","Completed translation 275 of 427 at 1715990786.4084234\n","Completed translation 276 of 427 at 1715990789.1412818\n","Completed translation 277 of 427 at 1715990791.846406\n","Completed translation 278 of 427 at 1715990794.5044084\n","Completed translation 279 of 427 at 1715990797.1900957\n","Completed translation 280 of 427 at 1715990799.9034696\n","Completed translation 281 of 427 at 1715990802.642858\n","Completed translation 282 of 427 at 1715990805.3231845\n","Completed translation 283 of 427 at 1715990808.0124905\n","Completed translation 284 of 427 at 1715990810.6891584\n","Completed translation 285 of 427 at 1715990813.412627\n","Completed translation 286 of 427 at 1715990816.1540222\n","Completed translation 287 of 427 at 1715990818.859192\n","Completed translation 288 of 427 at 1715990821.5290346\n","Completed translation 289 of 427 at 1715990824.2321322\n","Completed translation 290 of 427 at 1715990826.9451234\n","Completed translation 291 of 427 at 1715990829.6066234\n","Completed translation 292 of 427 at 1715990832.3017051\n","Completed translation 293 of 427 at 1715990835.0337582\n","Completed translation 294 of 427 at 1715990837.7368038\n","Completed translation 295 of 427 at 1715990840.4515436\n","Completed translation 296 of 427 at 1715990843.1602342\n","Completed translation 297 of 427 at 1715990845.8467705\n","Completed translation 298 of 427 at 1715990848.535454\n","Completed translation 299 of 427 at 1715990851.2380419\n","Completed translation 300 of 427 at 1715990853.9307806\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  69637 KiB | 104142 KiB |   6966 GiB |   6966 GiB |\n","|       from large pool |  52872 KiB |  68359 KiB |    244 GiB |    244 GiB |\n","|       from small pool |  16765 KiB |  46951 KiB |   6722 GiB |   6722 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  69637 KiB | 104142 KiB |   6966 GiB |   6966 GiB |\n","|       from large pool |  52872 KiB |  68359 KiB |    244 GiB |    244 GiB |\n","|       from small pool |  16765 KiB |  46951 KiB |   6722 GiB |   6722 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  68114 KiB | 102265 KiB |   6955 GiB |   6955 GiB |\n","|       from large pool |  51424 KiB |  66409 KiB |    233 GiB |    233 GiB |\n","|       from small pool |  16690 KiB |  46849 KiB |   6721 GiB |   6721 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  86016 KiB | 124928 KiB |   5618 MiB |   5534 MiB |\n","|       from large pool |  61440 KiB |  81920 KiB |    960 MiB |    900 MiB |\n","|       from small pool |  24576 KiB |  49152 KiB |   4658 MiB |   4634 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  16379 KiB |  29385 KiB |   7579 GiB |   7579 GiB |\n","|       from large pool |   8568 KiB |  21672 KiB |    436 GiB |    436 GiB |\n","|       from small pool |   7811 KiB |  20817 KiB |   7143 GiB |   7143 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     623    |   22704 K  |   22703 K  |\n","|       from large pool |      25    |      38    |     223 K  |     223 K  |\n","|       from small pool |     415    |     597    |   22480 K  |   22479 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     623    |   22704 K  |   22703 K  |\n","|       from large pool |      25    |      38    |     223 K  |     223 K  |\n","|       from small pool |     415    |     597    |   22480 K  |   22479 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      27    |    2377    |    2362    |\n","|       from large pool |       3    |       4    |      48    |      45    |\n","|       from small pool |      12    |      24    |    2329    |    2317    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       9    |      28    |    9421 K  |    9421 K  |\n","|       from large pool |       2    |       4    |     146 K  |     146 K  |\n","|       from small pool |       7    |      26    |    9274 K  |    9274 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation300: {'source': 'ЛОНДОН – Жүйе құрушы орталық банктердің қорғанысты өзгерте бастауы 2018-2019 жылдары жаңа ақша-несие саясаты дәуірінің басталатынынан хабар берсе керек.Ģ', 'target': 'LONDON - The changing of the guard that is taking place at the systemically important central banks in 2018-2019 will mark the beginning of a new era of monetary policy.Ģ', 'translation': 'ohnrrrunuruhrrhhuruhrhnhhuhhhhnnuhurhhunuhnrnrhunrnrnuhuruuhrhuunurnnnrrnhnnnhhurhuhuur)Q??Q?Q]))))]?]??]Q)?]])?Q]?)?))]])?????)]]Q]]])?]?QQQ]?])]?)Q??))]?Q]])]?]]??]]])Q)]))]Q]?Q)])]]?Q))QQ])??)?QQ)?)?]]?])?)Q?]Q?]?]?QQ??QQ)])]Q)]??Q)]]?)?])??])?]))Q?Q]Q?'}\n","Completed translation 301 of 427 at 1715990856.6115513\n","Completed translation 302 of 427 at 1715990859.3085644\n","Completed translation 303 of 427 at 1715990862.0224686\n","Completed translation 304 of 427 at 1715990864.7261963\n","Completed translation 305 of 427 at 1715990867.4129145\n","Completed translation 306 of 427 at 1715990870.0762875\n","Completed translation 307 of 427 at 1715990872.7585952\n","Completed translation 308 of 427 at 1715990875.4862883\n","Completed translation 309 of 427 at 1715990878.191098\n","Completed translation 310 of 427 at 1715990880.9055078\n","Completed translation 311 of 427 at 1715990883.5641894\n","Completed translation 312 of 427 at 1715990886.2864885\n","Completed translation 313 of 427 at 1715990888.9659264\n","Completed translation 314 of 427 at 1715990891.6702445\n","Completed translation 315 of 427 at 1715990894.3344238\n","Completed translation 316 of 427 at 1715990897.0121408\n","Completed translation 317 of 427 at 1715990899.7184837\n","Completed translation 318 of 427 at 1715990902.3960958\n","Completed translation 319 of 427 at 1715990905.100178\n","Completed translation 320 of 427 at 1715990907.7715857\n","Completed translation 321 of 427 at 1715990910.4740326\n","Completed translation 322 of 427 at 1715990913.142142\n","Completed translation 323 of 427 at 1715990915.8036838\n","Completed translation 324 of 427 at 1715990918.4964771\n","Completed translation 325 of 427 at 1715990921.144199\n","Completed translation 326 of 427 at 1715990923.8506289\n","Completed translation 327 of 427 at 1715990926.5303257\n","Completed translation 328 of 427 at 1715990929.2333274\n","Completed translation 329 of 427 at 1715990931.9065115\n","Completed translation 330 of 427 at 1715990934.592521\n","Completed translation 331 of 427 at 1715990937.306798\n","Completed translation 332 of 427 at 1715990939.9747021\n","Completed translation 333 of 427 at 1715990942.629212\n","Completed translation 334 of 427 at 1715990945.3223875\n","Completed translation 335 of 427 at 1715990948.0484867\n","Completed translation 336 of 427 at 1715990950.7468927\n","Completed translation 337 of 427 at 1715990953.4202118\n","Completed translation 338 of 427 at 1715990956.092352\n","Completed translation 339 of 427 at 1715990958.7717848\n","Completed translation 340 of 427 at 1715990961.4934201\n","Completed translation 341 of 427 at 1715990964.1682386\n","Completed translation 342 of 427 at 1715990966.8398728\n","Completed translation 343 of 427 at 1715990969.491737\n","Completed translation 344 of 427 at 1715990972.1945844\n","Completed translation 345 of 427 at 1715990974.8674858\n","Completed translation 346 of 427 at 1715990977.5227928\n","Completed translation 347 of 427 at 1715990980.1951234\n","Completed translation 348 of 427 at 1715990982.8693023\n","Completed translation 349 of 427 at 1715990985.5036316\n","Completed translation 350 of 427 at 1715990988.1916027\n","Completed translation 351 of 427 at 1715990990.880044\n","Completed translation 352 of 427 at 1715990993.558418\n","Completed translation 353 of 427 at 1715990996.2391803\n","Completed translation 354 of 427 at 1715990998.9043233\n","Completed translation 355 of 427 at 1715991001.5771303\n","Completed translation 356 of 427 at 1715991004.2438202\n","Completed translation 357 of 427 at 1715991006.9148836\n","Completed translation 358 of 427 at 1715991009.5882761\n","Completed translation 359 of 427 at 1715991012.2852929\n","Completed translation 360 of 427 at 1715991014.9619687\n","Completed translation 361 of 427 at 1715991017.6137762\n","Completed translation 362 of 427 at 1715991020.3226013\n","Completed translation 363 of 427 at 1715991022.9928312\n","Completed translation 364 of 427 at 1715991025.6565156\n","Completed translation 365 of 427 at 1715991028.344343\n","Completed translation 366 of 427 at 1715991031.0307174\n","Completed translation 367 of 427 at 1715991033.6939254\n","Completed translation 368 of 427 at 1715991036.3583143\n","Completed translation 369 of 427 at 1715991039.051364\n","Completed translation 370 of 427 at 1715991041.7145333\n","Completed translation 371 of 427 at 1715991044.4002354\n","Completed translation 372 of 427 at 1715991047.0750587\n","Completed translation 373 of 427 at 1715991049.748623\n","Completed translation 374 of 427 at 1715991052.3978522\n","Completed translation 375 of 427 at 1715991055.0675526\n","Completed translation 376 of 427 at 1715991057.7785923\n","Completed translation 377 of 427 at 1715991060.4294674\n","Completed translation 378 of 427 at 1715991063.1077137\n","Completed translation 379 of 427 at 1715991065.7624094\n","Completed translation 380 of 427 at 1715991068.434722\n","Completed translation 381 of 427 at 1715991071.1237497\n","Completed translation 382 of 427 at 1715991073.8181252\n","Completed translation 383 of 427 at 1715991076.4774632\n","Completed translation 384 of 427 at 1715991079.1334238\n","Completed translation 385 of 427 at 1715991081.812038\n","Completed translation 386 of 427 at 1715991084.4817722\n","Completed translation 387 of 427 at 1715991087.143995\n","Completed translation 388 of 427 at 1715991089.78601\n","Completed translation 389 of 427 at 1715991092.4595249\n","Completed translation 390 of 427 at 1715991095.1431637\n","Completed translation 391 of 427 at 1715991097.8175468\n","Completed translation 392 of 427 at 1715991100.4973242\n","Completed translation 393 of 427 at 1715991103.1757658\n","Completed translation 394 of 427 at 1715991105.8889441\n","Completed translation 395 of 427 at 1715991108.547112\n","Completed translation 396 of 427 at 1715991111.2293591\n","Completed translation 397 of 427 at 1715991113.906961\n","Completed translation 398 of 427 at 1715991116.5617204\n","Completed translation 399 of 427 at 1715991119.2417467\n","Completed translation 400 of 427 at 1715991121.9109492\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  69637 KiB | 116682 KiB |   9910 GiB |   9910 GiB |\n","|       from large pool |  52872 KiB |  73930 KiB |    927 GiB |    927 GiB |\n","|       from small pool |  16765 KiB |  46093 KiB |   8983 GiB |   8983 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  69637 KiB | 116682 KiB |   9910 GiB |   9910 GiB |\n","|       from large pool |  52872 KiB |  73930 KiB |    927 GiB |    927 GiB |\n","|       from small pool |  16765 KiB |  46093 KiB |   8983 GiB |   8983 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  68114 KiB | 114559 KiB |   9870 GiB |   9870 GiB |\n","|       from large pool |  51424 KiB |  71888 KiB |    889 GiB |    889 GiB |\n","|       from small pool |  16690 KiB |  45990 KiB |   8981 GiB |   8981 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  86016 KiB | 129024 KiB |   9512 MiB |   9428 MiB |\n","|       from large pool |  61440 KiB |  81920 KiB |   2940 MiB |   2880 MiB |\n","|       from small pool |  24576 KiB |  49152 KiB |   6572 MiB |   6548 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  16379 KiB |  46784 KiB |  11170 GiB |  11170 GiB |\n","|       from large pool |   8568 KiB |  27602 KiB |   1550 GiB |   1550 GiB |\n","|       from small pool |   7811 KiB |  19181 KiB |   9620 GiB |   9620 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     623    |   30246 K  |   30246 K  |\n","|       from large pool |      25    |      38    |     744 K  |     744 K  |\n","|       from small pool |     415    |     595    |   29501 K  |   29501 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     623    |   30246 K  |   30246 K  |\n","|       from large pool |      25    |      38    |     744 K  |     744 K  |\n","|       from small pool |     415    |     595    |   29501 K  |   29501 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      27    |    3433    |    3418    |\n","|       from large pool |       3    |       4    |     147    |     144    |\n","|       from small pool |      12    |      24    |    3286    |    3274    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       9    |      29    |   12868 K  |   12868 K  |\n","|       from large pool |       2    |       5    |     458 K  |     458 K  |\n","|       from small pool |       7    |      26    |   12410 K  |   12410 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation400: {'source': 'Сақтандырылмаған банк депозиттеріне байланысты тәуекелге бармай-ақ қолма-қол ақшаға оңай қол жеткізгісі келетін ұйымдарға Қазынашылықтың қысқа мерзімді вексельдері жетпегендіктен, бір түндік ипотекалық пул нарығы өркен жайды.Ģ', 'target': \"The overnight mortgage-pool market exists partly because there aren't enough short-term US Treasury bills available for businesses that want easy access to cash without the risk implied by uninsured bank deposits.Ģ\", 'translation': 'Kuurrhhnhhnuurhhhnrrunnrnrhhhuhhhhuunhnuurrnrrrhuhhruhnhnnrrnhuuhnrhhrrnunhnrhnurnunrnu]Q?Q]Q))?Q])??)?]?]??))?]Q???]])?]]])?]?]??))?)]?]?]Q)??])?))??QQ??)QQQ?)])]]]QQQ]]?))Q]????)Q)Q?)]))?Q)?]]]??Q))?]?))?))))]Q)Q]Q)?)))]Q))Q?]?Q???]QQQ]]Q]))?]QQ?)Q))]??Q'}\n","Completed translation 401 of 427 at 1715991124.6034625\n","Completed translation 402 of 427 at 1715991127.263498\n","Completed translation 403 of 427 at 1715991129.9937537\n","Completed translation 404 of 427 at 1715991132.6758084\n","Completed translation 405 of 427 at 1715991135.3457851\n","Completed translation 406 of 427 at 1715991138.0081542\n","Completed translation 407 of 427 at 1715991140.6911201\n","Completed translation 408 of 427 at 1715991143.3532305\n","Completed translation 409 of 427 at 1715991146.0296636\n","Completed translation 410 of 427 at 1715991148.7138329\n","Completed translation 411 of 427 at 1715991151.391836\n","Completed translation 412 of 427 at 1715991154.1403306\n","Completed translation 413 of 427 at 1715991156.814997\n","Completed translation 414 of 427 at 1715991159.5118454\n","Completed translation 415 of 427 at 1715991162.145767\n","Completed translation 416 of 427 at 1715991164.8070457\n","Completed translation 417 of 427 at 1715991167.497923\n","Completed translation 418 of 427 at 1715991170.1720693\n","Completed translation 419 of 427 at 1715991172.8286352\n","Completed translation 420 of 427 at 1715991175.4874177\n","Completed translation 421 of 427 at 1715991178.1829996\n","Completed translation 422 of 427 at 1715991180.8652914\n","Completed translation 423 of 427 at 1715991183.5324318\n","Completed translation 424 of 427 at 1715991186.2482443\n","Completed translation 425 of 427 at 1715991188.9292471\n","Completed translation 426 of 427 at 1715991191.5824578\n","Translation426: {'source': 'Басқаша айтқанда, адам құқықтарын түсіндіруде мәдениет адамдардың көбі ойлағаннан әлдеқайда үлкен рөл атқарады, сондықтан адам құқықтарын қорғаушылар мәдени не діни тамыры тереңде жатқан әлдебір мәселе жөнінде үстірт пікір айтудан сақ болғаны жөн.Ģ', 'target': 'In other words, culture plays a much larger role in shaping interpretations of human rights than many realize, which implies that human-rights practitioners should be wary of passing judgment on any practice with deep cultural or religious roots.Ģ', 'translation': 'vhrururhhnrrnnhhhuhunhunuhnrrrhrhnrhhnruhrnhrnnrunuhnrhhrnhnunhnnhuuuuhuruuhhhnnrrrnnhh))?]]]Q]??)QQ????])Q]]]?Q]]])??Q]?]?]?QQQ?]])?]Q]?Q]Q)Q)?]QQ?))))))Q?Q?)]?]?Q)?)]]?]Q]Q])QQ]))]?))?)Q]Q]QQ]))QQQ]]]]?))]Q?)]??Q)Q]Q)]?)??)Q)?Q]]]]))Q)??Q])]]]Q?Q??)Q))?Q'}\n"]}],"source":["from google.colab import drive\n","\n","import json\n","import random\n","import time\n","from typing import Optional\n","\n","import torch\n","import numpy as np\n","\n","drive.mount('/content/drive', force_remount=True)\n","root_filepath = '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/'\n","is_remote_execution = True\n","torch.set_printoptions(threshold=100000, edgeitems=10000, linewidth=100000)\n","torch.device(\"cuda\")\n","\n","\n","# 16M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_0 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_0',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_0-1715937924-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586293\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_0.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 2048,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 15M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_1 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_1',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_1-1715937961-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586974\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_1.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1796,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 4M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_2 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_2',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_2-1715936459-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586361\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_2.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 128,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1024,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 16M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_0 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_0',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_0-1715965639-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_0.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 2048,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 15M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_1 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_1',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_1-1715966141-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_1.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1796,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 4M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_2 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_2',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_2-1715965581-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_2.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 128,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1024,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","\n","class DatasetHolder:\n","\n","    def __init__(self):\n","        self.unknown_vocabulary_type = None\n","        self.padding_vocabulary_type = None\n","        self.end_of_sequence_type = None\n","        self.target_vocab = None\n","        self.target_vocab_array = None\n","        self.target_vocab_counts = None\n","        self.source_vocab = None\n","        self.source_vocab_array = None\n","        self.source_vocab_counts = None\n","        self.target_encodings = None\n","        self.target_encodings_train = None\n","        self.target_encodings_test = None\n","        self.source_encodings = None\n","        self.source_encodings_train = None\n","        self.source_encodings_test = None\n","        self.max_src_seq_obs = 0\n","        self.max_tgt_seq_obs = 0\n","\n","    def get_unknown_vocabulary_type(self):\n","        return self.unknown_vocabulary_type\n","\n","    def set_unknown_vocabulary_type(self, unknown_vocabulary_type):\n","        self.unknown_vocabulary_type = unknown_vocabulary_type\n","\n","    def get_padding_vocabulary_type(self):\n","        return self.padding_vocabulary_type\n","\n","    def set_padding_vocabulary_type(self, padding_vocabulary_type):\n","        self.padding_vocabulary_type = padding_vocabulary_type\n","\n","    def get_end_of_sequence_vocabulary_type(self):\n","        return self.end_of_sequence_type\n","\n","    def set_end_of_sequence_vocabulary_type(self, end_of_sequence_type):\n","        self.end_of_sequence_type = end_of_sequence_type\n","\n","    def get_target_vocab(self):\n","        return self.target_vocab\n","\n","    def set_target_vocab(self, target_vocab):\n","        self.target_vocab = target_vocab\n","\n","    def get_target_vocab_numpy(self):\n","        if self.target_vocab_array is None:\n","            self.target_vocab_array = np.array(self.target_vocab)\n","        return self.target_vocab_array\n","\n","    def get_target_vocab_counts(self):\n","        return self.target_vocab_counts\n","\n","    def set_target_vocab_counts(self, target_vocab_counts):\n","        self.target_vocab_counts = target_vocab_counts\n","\n","    def get_source_vocab(self):\n","        return self.source_vocab\n","\n","    def set_source_vocab(self, source_vocab):\n","        self.source_vocab = source_vocab\n","\n","    def get_source_vocab_numpy(self):\n","        if self.source_vocab_array is None:\n","            self.source_vocab_array = np.array(self.source_vocab)\n","        return self.source_vocab_array\n","\n","    def get_source_vocab_counts(self):\n","        return self.source_vocab_counts\n","\n","    def set_source_vocab_counts(self, source_vocab_counts):\n","        self.source_vocab_counts = source_vocab_counts\n","\n","    def get_target_encodings(self):\n","        return self.target_encodings\n","\n","    def set_target_encodings(self, target_encodings):\n","        del self.target_encodings\n","        self.target_encodings = target_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings(self):\n","        return self.source_encodings\n","\n","    def set_source_encodings(self, source_encodings):\n","        del self.source_encodings\n","        self.source_encodings = source_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_train(self):\n","        return self.target_encodings_train\n","\n","    def set_target_encodings_train(self, target_encodings_train):\n","        del self.target_encodings_train\n","        self.target_encodings_train = target_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_train(self):\n","        return self.source_encodings_train\n","\n","    def set_source_encodings_train(self, source_encodings_train):\n","        del self.source_encodings_train\n","        self.source_encodings_train = source_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_test(self):\n","        return self.target_encodings_test\n","\n","    def set_target_encodings_test(self, target_encodings_test):\n","        del self.target_encodings_test\n","        self.target_encodings_test = target_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_test(self):\n","        return self.source_encodings_test\n","\n","    def set_source_encodings_test(self, source_encodings_test):\n","        del self.source_encodings_test\n","        self.source_encodings_test = source_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_max_src_seq_obs(self):\n","        return self.max_src_seq_obs\n","\n","    def set_max_src_seq_obs(self, max_src_seq_obs):\n","        self.max_src_seq_obs = max_src_seq_obs\n","\n","    def get_max_tgt_seq_obs(self):\n","        return self.max_tgt_seq_obs\n","\n","    def set_max_tgt_seq_obs(self, max_tgt_seq_obs):\n","        self.max_tgt_seq_obs = max_tgt_seq_obs\n","\n","# class name matches file name\n","class transformer_vaswani2017(torch.nn.Transformer):\n","\n","    def __init__(self,\n","                 model_hyperparameters):\n","        super().__init__(\n","            d_model=model_hyperparameters['d_model'],\n","            nhead=model_hyperparameters['nhead'],\n","            num_encoder_layers=model_hyperparameters['num_encoder_layers'],\n","            num_decoder_layers=model_hyperparameters['num_decoder_layers'],\n","            dim_feedforward=model_hyperparameters['dim_feedforward'],\n","            dropout=model_hyperparameters['dropout'],\n","            activation=model_hyperparameters['activation'],\n","            custom_encoder=model_hyperparameters['custom_encoder'],\n","            custom_decoder=model_hyperparameters['custom_decoder'],\n","            layer_norm_eps=model_hyperparameters['layer_norm_eps'],\n","            batch_first=model_hyperparameters['batch_first'],\n","            norm_first=model_hyperparameters['norm_first'],\n","            bias=model_hyperparameters['bias'],\n","            device=model_hyperparameters['device'],\n","            dtype=model_hyperparameters['dtype']\n","        )\n","        self.model_hyperparameters = model_hyperparameters\n","        # add one for the end of sequence token\n","        self.max_src_seq_len = model_hyperparameters['max_src_seq_len'] + 1\n","        self.max_tgt_seq_len = model_hyperparameters['max_tgt_seq_len'] + 1\n","        self.src_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['src_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.tgt_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['tgt_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.src_pos_enc = torch.nn.Embedding(\n","            self.max_src_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.tgt_pos_enc = torch.nn.Embedding(\n","            self.max_tgt_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.linear_output_projection_1 = torch.nn.Linear(\n","            self.max_tgt_seq_len,\n","            1,\n","            bias=False\n","        )\n","        self.linear_output_projection_2 = torch.nn.Linear(\n","            model_hyperparameters['d_model'],\n","            model_hyperparameters['tgt_vocab_size'],\n","            bias=False\n","        )\n","        self.logsoftmax_output = torch.nn.LogSoftmax(dim=1)\n","        self.model_hyperparameters = model_hyperparameters\n","        self.tgt_mask_cache = {}\n","        for i in range(0, self.max_tgt_seq_len + 1):\n","            tgt_mask = self.generate_square_subsequent_mask(i)\n","            if is_remote_execution:\n","                self.tgt_mask_cache[i] = tgt_mask.to(device=\"cuda\")\n","            self.tgt_mask_cache[i] = tgt_mask\n","        self.indices_cache = {}\n","        for i in range(0, max([self.max_src_seq_len, self.max_tgt_seq_len]) + 1):\n","            indices = torch.tensor(np.arange(0, i), dtype=torch.long)\n","            if is_remote_execution:\n","                indices = indices.to(device=\"cuda\")\n","            self.indices_cache[i] = indices\n","\n","    def forward(self,\n","                src: torch.Tensor,\n","                tgt: torch.Tensor,\n","                src_mask: Optional[torch.Tensor] = None,\n","                tgt_mask: Optional[torch.Tensor] = None,\n","                memory_mask: Optional[torch.Tensor] = None,\n","                src_key_padding_mask: Optional[torch.Tensor] = None,\n","                tgt_key_padding_mask: Optional[torch.Tensor] = None,\n","                memory_key_padding_mask: Optional[torch.Tensor] = None,\n","                src_is_causal: Optional[bool] = None,\n","                tgt_is_causal: Optional[bool] = True,\n","                memory_is_causal: bool = False) -> torch.Tensor:\n","        if len(src.shape) == 1:\n","            src = src.unsqueeze(dim=0)\n","        if len(tgt.shape) == 1:\n","            tgt = tgt.unsqueeze(dim=0)\n","        src_embedding_pos_enc = self.src_embeddings(src) + self.src_pos_enc(self.indices_cache[src.shape[1]])\n","        tgt_embedding_pos_enc = self.tgt_embeddings(tgt) + self.tgt_pos_enc(self.indices_cache[tgt.shape[1]])\n","        tgt_mask = self.tgt_mask_cache[tgt.shape[1]]\n","        transformer_output = super().forward(src_embedding_pos_enc, tgt_embedding_pos_enc, src_mask,\n","                                             tgt_mask, memory_mask, src_key_padding_mask,\n","                                             tgt_key_padding_mask, memory_key_padding_mask,\n","                                             src_is_causal, tgt_is_causal, memory_is_causal)\n","        transformer_output = torch.swapaxes(transformer_output, -1, -2)\n","        transformer_output = torch.nn.functional.pad(\n","            transformer_output,\n","            (0, self.max_tgt_seq_len - transformer_output.shape[2]),\n","            value=0\n","        )\n","        output = self.logsoftmax_output(\n","            self.linear_output_projection_2(\n","                torch.squeeze(\n","                    self.linear_output_projection_1(\n","                        transformer_output\n","                    ),\n","                    dim=2\n","                )\n","            )\n","        )\n","        del src_embedding_pos_enc\n","        del tgt_embedding_pos_enc\n","        del transformer_output\n","        return output\n","\n","    def freeze_target_embeddings(self):\n","        del self.tgt_embeddings\n","        self.tgt_embeddings = torch.nn.Embedding(\n","            self.model_hyperparameters['tgt_vocab_size'],\n","            self.model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        del self.linear_output_projection_1\n","        self.linear_output_projection_1 = torch.nn.Linear(\n","            self.max_tgt_seq_len,\n","            1,\n","            bias=False\n","        ).requires_grad_(False)\n","        del self.linear_output_projection_2\n","        self.linear_output_projection_2 = torch.nn.Linear(\n","            self.model_hyperparameters['d_model'],\n","            self.model_hyperparameters['tgt_vocab_size'],\n","            bias=False\n","        ).requires_grad_(False)\n","\n","    def set_source_embeddings_for_transfer_learning(self, source_embeddings_dim):\n","        del self.src_embeddings\n","        self.src_embeddings = torch.nn.Embedding(\n","            source_embeddings_dim,\n","            self.model_hyperparameters['d_model']\n","        )\n","\n","\n","class Runner:\n","\n","    def __init__(self):\n","        self.runner_hyperparameters = NewsCommentaryByT5Vaswani2017Kocmi2018_1\n","        self.model_hyperparameters = self.runner_hyperparameters['model_hyperparameters']\n","        self.runner_hyperparameters_name = self.runner_hyperparameters['runner_hyperparameters_name']\n","        self.model_parameter_filepath = self.runner_hyperparameters['model_parameter_filepath']\n","        self.datasets_filepath = self.runner_hyperparameters['datasets_filepath']\n","        self.output_filepath = self.runner_hyperparameters['output_filepath']\n","        self.max_target_sequence_length = self.runner_hyperparameters['model_hyperparameters']['max_tgt_seq_len']\n","        self.dataset_holder: DatasetHolder = None\n","        self.model = None\n","        print(f\"Initialized runner {self.runner_hyperparameters_name} with parameters {self.runner_hyperparameters}\")\n","\n","    def load_dataset(self):\n","        self.dataset_holder = torch.load(self.datasets_filepath)\n","\n","    def load_model(self):\n","        self.model = transformer_vaswani2017(model_hyperparameters=self.model_hyperparameters)\n","        self.model.freeze_target_embeddings()\n","        if is_remote_execution:\n","            model_parameters = torch.load(self.model_parameter_filepath, map_location=torch.device('cuda'))\n","        else:\n","            model_parameters = torch.load(self.model_parameter_filepath, map_location=torch.device('cpu'))\n","        self.model.load_state_dict(model_parameters)\n","        if is_remote_execution:\n","            self.model.cuda()\n","        self.model.eval()\n","\n","    def evaluate_model(self):\n","        outputs = list()\n","        source_vocab_numpy = self.dataset_holder.get_source_vocab_numpy()\n","        target_vocab_numpy = self.dataset_holder.get_target_vocab_numpy()\n","        assert len(self.dataset_holder.get_source_encodings_test()) == len(self.dataset_holder.get_target_encodings_test())\n","        for i in range(0, len(self.dataset_holder.get_source_encodings_test())):\n","            if is_remote_execution:\n","                source_encoding = self.dataset_holder.get_source_encodings_test()[i].to(device=\"cuda\")\n","                target_encoding = self.dataset_holder.get_target_encodings_test()[i].to(device=\"cuda\")\n","            else:\n","                source_encoding = self.dataset_holder.get_source_encodings_test()[i].to(device=\"cpu\")\n","                target_encoding = self.dataset_holder.get_target_encodings_test()[i].to(device=\"cpu\")\n","            j = 0\n","            end_of_sequence = False\n","            while j < self.model_hyperparameters['max_tgt_seq_len'] and end_of_sequence == False:\n","                target_encoding_slice = torch.tensor_split(target_encoding, [j], dim=0)\n","                output_logits = self.model.forward(\n","                    source_encoding,\n","                    target_encoding_slice[0]\n","                ).detach().to(device=\"cpu\").flatten().numpy()\n","                output_logits_sort_pairs = list()\n","                for k in range(0, len(output_logits)):\n","                    output_logits_sort_pairs.append([output_logits[k], k])\n","                output_logits_sorted = sorted(output_logits_sort_pairs, key=lambda logit_pair: logit_pair[0], reverse=True)\n","                output_sort_index = 0\n","                # don't pick mark up characters or whitespace if first term\n","                if j == 0:\n","                    while output_logits_sorted[output_sort_index][1] in (0, 1, 2, 7):\n","                        output_sort_index = output_sort_index + 1\n","                else:\n","                    # don't repeat unknown, padding, or spaces\n","                    if (prediction_encoding[j-1] == 0\n","                            or prediction_encoding[j-1] == 1\n","                            or prediction_encoding[j-1] == 7):\n","                        while (output_logits_sorted[output_sort_index][1] == 0\n","                               or output_logits_sorted[output_sort_index][1] == 1\n","                               or output_logits_sorted[output_sort_index][1] == 7):\n","                            output_sort_index = output_sort_index + 1\n","                output_vocab_index = output_logits_sorted[output_sort_index][1]\n","                # index = output_vocab_index # always outputs \"Krrrr...<eos>\"\n","                index = min(random.randint(output_vocab_index, output_vocab_index + 3), len(output_logits_sorted))\n","                if j == 0:\n","                    prediction_encoding = torch.tensor([output_logits_sorted[index][1]], dtype=torch.float)\n","                else:\n","                    prediction_encoding = torch.cat([prediction_encoding, torch.tensor([output_logits_sorted[index][1]], dtype=torch.float)])\n","                if prediction_encoding[j] == self.dataset_holder.get_target_vocab().index(\n","                        self.dataset_holder.get_end_of_sequence_vocabulary_type()):\n","                    end_of_sequence = True\n","                j = j + 1\n","            decoded_source = np.take(source_vocab_numpy, source_encoding.detach().to(device=\"cpu\").flatten().numpy())\n","            decoded_target = np.take(target_vocab_numpy, target_encoding.detach().to(device=\"cpu\").flatten().numpy())\n","            decoded_translation = np.take(target_vocab_numpy, prediction_encoding.detach().to(device=\"cpu\", dtype=torch.int32).numpy())\n","            outputs.append({\n","                \"source\": \"\".join(decoded_source.tolist()),\n","                \"target\": \"\".join(decoded_target.tolist()),\n","                \"translation\": \"\".join(decoded_translation.tolist())\n","            })\n","            del source_encoding\n","            del target_encoding\n","            del prediction_encoding\n","            del target_encoding_slice\n","            del decoded_source\n","            del decoded_target\n","            del decoded_translation\n","            print(f\"Completed translation {i} of {len(self.dataset_holder.get_source_encodings_test())} at {time.time()}\")\n","            if is_remote_execution:\n","                torch.cuda.empty_cache()\n","                if i % 100 == 0:\n","                    print(f\"Memory usage summary:\")\n","                    print(f\"{torch.cuda.memory_summary()}\")\n","                    torch.cuda.reset_max_memory_allocated()\n","                    torch.cuda.reset_max_memory_cached()\n","                    torch.cuda.reset_peak_memory_stats()\n","            if i % 100 == 0 or i == len(self.dataset_holder.get_source_encodings_test()) - 1:\n","                print(f\"Translation{i}: {outputs[i]}\")\n","                output_file = open(self.output_filepath, \"w\")\n","                output_file.write(json.dumps(outputs, ensure_ascii=False, indent=1))\n","                output_file.close()\n","\n","\n","runner = Runner()\n","runner.load_dataset()\n","runner.load_model()\n","runner.evaluate_model()\n"]}]}