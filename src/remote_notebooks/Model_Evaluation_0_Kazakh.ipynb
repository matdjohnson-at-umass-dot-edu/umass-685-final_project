{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMPtyXGyiunAudIdFmMTO7l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XLxa0tEyK7r","outputId":"6a634167-8906-4ca0-b5ba-d9eb35601461","executionInfo":{"status":"ok","timestamp":1715991249150,"user_tz":240,"elapsed":1264027,"user":{"displayName":"Matthew Johnson","userId":"09672185346885346735"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Initialized runner NewsCommentaryByT5Vaswani2017Kocmi2018_0 with parameters {'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_0', 'model_parameter_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_0-1715965639-model.params', 'datasets_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808', 'output_filepath': '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_0.json', 'model_hyperparameters': {'src_vocab_size': 150, 'tgt_vocab_size': 81, 'max_src_seq_len': 266, 'max_tgt_seq_len': 256, 'd_model': 256, 'nhead': 8, 'num_encoder_layers': 9, 'num_decoder_layers': 3, 'dim_feedforward': 2048, 'dropout': 0.1, 'activation': <function relu at 0x7e6c1473fbe0>, 'custom_encoder': None, 'custom_decoder': None, 'layer_norm_eps': 1e-05, 'batch_first': True, 'norm_first': False, 'bias': True, 'device': None, 'dtype': None}}\n","Completed translation 0 of 427 at 1715990076.7333372\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  74243 KiB | 131498 KiB |  19195 MiB |  19122 MiB |\n","|       from large pool |  57472 KiB |  98304 KiB |    104 MiB |     48 MiB |\n","|       from small pool |  16771 KiB |  33194 KiB |  19091 MiB |  19074 MiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  74243 KiB | 131498 KiB |  19195 MiB |  19122 MiB |\n","|       from large pool |  57472 KiB |  98304 KiB |    104 MiB |     48 MiB |\n","|       from small pool |  16771 KiB |  33194 KiB |  19091 MiB |  19074 MiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  74174 KiB | 131429 KiB |  19190 MiB |  19117 MiB |\n","|       from large pool |  57472 KiB |  98304 KiB |    104 MiB |     48 MiB |\n","|       from small pool |  16702 KiB |  33125 KiB |  19086 MiB |  19069 MiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   | 106496 KiB | 137216 KiB | 137216 KiB |  30720 KiB |\n","|       from large pool |  81920 KiB | 102400 KiB | 102400 KiB |  20480 KiB |\n","|       from small pool |  24576 KiB |  34816 KiB |  34816 KiB |  10240 KiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  32253 KiB |  34299 KiB |  20401 MiB |  20370 MiB |\n","|       from large pool |  24448 KiB |  24448 KiB |    145 MiB |    122 MiB |\n","|       from small pool |   7805 KiB |   9851 KiB |  20255 MiB |  20248 MiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     622    |   76295    |   75855    |\n","|       from large pool |      25    |      48    |      49    |      24    |\n","|       from small pool |     415    |     597    |   76246    |   75831    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     622    |   76295    |   75855    |\n","|       from large pool |      25    |      48    |      49    |      24    |\n","|       from small pool |     415    |     597    |   76246    |   75831    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      22    |      22    |       6    |\n","|       from large pool |       4    |       5    |       5    |       1    |\n","|       from small pool |      12    |      17    |      17    |       5    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      11    |      21    |   33534    |   33523    |\n","|       from large pool |       3    |       3    |       9    |       6    |\n","|       from small pool |       8    |      18    |   33525    |   33517    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation0: {'source': 'Қазір жұрт технологияны да солай игеріп жатыр.Ģ', 'target': 'People engage with technology in similar ways.Ģ', 'translation': 'vrlhrlrslrllsllrrlrlslrhllshlllhhhshrslrshhsrlhrhhslrrshlhhhlllhsshrrshrrrsrlllhrrlrrhrrhrlrlslhhhsrsrshhhlllrlrlsrrrhlsllhrllsrssshhsrrrslhhhlrrlhrrlsrsrhrhrhslshlhlsshsrlsrsslsrhrrshrhhrrrshsslllrhshrrhssllhrlrlsshhlllslhhlhssshrshssllrlllllhshshhrsrrssr'}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Completed translation 1 of 427 at 1715990081.0866542\n","Completed translation 2 of 427 at 1715990083.7610316\n","Completed translation 3 of 427 at 1715990086.4917061\n","Completed translation 4 of 427 at 1715990089.2474828\n","Completed translation 5 of 427 at 1715990091.9292874\n","Completed translation 6 of 427 at 1715990094.5799549\n","Completed translation 7 of 427 at 1715990097.243859\n","Completed translation 8 of 427 at 1715990099.976619\n","Completed translation 9 of 427 at 1715990102.7165263\n","Completed translation 10 of 427 at 1715990105.4125345\n","Completed translation 11 of 427 at 1715990108.082444\n","Completed translation 12 of 427 at 1715990110.7738433\n","Completed translation 13 of 427 at 1715990113.4272366\n","Completed translation 14 of 427 at 1715990116.1241984\n","Completed translation 15 of 427 at 1715990118.8008568\n","Completed translation 16 of 427 at 1715990121.5337636\n","Completed translation 17 of 427 at 1715990124.280734\n","Completed translation 18 of 427 at 1715990126.9947078\n","Completed translation 19 of 427 at 1715990129.6941545\n","Completed translation 20 of 427 at 1715990132.382292\n","Completed translation 21 of 427 at 1715990135.1886685\n","Completed translation 22 of 427 at 1715990137.8823664\n","Completed translation 23 of 427 at 1715990140.573302\n","Completed translation 24 of 427 at 1715990143.370227\n","Completed translation 25 of 427 at 1715990146.0678012\n","Completed translation 26 of 427 at 1715990148.799847\n","Completed translation 27 of 427 at 1715990151.5464864\n","Completed translation 28 of 427 at 1715990154.2039797\n","Completed translation 29 of 427 at 1715990156.8572278\n","Completed translation 30 of 427 at 1715990159.5622299\n","Completed translation 31 of 427 at 1715990162.2124677\n","Completed translation 32 of 427 at 1715990164.9811783\n","Completed translation 33 of 427 at 1715990167.6527514\n","Completed translation 34 of 427 at 1715990170.3257954\n","Completed translation 35 of 427 at 1715990172.9956374\n","Completed translation 36 of 427 at 1715990175.715713\n","Completed translation 37 of 427 at 1715990178.4518805\n","Completed translation 38 of 427 at 1715990181.1480465\n","Completed translation 39 of 427 at 1715990183.9058142\n","Completed translation 40 of 427 at 1715990186.5568826\n","Completed translation 41 of 427 at 1715990189.259642\n","Completed translation 42 of 427 at 1715990191.9379234\n","Completed translation 43 of 427 at 1715990194.6296473\n","Completed translation 44 of 427 at 1715990197.4617019\n","Completed translation 45 of 427 at 1715990200.2395017\n","Completed translation 46 of 427 at 1715990202.9341676\n","Completed translation 47 of 427 at 1715990205.7137163\n","Completed translation 48 of 427 at 1715990208.490054\n","Completed translation 49 of 427 at 1715990211.192906\n","Completed translation 50 of 427 at 1715990213.858557\n","Completed translation 51 of 427 at 1715990216.5525098\n","Completed translation 52 of 427 at 1715990219.3286173\n","Completed translation 53 of 427 at 1715990222.0677497\n","Completed translation 54 of 427 at 1715990224.76742\n","Completed translation 55 of 427 at 1715990227.5858798\n","Completed translation 56 of 427 at 1715990230.2792044\n","Completed translation 57 of 427 at 1715990232.9810302\n","Completed translation 58 of 427 at 1715990235.7421281\n","Completed translation 59 of 427 at 1715990238.4246066\n","Completed translation 60 of 427 at 1715990241.188854\n","Completed translation 61 of 427 at 1715990243.9105062\n","Completed translation 62 of 427 at 1715990246.6613817\n","Completed translation 63 of 427 at 1715990249.4001431\n","Completed translation 64 of 427 at 1715990252.1251411\n","Completed translation 65 of 427 at 1715990254.8373287\n","Completed translation 66 of 427 at 1715990257.6041367\n","Completed translation 67 of 427 at 1715990260.4814057\n","Completed translation 68 of 427 at 1715990263.2631605\n","Completed translation 69 of 427 at 1715990265.9686108\n","Completed translation 70 of 427 at 1715990268.8530354\n","Completed translation 71 of 427 at 1715990271.5998578\n","Completed translation 72 of 427 at 1715990274.2760136\n","Completed translation 73 of 427 at 1715990276.9865606\n","Completed translation 74 of 427 at 1715990279.6852393\n","Completed translation 75 of 427 at 1715990282.441008\n","Completed translation 76 of 427 at 1715990285.1527445\n","Completed translation 77 of 427 at 1715990287.887416\n","Completed translation 78 of 427 at 1715990290.558988\n","Completed translation 79 of 427 at 1715990293.2977116\n","Completed translation 80 of 427 at 1715990296.0464332\n","Completed translation 81 of 427 at 1715990298.7817283\n","Completed translation 82 of 427 at 1715990301.5819778\n","Completed translation 83 of 427 at 1715990304.3141418\n","Completed translation 84 of 427 at 1715990307.0421226\n","Completed translation 85 of 427 at 1715990309.758412\n","Completed translation 86 of 427 at 1715990312.4520016\n","Completed translation 87 of 427 at 1715990315.1740215\n","Completed translation 88 of 427 at 1715990317.9116695\n","Completed translation 89 of 427 at 1715990320.6365001\n","Completed translation 90 of 427 at 1715990323.4038005\n","Completed translation 91 of 427 at 1715990326.1775417\n","Completed translation 92 of 427 at 1715990328.9393167\n","Completed translation 93 of 427 at 1715990331.6550167\n","Completed translation 94 of 427 at 1715990334.3527505\n","Completed translation 95 of 427 at 1715990337.0813591\n","Completed translation 96 of 427 at 1715990339.9418263\n","Completed translation 97 of 427 at 1715990342.70785\n","Completed translation 98 of 427 at 1715990345.3951943\n","Completed translation 99 of 427 at 1715990348.1677322\n","Completed translation 100 of 427 at 1715990350.841663\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  74243 KiB |  95258 KiB |   2094 GiB |   2094 GiB |\n","|       from large pool |  57472 KiB |  57472 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16771 KiB |  37786 KiB |   2094 GiB |   2094 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  74243 KiB |  95258 KiB |   2094 GiB |   2094 GiB |\n","|       from large pool |  57472 KiB |  57472 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16771 KiB |  37786 KiB |   2094 GiB |   2094 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  74174 KiB |  95181 KiB |   2093 GiB |   2093 GiB |\n","|       from large pool |  57472 KiB |  57472 KiB |      0 GiB |      0 GiB |\n","|       from small pool |  16702 KiB |  37709 KiB |   2093 GiB |   2093 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   | 106496 KiB | 122880 KiB |   1174 MiB |   1070 MiB |\n","|       from large pool |  81920 KiB |  81920 KiB |    100 MiB |     20 MiB |\n","|       from small pool |  24576 KiB |  40960 KiB |   1074 MiB |   1050 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  32253 KiB |  41390 KiB |   2213 GiB |   2213 GiB |\n","|       from large pool |  24448 KiB |  24448 KiB |      0 GiB |      0 GiB |\n","|       from small pool |   7805 KiB |  16942 KiB |   2213 GiB |   2213 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     622    |    7618 K  |    7618 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     597    |    7618 K  |    7618 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     622    |    7618 K  |    7618 K  |\n","|       from large pool |      25    |      25    |       0 K  |       0 K  |\n","|       from small pool |     415    |     597    |    7618 K  |    7618 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      24    |     542    |     526    |\n","|       from large pool |       4    |       4    |       5    |       1    |\n","|       from small pool |      12    |      20    |     537    |     525    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      11    |      31    |    3225 K  |    3225 K  |\n","|       from large pool |       3    |       3    |       0 K  |       0 K  |\n","|       from small pool |       8    |      28    |    3225 K  |    3225 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation100: {'source': 'Компания 20 жылдық қызмет мерзімі ішінде Айлзға қатысты осындай сценарийді ұстанды.Ģ', 'target': 'The company followed essentially the same script with regard to Ailes during his 20-year tenure.Ģ', 'translation': 'shrhhlrlslhsllrlsrhllhrsssshhhlrlhlhrshsrsshrsrslshrrlsslsrhrlllhshrlhssrlshlrshrrrlhhllrhsllrlslllllsrhssshhrshrlsshllhhhsssrlhllhsslhslsrhllrlllhsshhsrhlrhhrsslhrllhrsrhrlhhrhhslhhlhshhslsrhrhrrhrhsrlhssllsshlhrhhrrrssssrrlssrsrrsshsrlrsshrrlsshrlsshsrsr'}\n","Completed translation 101 of 427 at 1715990353.5383885\n","Completed translation 102 of 427 at 1715990356.2380493\n","Completed translation 103 of 427 at 1715990358.9588988\n","Completed translation 104 of 427 at 1715990361.7133737\n","Completed translation 105 of 427 at 1715990364.5043683\n","Completed translation 106 of 427 at 1715990367.3009813\n","Completed translation 107 of 427 at 1715990370.0607662\n","Completed translation 108 of 427 at 1715990372.7760003\n","Completed translation 109 of 427 at 1715990375.6443179\n","Completed translation 110 of 427 at 1715990378.3862927\n","Completed translation 111 of 427 at 1715990381.0950332\n","Completed translation 112 of 427 at 1715990383.8239944\n","Completed translation 113 of 427 at 1715990386.5647664\n","Completed translation 114 of 427 at 1715990389.27172\n","Completed translation 115 of 427 at 1715990391.9818728\n","Completed translation 116 of 427 at 1715990394.6561244\n","Completed translation 117 of 427 at 1715990397.3397954\n","Completed translation 118 of 427 at 1715990400.032378\n","Completed translation 119 of 427 at 1715990402.719448\n","Completed translation 120 of 427 at 1715990405.4142253\n","Completed translation 121 of 427 at 1715990408.1127748\n","Completed translation 122 of 427 at 1715990410.838997\n","Completed translation 123 of 427 at 1715990413.5044708\n","Completed translation 124 of 427 at 1715990416.2541733\n","Completed translation 125 of 427 at 1715990418.9708388\n","Completed translation 126 of 427 at 1715990421.720274\n","Completed translation 127 of 427 at 1715990424.4470513\n","Completed translation 128 of 427 at 1715990427.3072977\n","Completed translation 129 of 427 at 1715990430.0731049\n","Completed translation 130 of 427 at 1715990432.8375638\n","Completed translation 131 of 427 at 1715990435.6406724\n","Completed translation 132 of 427 at 1715990438.4458613\n","Completed translation 133 of 427 at 1715990441.2502317\n","Completed translation 134 of 427 at 1715990443.9705708\n","Completed translation 135 of 427 at 1715990446.693026\n","Completed translation 136 of 427 at 1715990449.5085757\n","Completed translation 137 of 427 at 1715990452.250036\n","Completed translation 138 of 427 at 1715990454.9906232\n","Completed translation 139 of 427 at 1715990457.7145507\n","Completed translation 140 of 427 at 1715990460.4528563\n","Completed translation 141 of 427 at 1715990463.2547846\n","Completed translation 142 of 427 at 1715990466.0252848\n","Completed translation 143 of 427 at 1715990468.7395666\n","Completed translation 144 of 427 at 1715990471.4427235\n","Completed translation 145 of 427 at 1715990474.145107\n","Completed translation 146 of 427 at 1715990476.917475\n","Completed translation 147 of 427 at 1715990479.7161849\n","Completed translation 148 of 427 at 1715990482.4375715\n","Completed translation 149 of 427 at 1715990485.157826\n","Completed translation 150 of 427 at 1715990487.8736308\n","Completed translation 151 of 427 at 1715990490.6122494\n","Completed translation 152 of 427 at 1715990493.3287964\n","Completed translation 153 of 427 at 1715990496.0399716\n","Completed translation 154 of 427 at 1715990498.7238333\n","Completed translation 155 of 427 at 1715990501.4616945\n","Completed translation 156 of 427 at 1715990504.1538188\n","Completed translation 157 of 427 at 1715990506.872278\n","Completed translation 158 of 427 at 1715990509.5725412\n","Completed translation 159 of 427 at 1715990512.2674406\n","Completed translation 160 of 427 at 1715990515.0217848\n","Completed translation 161 of 427 at 1715990517.7175148\n","Completed translation 162 of 427 at 1715990520.4640422\n","Completed translation 163 of 427 at 1715990523.1985133\n","Completed translation 164 of 427 at 1715990525.909106\n","Completed translation 165 of 427 at 1715990528.6078796\n","Completed translation 166 of 427 at 1715990531.3625803\n","Completed translation 167 of 427 at 1715990534.1052623\n","Completed translation 168 of 427 at 1715990536.820787\n","Completed translation 169 of 427 at 1715990539.5585105\n","Completed translation 170 of 427 at 1715990542.2871604\n","Completed translation 171 of 427 at 1715990545.0823696\n","Completed translation 172 of 427 at 1715990547.889781\n","Completed translation 173 of 427 at 1715990550.597999\n","Completed translation 174 of 427 at 1715990553.3540852\n","Completed translation 175 of 427 at 1715990556.0508003\n","Completed translation 176 of 427 at 1715990558.7525244\n","Completed translation 177 of 427 at 1715990561.4752514\n","Completed translation 178 of 427 at 1715990564.1748965\n","Completed translation 179 of 427 at 1715990566.8743606\n","Completed translation 180 of 427 at 1715990569.6813414\n","Completed translation 181 of 427 at 1715990572.446044\n","Completed translation 182 of 427 at 1715990575.174635\n","Completed translation 183 of 427 at 1715990577.8731492\n","Completed translation 184 of 427 at 1715990580.6613624\n","Completed translation 185 of 427 at 1715990583.3559859\n","Completed translation 186 of 427 at 1715990586.0886981\n","Completed translation 187 of 427 at 1715990588.8263125\n","Completed translation 188 of 427 at 1715990591.5826685\n","Completed translation 189 of 427 at 1715990594.3469043\n","Completed translation 190 of 427 at 1715990597.0850463\n","Completed translation 191 of 427 at 1715990599.7725196\n","Completed translation 192 of 427 at 1715990602.4794428\n","Completed translation 193 of 427 at 1715990605.2646747\n","Completed translation 194 of 427 at 1715990607.957073\n","Completed translation 195 of 427 at 1715990610.714785\n","Completed translation 196 of 427 at 1715990613.4452915\n","Completed translation 197 of 427 at 1715990616.228331\n","Completed translation 198 of 427 at 1715990618.9439764\n","Completed translation 199 of 427 at 1715990621.8022974\n","Completed translation 200 of 427 at 1715990624.888244\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  74243 KiB | 104052 KiB |   4476 GiB |   4476 GiB |\n","|       from large pool |  57472 KiB |  69760 KiB |     40 GiB |     40 GiB |\n","|       from small pool |  16771 KiB |  44240 KiB |   4436 GiB |   4436 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  74243 KiB | 104052 KiB |   4476 GiB |   4476 GiB |\n","|       from large pool |  57472 KiB |  69760 KiB |     40 GiB |     40 GiB |\n","|       from small pool |  16771 KiB |  44240 KiB |   4436 GiB |   4436 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  74174 KiB | 102476 KiB |   4472 GiB |   4472 GiB |\n","|       from large pool |  57472 KiB |  68512 KiB |     36 GiB |     36 GiB |\n","|       from small pool |  16702 KiB |  44170 KiB |   4435 GiB |   4435 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   | 106496 KiB | 129024 KiB |   2940 MiB |   2836 MiB |\n","|       from large pool |  81920 KiB |  81920 KiB |    100 MiB |     20 MiB |\n","|       from small pool |  24576 KiB |  47104 KiB |   2840 MiB |   2816 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  32253 KiB |  42432 KiB |   4723 GiB |   4723 GiB |\n","|       from large pool |  24448 KiB |  24448 KiB |     40 GiB |     40 GiB |\n","|       from small pool |   7805 KiB |  17984 KiB |   4682 GiB |   4682 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     622    |   15161 K  |   15161 K  |\n","|       from large pool |      25    |      35    |      36 K  |      36 K  |\n","|       from small pool |     415    |     597    |   15125 K  |   15124 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     622    |   15161 K  |   15161 K  |\n","|       from large pool |      25    |      35    |      36 K  |      36 K  |\n","|       from small pool |     415    |     597    |   15125 K  |   15124 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      27    |    1425    |    1409    |\n","|       from large pool |       4    |       4    |       5    |       1    |\n","|       from small pool |      12    |      23    |    1420    |    1408    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      11    |      26    |    6388 K  |    6388 K  |\n","|       from large pool |       3    |       4    |      22 K  |      22 K  |\n","|       from small pool |       8    |      23    |    6366 K  |    6365 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation200: {'source': '1981 жылы билікке модернист Махатхир келген соң Ислам – PAS-тың UMNO-ға қарсы ең тиімді идеологиялық қаруына айналды.Ģ', 'target': \"When the modernist Mahathir came to power in 1981, Islamism became the PAS's most effective ideological weapon against the UMNO.Ģ\", 'translation': 'ohllsshhrsrlsrhhlhhsrllsrsrhslrlslrssrsshslsshsshsrlllhhlrrslllllhshlrhlhrslhlsssrslhhlrhrhslrrsrhlsssrrslrlrlllhlhhshsllhrhslshhhhsrlllslshsrrslsrhhhrrsslrshsshlshlllrhhlllrshsrrssshlshhhslhsrrlrllhsrhsslslshrlrrlllslsrrlshhslsslhslrhhshhssrrhslrlrhrsslsl'}\n","Completed translation 201 of 427 at 1715990627.6644468\n","Completed translation 202 of 427 at 1715990630.3592336\n","Completed translation 203 of 427 at 1715990633.0791407\n","Completed translation 204 of 427 at 1715990635.7792265\n","Completed translation 205 of 427 at 1715990638.4881008\n","Completed translation 206 of 427 at 1715990641.2347376\n","Completed translation 207 of 427 at 1715990643.9534051\n","Completed translation 208 of 427 at 1715990646.6872447\n","Completed translation 209 of 427 at 1715990649.4251704\n","Completed translation 210 of 427 at 1715990652.143185\n","Completed translation 211 of 427 at 1715990654.8570497\n","Completed translation 212 of 427 at 1715990657.5926867\n","Completed translation 213 of 427 at 1715990660.5433302\n","Completed translation 214 of 427 at 1715990663.2731078\n","Completed translation 215 of 427 at 1715990666.0217807\n","Completed translation 216 of 427 at 1715990668.777662\n","Completed translation 217 of 427 at 1715990671.588644\n","Completed translation 218 of 427 at 1715990674.300992\n","Completed translation 219 of 427 at 1715990677.1626604\n","Completed translation 220 of 427 at 1715990679.913868\n","Completed translation 221 of 427 at 1715990682.6223943\n","Completed translation 222 of 427 at 1715990685.3786225\n","Completed translation 223 of 427 at 1715990688.2243047\n","Completed translation 224 of 427 at 1715990690.990317\n","Completed translation 225 of 427 at 1715990693.6977522\n","Completed translation 226 of 427 at 1715990696.4617743\n","Completed translation 227 of 427 at 1715990699.1456645\n","Completed translation 228 of 427 at 1715990701.9019537\n","Completed translation 229 of 427 at 1715990704.6183805\n","Completed translation 230 of 427 at 1715990707.4015894\n","Completed translation 231 of 427 at 1715990710.0939162\n","Completed translation 232 of 427 at 1715990712.9671378\n","Completed translation 233 of 427 at 1715990715.6816528\n","Completed translation 234 of 427 at 1715990718.4222536\n","Completed translation 235 of 427 at 1715990721.19773\n","Completed translation 236 of 427 at 1715990723.9227133\n","Completed translation 237 of 427 at 1715990726.8200788\n","Completed translation 238 of 427 at 1715990729.5515637\n","Completed translation 239 of 427 at 1715990732.4518924\n","Completed translation 240 of 427 at 1715990735.3465364\n","Completed translation 241 of 427 at 1715990738.061141\n","Completed translation 242 of 427 at 1715990740.756154\n","Completed translation 243 of 427 at 1715990743.5563169\n","Completed translation 244 of 427 at 1715990746.2880256\n","Completed translation 245 of 427 at 1715990749.0699954\n","Completed translation 246 of 427 at 1715990751.8549082\n","Completed translation 247 of 427 at 1715990754.6394129\n","Completed translation 248 of 427 at 1715990757.4509704\n","Completed translation 249 of 427 at 1715990760.1640832\n","Completed translation 250 of 427 at 1715990762.8633347\n","Completed translation 251 of 427 at 1715990765.5863454\n","Completed translation 252 of 427 at 1715990768.4947429\n","Completed translation 253 of 427 at 1715990771.1834552\n","Completed translation 254 of 427 at 1715990773.9083266\n","Completed translation 255 of 427 at 1715990776.6387017\n","Completed translation 256 of 427 at 1715990779.3717878\n","Completed translation 257 of 427 at 1715990782.202445\n","Completed translation 258 of 427 at 1715990784.9181728\n","Completed translation 259 of 427 at 1715990787.618248\n","Completed translation 260 of 427 at 1715990790.3271196\n","Completed translation 261 of 427 at 1715990793.0226612\n","Completed translation 262 of 427 at 1715990795.844711\n","Completed translation 263 of 427 at 1715990798.5521038\n","Completed translation 264 of 427 at 1715990801.2603235\n","Completed translation 265 of 427 at 1715990803.9983351\n","Completed translation 266 of 427 at 1715990806.72888\n","Completed translation 267 of 427 at 1715990809.4147239\n","Completed translation 268 of 427 at 1715990812.2878435\n","Completed translation 269 of 427 at 1715990815.0121984\n","Completed translation 270 of 427 at 1715990817.7282462\n","Completed translation 271 of 427 at 1715990820.442662\n","Completed translation 272 of 427 at 1715990823.1209865\n","Completed translation 273 of 427 at 1715990825.8332474\n","Completed translation 274 of 427 at 1715990828.6634066\n","Completed translation 275 of 427 at 1715990831.5934129\n","Completed translation 276 of 427 at 1715990834.307177\n","Completed translation 277 of 427 at 1715990837.0309794\n","Completed translation 278 of 427 at 1715990839.70962\n","Completed translation 279 of 427 at 1715990842.4322486\n","Completed translation 280 of 427 at 1715990845.6507752\n","Completed translation 281 of 427 at 1715990848.36014\n","Completed translation 282 of 427 at 1715990851.1684022\n","Completed translation 283 of 427 at 1715990854.0372667\n","Completed translation 284 of 427 at 1715990856.818585\n","Completed translation 285 of 427 at 1715990859.5593\n","Completed translation 286 of 427 at 1715990862.5422246\n","Completed translation 287 of 427 at 1715990865.3800044\n","Completed translation 288 of 427 at 1715990868.193784\n","Completed translation 289 of 427 at 1715990870.9108746\n","Completed translation 290 of 427 at 1715990873.5898385\n","Completed translation 291 of 427 at 1715990876.4187393\n","Completed translation 292 of 427 at 1715990879.2112448\n","Completed translation 293 of 427 at 1715990881.9429471\n","Completed translation 294 of 427 at 1715990884.6746178\n","Completed translation 295 of 427 at 1715990887.518993\n","Completed translation 296 of 427 at 1715990890.314214\n","Completed translation 297 of 427 at 1715990893.0381124\n","Completed translation 298 of 427 at 1715990895.767251\n","Completed translation 299 of 427 at 1715990898.6500459\n","Completed translation 300 of 427 at 1715990901.5533273\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  74243 KiB | 110881 KiB |   7159 GiB |   7159 GiB |\n","|       from large pool |  57472 KiB |  75200 KiB |    555 GiB |    555 GiB |\n","|       from small pool |  16771 KiB |  45020 KiB |   6603 GiB |   6603 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  74243 KiB | 110881 KiB |   7159 GiB |   7159 GiB |\n","|       from large pool |  57472 KiB |  75200 KiB |    555 GiB |    555 GiB |\n","|       from small pool |  16771 KiB |  45020 KiB |   6603 GiB |   6603 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  74174 KiB | 110288 KiB |   7130 GiB |   7130 GiB |\n","|       from large pool |  57472 KiB |  74560 KiB |    527 GiB |    527 GiB |\n","|       from small pool |  16702 KiB |  44951 KiB |   6602 GiB |   6602 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   | 106496 KiB | 129024 KiB |   4538 MiB |   4434 MiB |\n","|       from large pool |  81920 KiB |  81920 KiB |    100 MiB |     20 MiB |\n","|       from small pool |  24576 KiB |  47104 KiB |   4438 MiB |   4414 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  32253 KiB |  43796 KiB |   7565 GiB |   7565 GiB |\n","|       from large pool |  24448 KiB |  24448 KiB |    555 GiB |    555 GiB |\n","|       from small pool |   7805 KiB |  19348 KiB |   7010 GiB |   7010 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     622    |   22704 K  |   22703 K  |\n","|       from large pool |      25    |      38    |     474 K  |     474 K  |\n","|       from small pool |     415    |     597    |   22229 K  |   22228 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     622    |   22704 K  |   22703 K  |\n","|       from large pool |      25    |      38    |     474 K  |     474 K  |\n","|       from small pool |     415    |     597    |   22229 K  |   22228 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      27    |    2224    |    2208    |\n","|       from large pool |       4    |       4    |       5    |       1    |\n","|       from small pool |      12    |      23    |    2219    |    2207    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      11    |      27    |    9698 K  |    9698 K  |\n","|       from large pool |       3    |       4    |     284 K  |     284 K  |\n","|       from small pool |       8    |      24    |    9414 K  |    9414 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation300: {'source': 'ЛОНДОН – Жүйе құрушы орталық банктердің қорғанысты өзгерте бастауы 2018-2019 жылдары жаңа ақша-несие саясаты дәуірінің басталатынынан хабар берсе керек.Ģ', 'target': 'LONDON - The changing of the guard that is taking place at the systemically important central banks in 2018-2019 will mark the beginning of a new era of monetary policy.Ģ', 'translation': 'KrrshshllshshhrrlhhsrsllrhrlhrlhhhlhrslrssslssllrrlllslshshshhsrhsshsrsrrrhlrhrsrsrhrslsrhssrhllsrsrhrrllrhhrhlhllhrrlhsshshrlsshrrslshhrhslrlhrrlslshshrrFZ$ZZZFF$$FZ%%Z$%Z$ZZ%$$$F$$Z$$$%$F%Z%$ZF$%ZF$F$%Z$Z%$F%Z%FF%%$FZ%Z$Z%%$$%F$FF$$Z$%%%%%F$%$FZ$$%F%$$FZ'}\n","Completed translation 301 of 427 at 1715990904.5613503\n","Completed translation 302 of 427 at 1715990907.4161313\n","Completed translation 303 of 427 at 1715990910.104206\n","Completed translation 304 of 427 at 1715990912.8387592\n","Completed translation 305 of 427 at 1715990915.6530259\n","Completed translation 306 of 427 at 1715990918.6521163\n","Completed translation 307 of 427 at 1715990921.3598776\n","Completed translation 308 of 427 at 1715990924.0833664\n","Completed translation 309 of 427 at 1715990926.8036494\n","Completed translation 310 of 427 at 1715990929.7117584\n","Completed translation 311 of 427 at 1715990932.6476111\n","Completed translation 312 of 427 at 1715990935.3792653\n","Completed translation 313 of 427 at 1715990938.0779448\n","Completed translation 314 of 427 at 1715990940.7808778\n","Completed translation 315 of 427 at 1715990943.508007\n","Completed translation 316 of 427 at 1715990946.1973574\n","Completed translation 317 of 427 at 1715990949.032517\n","Completed translation 318 of 427 at 1715990951.724281\n","Completed translation 319 of 427 at 1715990954.4520233\n","Completed translation 320 of 427 at 1715990957.2974927\n","Completed translation 321 of 427 at 1715990959.9928865\n","Completed translation 322 of 427 at 1715990962.7555401\n","Completed translation 323 of 427 at 1715990965.4708586\n","Completed translation 324 of 427 at 1715990968.5028822\n","Completed translation 325 of 427 at 1715990971.2799857\n","Completed translation 326 of 427 at 1715990974.0311453\n","Completed translation 327 of 427 at 1715990976.7154021\n","Completed translation 328 of 427 at 1715990979.4154394\n","Completed translation 329 of 427 at 1715990982.109278\n","Completed translation 330 of 427 at 1715990984.8113916\n","Completed translation 331 of 427 at 1715990987.5932572\n","Completed translation 332 of 427 at 1715990990.2757957\n","Completed translation 333 of 427 at 1715990993.103547\n","Completed translation 334 of 427 at 1715990995.7889216\n","Completed translation 335 of 427 at 1715990998.6565275\n","Completed translation 336 of 427 at 1715991001.3919055\n","Completed translation 337 of 427 at 1715991004.1275558\n","Completed translation 338 of 427 at 1715991006.7951632\n","Completed translation 339 of 427 at 1715991009.6358078\n","Completed translation 340 of 427 at 1715991012.3329117\n","Completed translation 341 of 427 at 1715991015.1280181\n","Completed translation 342 of 427 at 1715991017.815459\n","Completed translation 343 of 427 at 1715991020.5008867\n","Completed translation 344 of 427 at 1715991023.2753499\n","Completed translation 345 of 427 at 1715991026.1140916\n","Completed translation 346 of 427 at 1715991028.8074672\n","Completed translation 347 of 427 at 1715991031.4865172\n","Completed translation 348 of 427 at 1715991034.2264526\n","Completed translation 349 of 427 at 1715991036.9792745\n","Completed translation 350 of 427 at 1715991039.6602988\n","Completed translation 351 of 427 at 1715991042.3862112\n","Completed translation 352 of 427 at 1715991045.0562675\n","Completed translation 353 of 427 at 1715991047.8322299\n","Completed translation 354 of 427 at 1715991050.728095\n","Completed translation 355 of 427 at 1715991053.4203799\n","Completed translation 356 of 427 at 1715991056.0810325\n","Completed translation 357 of 427 at 1715991058.7911725\n","Completed translation 358 of 427 at 1715991061.50522\n","Completed translation 359 of 427 at 1715991064.3527062\n","Completed translation 360 of 427 at 1715991067.035403\n","Completed translation 361 of 427 at 1715991069.702958\n","Completed translation 362 of 427 at 1715991072.411116\n","Completed translation 363 of 427 at 1715991075.2480712\n","Completed translation 364 of 427 at 1715991077.906012\n","Completed translation 365 of 427 at 1715991080.7485306\n","Completed translation 366 of 427 at 1715991083.7086055\n","Completed translation 367 of 427 at 1715991086.4849129\n","Completed translation 368 of 427 at 1715991089.2178853\n","Completed translation 369 of 427 at 1715991091.9975176\n","Completed translation 370 of 427 at 1715991094.7483575\n","Completed translation 371 of 427 at 1715991097.509967\n","Completed translation 372 of 427 at 1715991100.2242718\n","Completed translation 373 of 427 at 1715991102.9275339\n","Completed translation 374 of 427 at 1715991105.6876469\n","Completed translation 375 of 427 at 1715991108.3966794\n","Completed translation 376 of 427 at 1715991111.0967476\n","Completed translation 377 of 427 at 1715991113.7796342\n","Completed translation 378 of 427 at 1715991116.5848942\n","Completed translation 379 of 427 at 1715991119.3507402\n","Completed translation 380 of 427 at 1715991122.0522635\n","Completed translation 381 of 427 at 1715991124.8700185\n","Completed translation 382 of 427 at 1715991127.5629132\n","Completed translation 383 of 427 at 1715991130.2379615\n","Completed translation 384 of 427 at 1715991132.9485745\n","Completed translation 385 of 427 at 1715991135.6366367\n","Completed translation 386 of 427 at 1715991138.4356134\n","Completed translation 387 of 427 at 1715991141.1738155\n","Completed translation 388 of 427 at 1715991143.901974\n","Completed translation 389 of 427 at 1715991146.6794806\n","Completed translation 390 of 427 at 1715991149.3803651\n","Completed translation 391 of 427 at 1715991152.1144943\n","Completed translation 392 of 427 at 1715991154.7749648\n","Completed translation 393 of 427 at 1715991157.6407025\n","Completed translation 394 of 427 at 1715991160.3822105\n","Completed translation 395 of 427 at 1715991163.0723531\n","Completed translation 396 of 427 at 1715991165.7478607\n","Completed translation 397 of 427 at 1715991168.5025933\n","Completed translation 398 of 427 at 1715991171.2520294\n","Completed translation 399 of 427 at 1715991173.9664693\n","Completed translation 400 of 427 at 1715991176.690519\n","Memory usage summary:\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  74243 KiB | 124894 KiB |  10198 GiB |  10198 GiB |\n","|       from large pool |  57472 KiB |  82136 KiB |   1344 GiB |   1343 GiB |\n","|       from small pool |  16771 KiB |  44261 KiB |   8854 GiB |   8854 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  74243 KiB | 124894 KiB |  10198 GiB |  10198 GiB |\n","|       from large pool |  57472 KiB |  82136 KiB |   1344 GiB |   1343 GiB |\n","|       from small pool |  16771 KiB |  44261 KiB |   8854 GiB |   8854 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  74174 KiB | 123490 KiB |  10144 GiB |  10144 GiB |\n","|       from large pool |  57472 KiB |  80808 KiB |   1291 GiB |   1291 GiB |\n","|       from small pool |  16702 KiB |  44188 KiB |   8853 GiB |   8853 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   | 106496 KiB | 149504 KiB |   6532 MiB |   6428 MiB |\n","|       from large pool |  81920 KiB | 102400 KiB |    260 MiB |    180 MiB |\n","|       from small pool |  24576 KiB |  47104 KiB |   6272 MiB |   6248 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  32253 KiB |  42837 KiB |  10825 GiB |  10825 GiB |\n","|       from large pool |  24448 KiB |  24992 KiB |   1349 GiB |   1349 GiB |\n","|       from small pool |   7805 KiB |  18389 KiB |   9475 GiB |   9475 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     440    |     622    |   30246 K  |   30246 K  |\n","|       from large pool |      25    |      38    |    1011 K  |    1011 K  |\n","|       from small pool |     415    |     588    |   29234 K  |   29234 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     440    |     622    |   30246 K  |   30246 K  |\n","|       from large pool |      25    |      38    |    1011 K  |    1011 K  |\n","|       from small pool |     415    |     588    |   29234 K  |   29234 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      28    |    3149    |    3133    |\n","|       from large pool |       4    |       5    |      13    |       9    |\n","|       from small pool |      12    |      23    |    3136    |    3124    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      11    |      29    |   13139 K  |   13139 K  |\n","|       from large pool |       3    |       5    |     578 K  |     578 K  |\n","|       from small pool |       8    |      25    |   12560 K  |   12560 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","Translation400: {'source': 'Сақтандырылмаған банк депозиттеріне байланысты тәуекелге бармай-ақ қолма-қол ақшаға оңай қол жеткізгісі келетін ұйымдарға Қазынашылықтың қысқа мерзімді вексельдері жетпегендіктен, бір түндік ипотекалық пул нарығы өркен жайды.Ģ', 'target': \"The overnight mortgage-pool market exists partly because there aren't enough short-term US Treasury bills available for businesses that want easy access to cash without the risk implied by uninsured bank deposits.Ģ\", 'translation': 'vhrrrhhsllsrllsrsrrhlhlssrrlssrrrlllhhlsrhslsrllsllrlrhlssrlslhlhlsshrlsrrrssrhsshrrhshllhrhrlsrrhrhhrsllrlrshhshhlshhlslhslrhllhhsrssrslhrhrhlsllrrhsllll%F$ZZZZF$F%ZZZ$ZFFZZFFZZZZ%$$$ZZ%%F%%$%$$$$ZZ%FF%Z%Z%%FZZ$F$Z$F%Z$$Z%%$$F$FZZZ$Z%Z$$FFZ$$$$%ZZFF%F$F$F'}\n","Completed translation 401 of 427 at 1715991179.422856\n","Completed translation 402 of 427 at 1715991182.2248216\n","Completed translation 403 of 427 at 1715991184.9029126\n","Completed translation 404 of 427 at 1715991187.6316004\n","Completed translation 405 of 427 at 1715991190.4115243\n","Completed translation 406 of 427 at 1715991193.149461\n","Completed translation 407 of 427 at 1715991195.859305\n","Completed translation 408 of 427 at 1715991198.5676124\n","Completed translation 409 of 427 at 1715991201.3852794\n","Completed translation 410 of 427 at 1715991204.129037\n","Completed translation 411 of 427 at 1715991206.860461\n","Completed translation 412 of 427 at 1715991209.6542606\n","Completed translation 413 of 427 at 1715991212.4331474\n","Completed translation 414 of 427 at 1715991215.1728106\n","Completed translation 415 of 427 at 1715991217.8748784\n","Completed translation 416 of 427 at 1715991220.6053057\n","Completed translation 417 of 427 at 1715991223.300731\n","Completed translation 418 of 427 at 1715991226.061651\n","Completed translation 419 of 427 at 1715991228.8338578\n","Completed translation 420 of 427 at 1715991231.553472\n","Completed translation 421 of 427 at 1715991234.2718687\n","Completed translation 422 of 427 at 1715991236.9939706\n","Completed translation 423 of 427 at 1715991239.7435935\n","Completed translation 424 of 427 at 1715991242.461045\n","Completed translation 425 of 427 at 1715991245.16923\n","Completed translation 426 of 427 at 1715991247.8635867\n","Translation426: {'source': 'Басқаша айтқанда, адам құқықтарын түсіндіруде мәдениет адамдардың көбі ойлағаннан әлдеқайда үлкен рөл атқарады, сондықтан адам құқықтарын қорғаушылар мәдени не діни тамыры тереңде жатқан әлдебір мәселе жөнінде үстірт пікір айтудан сақ болғаны жөн.Ģ', 'target': 'In other words, culture plays a much larger role in shaping interpretations of human rights than many realize, which implies that human-rights practitioners should be wary of passing judgment on any practice with deep cultural or religious roots.Ģ', 'translation': 'ssshrslshhlhlssrsrsslssshlrrlslrrhrssshhrhhlhrhllrhlrllhrsllhshhslllrhhhssrrhhrsrhsllhlshsllrlrrsslrllhrsrhrshrhhhhllslhrrrhhrllrhhshrrlsrrrhhlslsrslrsrhl$%Z%FF$F%ZZF$ZZZ$%%%$$$ZF$F$F$ZF%Z%Z$%ZZ%%Z$F$$$Z$$ZF%%$%%$ZFF$Z$$%Z%$ZZF%$F%%$F$$$$F$$FZZF%FFFFF%Z$ZF'}\n"]}],"source":["from google.colab import drive\n","\n","import json\n","import random\n","import time\n","from typing import Optional\n","\n","import torch\n","import numpy as np\n","\n","drive.mount('/content/drive', force_remount=True)\n","root_filepath = '/content/drive/MyDrive/CS685-NLP-GroupProject/Code/singleNotebookFromPythonFilesStructure/'\n","is_remote_execution = True\n","torch.set_printoptions(threshold=100000, edgeitems=10000, linewidth=100000)\n","torch.device(\"cuda\")\n","\n","\n","# 16M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_0 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_0',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_0-1715937924-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586293\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_0.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 2048,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 15M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_1 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_1',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_1-1715937961-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586974\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_1.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1796,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 4M Turkish\n","SETimesByT5Vaswani2017Kocmi2018_2 = {\n","    'runner_hyperparameters_name': 'SETimesByT5Vaswani2017Kocmi2018_2',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/SETimesByT5Vaswani2017Kocmi2018_2-1715936459-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/setimes/setimes_parsed-1715586361\",\n","    'output_filepath': root_filepath + \"resources/model_translations/SETimesByT5Vaswani2017Kocmi2018_2.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 0,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 0,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 128,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1024,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 16M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_0 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_0',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_0-1715965639-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_0.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 2048,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 15M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_1 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_1',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_1-1715966141-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_1.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 256,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1796,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","# 4M Kazakh\n","NewsCommentaryByT5Vaswani2017Kocmi2018_2 = {\n","    'runner_hyperparameters_name': 'NewsCommentaryByT5Vaswani2017Kocmi2018_2',\n","    'model_parameter_filepath': root_filepath + \"resources/saved_parameters/NewsCommentaryByT5Vaswani2017Kocmi2018_2-1715965581-model.params\",\n","    'datasets_filepath': root_filepath + \"resources/parsed_datasets/SMTNewsCommentary/SMTNewsCommentary_parsed-1715949808\",\n","    'output_filepath': root_filepath + \"resources/model_translations/NewsCommentaryByT5Vaswani2017Kocmi2018_2.json\",\n","    'model_hyperparameters': {\n","        'src_vocab_size': 150,\n","        'tgt_vocab_size': 81,\n","        'max_src_seq_len': 266,\n","        'max_tgt_seq_len': 256,\n","        'd_model': 128,\n","        'nhead': 8,\n","        # number of encoders is 3 times that of decoders, following Xue 2021 - ByT5 - Sec 3.1\n","        'num_encoder_layers': 9,\n","        'num_decoder_layers': 3,\n","        'dim_feedforward': 1024,\n","        'dropout': 0.1,\n","        'activation': torch.nn.functional.relu,\n","        'custom_encoder': None,\n","        'custom_decoder': None,\n","        'layer_norm_eps': 1e-5,\n","        'batch_first': True,\n","        'norm_first': False,\n","        'bias': True,\n","        'device': None,\n","        'dtype': None\n","    }\n","}\n","\n","\n","class DatasetHolder:\n","\n","    def __init__(self):\n","        self.unknown_vocabulary_type = None\n","        self.padding_vocabulary_type = None\n","        self.end_of_sequence_type = None\n","        self.target_vocab = None\n","        self.target_vocab_array = None\n","        self.target_vocab_counts = None\n","        self.source_vocab = None\n","        self.source_vocab_array = None\n","        self.source_vocab_counts = None\n","        self.target_encodings = None\n","        self.target_encodings_train = None\n","        self.target_encodings_test = None\n","        self.source_encodings = None\n","        self.source_encodings_train = None\n","        self.source_encodings_test = None\n","        self.max_src_seq_obs = 0\n","        self.max_tgt_seq_obs = 0\n","\n","    def get_unknown_vocabulary_type(self):\n","        return self.unknown_vocabulary_type\n","\n","    def set_unknown_vocabulary_type(self, unknown_vocabulary_type):\n","        self.unknown_vocabulary_type = unknown_vocabulary_type\n","\n","    def get_padding_vocabulary_type(self):\n","        return self.padding_vocabulary_type\n","\n","    def set_padding_vocabulary_type(self, padding_vocabulary_type):\n","        self.padding_vocabulary_type = padding_vocabulary_type\n","\n","    def get_end_of_sequence_vocabulary_type(self):\n","        return self.end_of_sequence_type\n","\n","    def set_end_of_sequence_vocabulary_type(self, end_of_sequence_type):\n","        self.end_of_sequence_type = end_of_sequence_type\n","\n","    def get_target_vocab(self):\n","        return self.target_vocab\n","\n","    def set_target_vocab(self, target_vocab):\n","        self.target_vocab = target_vocab\n","\n","    def get_target_vocab_numpy(self):\n","        if self.target_vocab_array is None:\n","            self.target_vocab_array = np.array(self.target_vocab)\n","        return self.target_vocab_array\n","\n","    def get_target_vocab_counts(self):\n","        return self.target_vocab_counts\n","\n","    def set_target_vocab_counts(self, target_vocab_counts):\n","        self.target_vocab_counts = target_vocab_counts\n","\n","    def get_source_vocab(self):\n","        return self.source_vocab\n","\n","    def set_source_vocab(self, source_vocab):\n","        self.source_vocab = source_vocab\n","\n","    def get_source_vocab_numpy(self):\n","        if self.source_vocab_array is None:\n","            self.source_vocab_array = np.array(self.source_vocab)\n","        return self.source_vocab_array\n","\n","    def get_source_vocab_counts(self):\n","        return self.source_vocab_counts\n","\n","    def set_source_vocab_counts(self, source_vocab_counts):\n","        self.source_vocab_counts = source_vocab_counts\n","\n","    def get_target_encodings(self):\n","        return self.target_encodings\n","\n","    def set_target_encodings(self, target_encodings):\n","        del self.target_encodings\n","        self.target_encodings = target_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings(self):\n","        return self.source_encodings\n","\n","    def set_source_encodings(self, source_encodings):\n","        del self.source_encodings\n","        self.source_encodings = source_encodings\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_train(self):\n","        return self.target_encodings_train\n","\n","    def set_target_encodings_train(self, target_encodings_train):\n","        del self.target_encodings_train\n","        self.target_encodings_train = target_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_train(self):\n","        return self.source_encodings_train\n","\n","    def set_source_encodings_train(self, source_encodings_train):\n","        del self.source_encodings_train\n","        self.source_encodings_train = source_encodings_train\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_target_encodings_test(self):\n","        return self.target_encodings_test\n","\n","    def set_target_encodings_test(self, target_encodings_test):\n","        del self.target_encodings_test\n","        self.target_encodings_test = target_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_source_encodings_test(self):\n","        return self.source_encodings_test\n","\n","    def set_source_encodings_test(self, source_encodings_test):\n","        del self.source_encodings_test\n","        self.source_encodings_test = source_encodings_test\n","        if is_remote_execution:\n","            torch.cuda.empty_cache()\n","\n","    def get_max_src_seq_obs(self):\n","        return self.max_src_seq_obs\n","\n","    def set_max_src_seq_obs(self, max_src_seq_obs):\n","        self.max_src_seq_obs = max_src_seq_obs\n","\n","    def get_max_tgt_seq_obs(self):\n","        return self.max_tgt_seq_obs\n","\n","    def set_max_tgt_seq_obs(self, max_tgt_seq_obs):\n","        self.max_tgt_seq_obs = max_tgt_seq_obs\n","\n","# class name matches file name\n","class transformer_vaswani2017(torch.nn.Transformer):\n","\n","    def __init__(self,\n","                 model_hyperparameters):\n","        super().__init__(\n","            d_model=model_hyperparameters['d_model'],\n","            nhead=model_hyperparameters['nhead'],\n","            num_encoder_layers=model_hyperparameters['num_encoder_layers'],\n","            num_decoder_layers=model_hyperparameters['num_decoder_layers'],\n","            dim_feedforward=model_hyperparameters['dim_feedforward'],\n","            dropout=model_hyperparameters['dropout'],\n","            activation=model_hyperparameters['activation'],\n","            custom_encoder=model_hyperparameters['custom_encoder'],\n","            custom_decoder=model_hyperparameters['custom_decoder'],\n","            layer_norm_eps=model_hyperparameters['layer_norm_eps'],\n","            batch_first=model_hyperparameters['batch_first'],\n","            norm_first=model_hyperparameters['norm_first'],\n","            bias=model_hyperparameters['bias'],\n","            device=model_hyperparameters['device'],\n","            dtype=model_hyperparameters['dtype']\n","        )\n","        self.model_hyperparameters = model_hyperparameters\n","        # add one for the end of sequence token\n","        self.max_src_seq_len = model_hyperparameters['max_src_seq_len'] + 1\n","        self.max_tgt_seq_len = model_hyperparameters['max_tgt_seq_len'] + 1\n","        self.src_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['src_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.tgt_embeddings = torch.nn.Embedding(\n","            model_hyperparameters['tgt_vocab_size'],\n","            model_hyperparameters['d_model']\n","        )\n","        self.src_pos_enc = torch.nn.Embedding(\n","            self.max_src_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.tgt_pos_enc = torch.nn.Embedding(\n","            self.max_tgt_seq_len,\n","            model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        self.linear_output_projection_1 = torch.nn.Linear(\n","            self.max_tgt_seq_len,\n","            1,\n","            bias=False\n","        )\n","        self.linear_output_projection_2 = torch.nn.Linear(\n","            model_hyperparameters['d_model'],\n","            model_hyperparameters['tgt_vocab_size'],\n","            bias=False\n","        )\n","        self.logsoftmax_output = torch.nn.LogSoftmax(dim=1)\n","        self.model_hyperparameters = model_hyperparameters\n","        self.tgt_mask_cache = {}\n","        for i in range(0, self.max_tgt_seq_len + 1):\n","            tgt_mask = self.generate_square_subsequent_mask(i)\n","            if is_remote_execution:\n","                self.tgt_mask_cache[i] = tgt_mask.to(device=\"cuda\")\n","            self.tgt_mask_cache[i] = tgt_mask\n","        self.indices_cache = {}\n","        for i in range(0, max([self.max_src_seq_len, self.max_tgt_seq_len]) + 1):\n","            indices = torch.tensor(np.arange(0, i), dtype=torch.long)\n","            if is_remote_execution:\n","                indices = indices.to(device=\"cuda\")\n","            self.indices_cache[i] = indices\n","\n","    def forward(self,\n","                src: torch.Tensor,\n","                tgt: torch.Tensor,\n","                src_mask: Optional[torch.Tensor] = None,\n","                tgt_mask: Optional[torch.Tensor] = None,\n","                memory_mask: Optional[torch.Tensor] = None,\n","                src_key_padding_mask: Optional[torch.Tensor] = None,\n","                tgt_key_padding_mask: Optional[torch.Tensor] = None,\n","                memory_key_padding_mask: Optional[torch.Tensor] = None,\n","                src_is_causal: Optional[bool] = None,\n","                tgt_is_causal: Optional[bool] = True,\n","                memory_is_causal: bool = False) -> torch.Tensor:\n","        if len(src.shape) == 1:\n","            src = src.unsqueeze(dim=0)\n","        if len(tgt.shape) == 1:\n","            tgt = tgt.unsqueeze(dim=0)\n","        src_embedding_pos_enc = self.src_embeddings(src) + self.src_pos_enc(self.indices_cache[src.shape[1]])\n","        tgt_embedding_pos_enc = self.tgt_embeddings(tgt) + self.tgt_pos_enc(self.indices_cache[tgt.shape[1]])\n","        tgt_mask = self.tgt_mask_cache[tgt.shape[1]]\n","        transformer_output = super().forward(src_embedding_pos_enc, tgt_embedding_pos_enc, src_mask,\n","                                             tgt_mask, memory_mask, src_key_padding_mask,\n","                                             tgt_key_padding_mask, memory_key_padding_mask,\n","                                             src_is_causal, tgt_is_causal, memory_is_causal)\n","        transformer_output = torch.swapaxes(transformer_output, -1, -2)\n","        transformer_output = torch.nn.functional.pad(\n","            transformer_output,\n","            (0, self.max_tgt_seq_len - transformer_output.shape[2]),\n","            value=0\n","        )\n","        output = self.logsoftmax_output(\n","            self.linear_output_projection_2(\n","                torch.squeeze(\n","                    self.linear_output_projection_1(\n","                        transformer_output\n","                    ),\n","                    dim=2\n","                )\n","            )\n","        )\n","        del src_embedding_pos_enc\n","        del tgt_embedding_pos_enc\n","        del transformer_output\n","        return output\n","\n","    def freeze_target_embeddings(self):\n","        del self.tgt_embeddings\n","        self.tgt_embeddings = torch.nn.Embedding(\n","            self.model_hyperparameters['tgt_vocab_size'],\n","            self.model_hyperparameters['d_model']\n","        ).requires_grad_(False)\n","        del self.linear_output_projection_1\n","        self.linear_output_projection_1 = torch.nn.Linear(\n","            self.max_tgt_seq_len,\n","            1,\n","            bias=False\n","        ).requires_grad_(False)\n","        del self.linear_output_projection_2\n","        self.linear_output_projection_2 = torch.nn.Linear(\n","            self.model_hyperparameters['d_model'],\n","            self.model_hyperparameters['tgt_vocab_size'],\n","            bias=False\n","        ).requires_grad_(False)\n","\n","    def set_source_embeddings_for_transfer_learning(self, source_embeddings_dim):\n","        del self.src_embeddings\n","        self.src_embeddings = torch.nn.Embedding(\n","            source_embeddings_dim,\n","            self.model_hyperparameters['d_model']\n","        )\n","\n","\n","class Runner:\n","\n","    def __init__(self):\n","        self.runner_hyperparameters = NewsCommentaryByT5Vaswani2017Kocmi2018_0\n","        self.model_hyperparameters = self.runner_hyperparameters['model_hyperparameters']\n","        self.runner_hyperparameters_name = self.runner_hyperparameters['runner_hyperparameters_name']\n","        self.model_parameter_filepath = self.runner_hyperparameters['model_parameter_filepath']\n","        self.datasets_filepath = self.runner_hyperparameters['datasets_filepath']\n","        self.output_filepath = self.runner_hyperparameters['output_filepath']\n","        self.max_target_sequence_length = self.runner_hyperparameters['model_hyperparameters']['max_tgt_seq_len']\n","        self.dataset_holder: DatasetHolder = None\n","        self.model = None\n","        print(f\"Initialized runner {self.runner_hyperparameters_name} with parameters {self.runner_hyperparameters}\")\n","\n","    def load_dataset(self):\n","        self.dataset_holder = torch.load(self.datasets_filepath)\n","\n","    def load_model(self):\n","        self.model = transformer_vaswani2017(model_hyperparameters=self.model_hyperparameters)\n","        self.model.freeze_target_embeddings()\n","        if is_remote_execution:\n","            model_parameters = torch.load(self.model_parameter_filepath, map_location=torch.device('cuda'))\n","        else:\n","            model_parameters = torch.load(self.model_parameter_filepath, map_location=torch.device('cpu'))\n","        self.model.load_state_dict(model_parameters)\n","        if is_remote_execution:\n","            self.model.cuda()\n","        self.model.eval()\n","\n","    def evaluate_model(self):\n","        outputs = list()\n","        source_vocab_numpy = self.dataset_holder.get_source_vocab_numpy()\n","        target_vocab_numpy = self.dataset_holder.get_target_vocab_numpy()\n","        assert len(self.dataset_holder.get_source_encodings_test()) == len(self.dataset_holder.get_target_encodings_test())\n","        for i in range(0, len(self.dataset_holder.get_source_encodings_test())):\n","            if is_remote_execution:\n","                source_encoding = self.dataset_holder.get_source_encodings_test()[i].to(device=\"cuda\")\n","                target_encoding = self.dataset_holder.get_target_encodings_test()[i].to(device=\"cuda\")\n","            else:\n","                source_encoding = self.dataset_holder.get_source_encodings_test()[i].to(device=\"cpu\")\n","                target_encoding = self.dataset_holder.get_target_encodings_test()[i].to(device=\"cpu\")\n","            j = 0\n","            end_of_sequence = False\n","            while j < self.model_hyperparameters['max_tgt_seq_len'] and end_of_sequence == False:\n","                target_encoding_slice = torch.tensor_split(target_encoding, [j], dim=0)\n","                output_logits = self.model.forward(\n","                    source_encoding,\n","                    target_encoding_slice[0]\n","                ).detach().to(device=\"cpu\").flatten().numpy()\n","                output_logits_sort_pairs = list()\n","                for k in range(0, len(output_logits)):\n","                    output_logits_sort_pairs.append([output_logits[k], k])\n","                output_logits_sorted = sorted(output_logits_sort_pairs, key=lambda logit_pair: logit_pair[0], reverse=True)\n","                output_sort_index = 0\n","                # don't pick mark up characters or whitespace if first term\n","                if j == 0:\n","                    while output_logits_sorted[output_sort_index][1] in (0, 1, 2, 7):\n","                        output_sort_index = output_sort_index + 1\n","                else:\n","                    # don't repeat unknown, padding, or spaces\n","                    if (prediction_encoding[j-1] == 0\n","                            or prediction_encoding[j-1] == 1\n","                            or prediction_encoding[j-1] == 7):\n","                        while (output_logits_sorted[output_sort_index][1] == 0\n","                               or output_logits_sorted[output_sort_index][1] == 1\n","                               or output_logits_sorted[output_sort_index][1] == 7):\n","                            output_sort_index = output_sort_index + 1\n","                output_vocab_index = output_logits_sorted[output_sort_index][1]\n","                # index = output_vocab_index # always outputs \"Krrrr...<eos>\"\n","                index = min(random.randint(output_vocab_index, output_vocab_index + 3), len(output_logits_sorted))\n","                if j == 0:\n","                    prediction_encoding = torch.tensor([output_logits_sorted[index][1]], dtype=torch.float)\n","                else:\n","                    prediction_encoding = torch.cat([prediction_encoding, torch.tensor([output_logits_sorted[index][1]], dtype=torch.float)])\n","                if prediction_encoding[j] == self.dataset_holder.get_target_vocab().index(\n","                        self.dataset_holder.get_end_of_sequence_vocabulary_type()):\n","                    end_of_sequence = True\n","                j = j + 1\n","            decoded_source = np.take(source_vocab_numpy, source_encoding.detach().to(device=\"cpu\").flatten().numpy())\n","            decoded_target = np.take(target_vocab_numpy, target_encoding.detach().to(device=\"cpu\").flatten().numpy())\n","            decoded_translation = np.take(target_vocab_numpy, prediction_encoding.detach().to(device=\"cpu\", dtype=torch.int32).numpy())\n","            outputs.append({\n","                \"source\": \"\".join(decoded_source.tolist()),\n","                \"target\": \"\".join(decoded_target.tolist()),\n","                \"translation\": \"\".join(decoded_translation.tolist())\n","            })\n","            del source_encoding\n","            del target_encoding\n","            del prediction_encoding\n","            del target_encoding_slice\n","            del decoded_source\n","            del decoded_target\n","            del decoded_translation\n","            print(f\"Completed translation {i} of {len(self.dataset_holder.get_source_encodings_test())} at {time.time()}\")\n","            if is_remote_execution:\n","                torch.cuda.empty_cache()\n","                if i % 100 == 0:\n","                    print(f\"Memory usage summary:\")\n","                    print(f\"{torch.cuda.memory_summary()}\")\n","                    torch.cuda.reset_max_memory_allocated()\n","                    torch.cuda.reset_max_memory_cached()\n","                    torch.cuda.reset_peak_memory_stats()\n","            if i % 100 == 0 or i == len(self.dataset_holder.get_source_encodings_test()) - 1:\n","                print(f\"Translation{i}: {outputs[i]}\")\n","                output_file = open(self.output_filepath, \"w\")\n","                output_file.write(json.dumps(outputs, ensure_ascii=False, indent=1))\n","                output_file.close()\n","\n","\n","runner = Runner()\n","runner.load_dataset()\n","runner.load_model()\n","runner.evaluate_model()\n"]}]}